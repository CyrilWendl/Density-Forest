{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Hypercolumn CNN  for Zurich Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os, sys\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# custom libraries\n",
    "base_dir = '/raid/home/cwendl'  # for guanabana\n",
    "sys.path.append(base_dir + '/SIE-Master/Code')  # Path to density Tree package\n",
    "sys.path.append(base_dir + '/SIE-Master/Zurich')  # Path to density Tree package\n",
    "from helpers.data_loader import ZurichLoader\n",
    "from helpers.helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/raid/home/cwendl'  # for guanabana\n",
    "root_dir = base_dir + '/SIE-Master/Zurich'\n",
    "patch_size = 128\n",
    "\n",
    "# load data\n",
    "dataset_train = ZurichLoader(root_dir, 'train', patch_size=patch_size, stride=patch_size, transform='augment', \n",
    "                             random_crop=True)\n",
    "\n",
    "dataset_val = ZurichLoader(root_dir, 'val', patch_size=patch_size, stride=patch_size)\n",
    "dataset_test = ZurichLoader(root_dir, 'test', patch_size=patch_size, stride=patch_size)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=10, shuffle=True, num_workers=10)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=10, shuffle=False, num_workers=10)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=10, shuffle=False, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperColumn(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, n_filters, act=nn.ReLU, patch_size=128):\n",
    "        super(HyperColumn, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_filters = n_filters\n",
    "        self.act = act()\n",
    "\n",
    "        self.down_1 = self.conv_block(self.in_dim, self.n_filters, self.act)\n",
    "        self.down_2 = self.conv_block(self.n_filters, self.n_filters * 2, self.act)\n",
    "        self.down_3 = self.conv_block(self.n_filters * 2, self.n_filters * 4, self.act)\n",
    "        self.down_4 = self.conv_block(self.n_filters * 4, self.n_filters * 8, self.act)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.upsample = nn.Upsample(size=[patch_size, patch_size], mode='bilinear', align_corners=True)\n",
    "\n",
    "        dim_end = self.in_dim + sum(self.n_filters * [1, 2, 4, 8])\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(dim_end, dim_end, kernel_size=1, stride=1),\n",
    "            self.act,\n",
    "            nn.Dropout(.5),\n",
    "            nn.Conv2d(dim_end, self.out_dim, kernel_size=1, stride=1)\n",
    "        )\n",
    "\n",
    "    def conv_block(self, dim_in, dim_out, act, kernel_size=3, stride=1, padding=0):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(dim_out),\n",
    "            act\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        down_1 = self.down_1(input)\n",
    "        pool_1 = self.pool(down_1)\n",
    "        down_2 = self.down_2(pool_1)\n",
    "        pool_2 = self.pool(down_2)\n",
    "        down_3 = self.down_3(pool_2)\n",
    "        pool_3 = self.pool(down_3)\n",
    "        down_4 = self.down_4(pool_3)\n",
    "        pool_4 = self.pool(down_4)\n",
    "\n",
    "        pool_1 = self.upsample(pool_1)\n",
    "        pool_2 = self.upsample(pool_2)\n",
    "        pool_3 = self.upsample(pool_3)\n",
    "        pool_4 = self.upsample(pool_4)\n",
    "\n",
    "        cat_l = torch.cat([input, pool_1, pool_2, pool_3, pool_4], dim=1)\n",
    "\n",
    "        out = self.out(cat_l)\n",
    "        if self.training:\n",
    "            out = nn.LogSoftmax(dim=1)(out)\n",
    "        else:\n",
    "            out = nn.Softmax(dim=1)(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "def acc_with_filt(y_true, y_pred, label_to_ignore):\n",
    "    \"\"\"\n",
    "    get accuracy ignoring a label in y_true\n",
    "    :param y_true: ground truth (tensor)\n",
    "    :param y_pred: predicted label (tensor)\n",
    "    :param label_to_ignore: label to ignore\n",
    "    :return: accuracy\n",
    "    \"\"\"\n",
    "    y_true = y_true.numpy().flatten()\n",
    "    y_pred = y_pred.numpy().flatten()\n",
    "    filt = y_true != label_to_ignore\n",
    "    return np.sum(np.equal(y_pred[filt], y_true[filt]))/len(y_true[filt])\n",
    "\n",
    "\n",
    "def test(model, f_loss, dataloader_train, dataloader_val, verbosity=False):\n",
    "    with torch.no_grad():\n",
    "        acc_tr_val, loss_tr_val = [], []\n",
    "        for dataloader, name in zip([dataloader_train, dataloader_val], ['Training', 'Validation']):\n",
    "            model.eval()\n",
    "            loss = 0\n",
    "            acc = []  # average accuracy\n",
    "            for i_batch, (im, gt) in enumerate(dataloader):\n",
    "                im = im.cuda()\n",
    "                gt = gt.cuda()\n",
    "                output = model(im)\n",
    "                loss += f_loss(output, gt).cpu()\n",
    "                _, pred = output.cpu().max(1, keepdim=True)\n",
    "                acc.append(acc_with_filt(gt.cpu(), pred.cpu(), 0))\n",
    "                #correct += float(pred.eq(gt.cpu().view_as(pred)).sum()) / (64.0 ** 2)\n",
    "                #print(correct)\n",
    "\n",
    "            loss /= len(dataloader.dataset)\n",
    "            accuracy = np.mean(acc)\n",
    "            acc_tr_val.append(accuracy)\n",
    "            loss_tr_val.append(loss)\n",
    "            if verbosity>0:\n",
    "                print(name + ' set: Average loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                      .format(loss, accuracy * 100))\n",
    "        return acc_tr_val[0], acc_tr_val[1], loss_tr_val[0], loss_tr_val[1]\n",
    "\n",
    "def predict(model, dataloader_pred):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_pred = torch.LongTensor()\n",
    "        for i_batch, (im, gt) in enumerate(dataloader_pred):\n",
    "            im = im.cuda()\n",
    "            gt = gt.cuda()\n",
    "            output = model(im)\n",
    "            _, pred = output.cpu().max(1, keepdim=True)\n",
    "            test_pred = torch.cat((test_pred, pred), dim=0)\n",
    "    \n",
    "    return test_pred\n",
    "\n",
    "\n",
    "def train(model, dataloader_train, dataloader_val, epochs, verbosity=0, plot=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Train a model for a given number of epochs\n",
    "    :param model: Model to train\n",
    "    :param dataloader_train: dataloader for training data\n",
    "    :param dataloader_val: dataloader for test data\n",
    "    :param epochs: number of epochs to train\n",
    "    :param verbosity: verbosity level of status messages\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    # weights = torch.from_numpy(dataset_train.weights).float().cuda()\n",
    "    f_loss = nn.NLLLoss(ignore_index=0)\n",
    "    model.train()\n",
    "    acc_tr_hist, acc_val_hist = [], []\n",
    "    loss_tr_hist, loss_val_hist = [], []\n",
    "    for epoch in range(epochs):\n",
    "        # validation\n",
    "        av_loss = 0\n",
    "\n",
    "        for i_batch, (im, gt) in (tqdm(enumerate(dataloader_train)) if verbosity>1 else enumerate(dataloader_train)):\n",
    "            im = im.cuda()\n",
    "            gt = gt.cuda()\n",
    "            opt.zero_grad()\n",
    "            output = model(im)\n",
    "            loss_out = f_loss(output, gt)\n",
    "            av_loss += loss_out.cpu().detach().numpy()\n",
    "            loss_out.backward()\n",
    "            opt.step()\n",
    "\n",
    "            if not i_batch % 100 and verbosity > 1:\n",
    "                tqdm.write(\"Average loss: {:.2f}\".format(av_loss/(i_batch+1)))\n",
    "\n",
    "            # log to tensorboard\n",
    "            # info = {'loss': av_loss, 'accuracy': accuracy, i_batch\n",
    "        if verbosity>0:\n",
    "            print(\"Epoch %i:\" % epoch)\n",
    "        acc_tr, acc_val, loss_tr, loss_val = test(model, f_loss, dataloader_train, dataloader_val, verbosity=verbosity)\n",
    "        acc_tr_hist.append(acc_tr)\n",
    "        acc_val_hist.append(acc_val)\n",
    "        loss_tr_hist.append(loss_tr)\n",
    "        loss_val_hist.append(loss_val)\n",
    "        if plot:\n",
    "            # plot accuracy history\n",
    "            fig, ax = plt.subplots(1,1)\n",
    "            ax.plot(np.arange(epoch+1), acc_tr_hist)\n",
    "            ax.plot(np.arange(epoch+1), acc_val_hist)\n",
    "            ax.set_xlabel(\"Epochs\")\n",
    "            ax.set_ylabel(\"OA\")\n",
    "            ax.set_ylim([0,1])\n",
    "            ax.grid(alpha=.3)\n",
    "            fig.axes[0].spines['right'].set_visible(False)\n",
    "            fig.axes[0].spines['top'].set_visible(False)\n",
    "            ax.legend(['Training Set','Validation Set'])\n",
    "            plt.savefig('Figures/hist_train_all_acc.pdf')\n",
    "            plt.close()\n",
    "            \n",
    "            # plot loss history\n",
    "            fig, ax = plt.subplots(1,1)\n",
    "            ax.plot(np.arange(epoch+1), loss_tr_hist)\n",
    "            ax.plot(np.arange(epoch+1), loss_val_hist)\n",
    "            ax.set_xlabel(\"Epochs\")\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "            ax.grid(alpha=.3)\n",
    "            fig.axes[0].spines['right'].set_visible(False)\n",
    "            fig.axes[0].spines['top'].set_visible(False)\n",
    "            ax.legend(['Training Set','Validation Set'])\n",
    "            plt.savefig('Figures/hist_train_all_loss.pdf')\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HyperColumn(in_dim=4, out_dim=9, n_filters=32, patch_size=patch_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Training set: Average loss: -0.0489, Accuracy: 61.95%\n",
      "Validation set: Average loss: -0.0479, Accuracy: 56.96%\n",
      "Epoch 1:\n",
      "Training set: Average loss: -0.0534, Accuracy: 53.75%\n",
      "Validation set: Average loss: -0.0493, Accuracy: 49.11%\n",
      "Epoch 2:\n",
      "Training set: Average loss: -0.0561, Accuracy: 56.22%\n",
      "Validation set: Average loss: -0.0559, Accuracy: 55.66%\n",
      "Epoch 3:\n",
      "Training set: Average loss: -0.0552, Accuracy: 55.23%\n",
      "Validation set: Average loss: -0.0546, Accuracy: 54.27%\n",
      "Epoch 4:\n",
      "Training set: Average loss: -0.0554, Accuracy: 55.46%\n",
      "Validation set: Average loss: -0.0555, Accuracy: 55.19%\n",
      "Epoch 5:\n",
      "Training set: Average loss: -0.0553, Accuracy: 55.38%\n",
      "Validation set: Average loss: -0.0558, Accuracy: 55.53%\n",
      "Epoch 6:\n",
      "Training set: Average loss: -0.0550, Accuracy: 54.97%\n",
      "Validation set: Average loss: -0.0556, Accuracy: 55.31%\n",
      "Epoch 7:\n",
      "Training set: Average loss: -0.0599, Accuracy: 60.33%\n",
      "Validation set: Average loss: -0.0611, Accuracy: 60.80%\n",
      "Epoch 8:\n",
      "Training set: Average loss: -0.0516, Accuracy: 51.90%\n",
      "Validation set: Average loss: -0.0573, Accuracy: 57.19%\n",
      "Epoch 9:\n",
      "Training set: Average loss: -0.0421, Accuracy: 41.88%\n",
      "Validation set: Average loss: -0.0426, Accuracy: 42.09%\n",
      "Epoch 10:\n",
      "Training set: Average loss: -0.0560, Accuracy: 55.95%\n",
      "Validation set: Average loss: -0.0562, Accuracy: 55.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python36/lib/python3.6/site-packages/matplotlib/pyplot.py:537: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:\n",
      "Training set: Average loss: -0.0665, Accuracy: 66.81%\n",
      "Validation set: Average loss: -0.0657, Accuracy: 65.22%\n",
      "Epoch 12:\n",
      "Training set: Average loss: -0.0677, Accuracy: 67.63%\n",
      "Validation set: Average loss: -0.0641, Accuracy: 63.56%\n",
      "Epoch 13:\n",
      "Training set: Average loss: -0.0664, Accuracy: 66.41%\n",
      "Validation set: Average loss: -0.0625, Accuracy: 61.93%\n",
      "Epoch 14:\n",
      "Training set: Average loss: -0.0693, Accuracy: 69.27%\n",
      "Validation set: Average loss: -0.0639, Accuracy: 63.33%\n",
      "Epoch 15:\n",
      "Training set: Average loss: -0.0701, Accuracy: 70.03%\n",
      "Validation set: Average loss: -0.0650, Accuracy: 64.46%\n",
      "Epoch 16:\n",
      "Training set: Average loss: -0.0704, Accuracy: 70.32%\n",
      "Validation set: Average loss: -0.0657, Accuracy: 65.17%\n",
      "Epoch 17:\n",
      "Training set: Average loss: -0.0685, Accuracy: 68.57%\n",
      "Validation set: Average loss: -0.0631, Accuracy: 62.81%\n",
      "Epoch 18:\n",
      "Training set: Average loss: -0.0706, Accuracy: 70.57%\n",
      "Validation set: Average loss: -0.0648, Accuracy: 64.31%\n",
      "Epoch 19:\n",
      "Training set: Average loss: -0.0670, Accuracy: 67.09%\n",
      "Validation set: Average loss: -0.0609, Accuracy: 60.63%\n",
      "Epoch 20:\n",
      "Training set: Average loss: -0.0681, Accuracy: 68.27%\n",
      "Validation set: Average loss: -0.0620, Accuracy: 61.54%\n",
      "Epoch 21:\n",
      "Training set: Average loss: -0.0660, Accuracy: 66.22%\n",
      "Validation set: Average loss: -0.0590, Accuracy: 58.68%\n",
      "Epoch 22:\n",
      "Training set: Average loss: -0.0687, Accuracy: 68.89%\n",
      "Validation set: Average loss: -0.0612, Accuracy: 60.98%\n",
      "Epoch 23:\n",
      "Training set: Average loss: -0.0670, Accuracy: 67.13%\n",
      "Validation set: Average loss: -0.0571, Accuracy: 56.82%\n",
      "Epoch 24:\n",
      "Training set: Average loss: -0.0641, Accuracy: 64.07%\n",
      "Validation set: Average loss: -0.0566, Accuracy: 56.23%\n",
      "Epoch 25:\n",
      "Training set: Average loss: -0.0714, Accuracy: 71.27%\n",
      "Validation set: Average loss: -0.0620, Accuracy: 61.42%\n",
      "Epoch 26:\n",
      "Training set: Average loss: -0.0717, Accuracy: 71.83%\n",
      "Validation set: Average loss: -0.0617, Accuracy: 61.32%\n",
      "Epoch 27:\n",
      "Training set: Average loss: -0.0703, Accuracy: 70.30%\n",
      "Validation set: Average loss: -0.0596, Accuracy: 59.18%\n",
      "Epoch 28:\n",
      "Training set: Average loss: -0.0732, Accuracy: 73.18%\n",
      "Validation set: Average loss: -0.0642, Accuracy: 63.61%\n",
      "Epoch 29:\n",
      "Training set: Average loss: -0.0704, Accuracy: 70.64%\n",
      "Validation set: Average loss: -0.0607, Accuracy: 60.28%\n",
      "Epoch 30:\n",
      "Training set: Average loss: -0.0719, Accuracy: 71.97%\n",
      "Validation set: Average loss: -0.0603, Accuracy: 59.83%\n",
      "Epoch 31:\n",
      "Training set: Average loss: -0.0739, Accuracy: 73.85%\n",
      "Validation set: Average loss: -0.0625, Accuracy: 61.97%\n",
      "Epoch 32:\n",
      "Training set: Average loss: -0.0745, Accuracy: 74.48%\n",
      "Validation set: Average loss: -0.0636, Accuracy: 63.10%\n",
      "Epoch 33:\n",
      "Training set: Average loss: -0.0748, Accuracy: 74.74%\n",
      "Validation set: Average loss: -0.0630, Accuracy: 62.48%\n",
      "Epoch 34:\n",
      "Training set: Average loss: -0.0752, Accuracy: 75.11%\n",
      "Validation set: Average loss: -0.0628, Accuracy: 62.28%\n",
      "Epoch 35:\n",
      "Training set: Average loss: -0.0754, Accuracy: 75.34%\n",
      "Validation set: Average loss: -0.0636, Accuracy: 63.10%\n",
      "Epoch 36:\n",
      "Training set: Average loss: -0.0759, Accuracy: 75.76%\n",
      "Validation set: Average loss: -0.0637, Accuracy: 63.15%\n",
      "Epoch 37:\n",
      "Training set: Average loss: -0.0759, Accuracy: 75.79%\n",
      "Validation set: Average loss: -0.0645, Accuracy: 63.92%\n",
      "Epoch 38:\n",
      "Training set: Average loss: -0.0760, Accuracy: 75.91%\n",
      "Validation set: Average loss: -0.0632, Accuracy: 62.63%\n",
      "Epoch 39:\n",
      "Training set: Average loss: -0.0760, Accuracy: 75.83%\n",
      "Validation set: Average loss: -0.0634, Accuracy: 62.86%\n",
      "Epoch 40:\n",
      "Training set: Average loss: -0.0758, Accuracy: 75.61%\n",
      "Validation set: Average loss: -0.0634, Accuracy: 62.81%\n",
      "Epoch 41:\n",
      "Training set: Average loss: -0.0758, Accuracy: 75.61%\n",
      "Validation set: Average loss: -0.0641, Accuracy: 63.54%\n",
      "Epoch 42:\n",
      "Training set: Average loss: -0.0757, Accuracy: 75.61%\n",
      "Validation set: Average loss: -0.0635, Accuracy: 62.97%\n",
      "Epoch 43:\n",
      "Training set: Average loss: -0.0759, Accuracy: 75.77%\n",
      "Validation set: Average loss: -0.0623, Accuracy: 61.77%\n",
      "Epoch 44:\n",
      "Training set: Average loss: -0.0758, Accuracy: 75.65%\n",
      "Validation set: Average loss: -0.0608, Accuracy: 60.22%\n",
      "Epoch 45:\n"
     ]
    }
   ],
   "source": [
    "train(model, dataloader_train, dataloader_val, epochs=50, verbosity=1, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data with overlap\n",
    "dataset_train_overlap = ZurichLoader(root_dir, 'train', patch_size=patch_size, stride=int(patch_size/2))\n",
    "dataset_val_overlap = ZurichLoader(root_dir, 'val', patch_size=patch_size, stride=int(patch_size/2))\n",
    "dataset_test_overlap = ZurichLoader(root_dir, 'test', patch_size=patch_size, stride=int(patch_size/2))\n",
    "\n",
    "dataloader_train_overlap = DataLoader(dataset_train_overlap, batch_size=10, shuffle=False, num_workers=20)\n",
    "dataloader_val_overlap = DataLoader(dataset_val_overlap, batch_size=10, shuffle=False, num_workers=20)\n",
    "dataloader_test_overlap = DataLoader(dataset_test_overlap, batch_size=10, shuffle=False, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(model, dataloader_val_overlap)\n",
    "preds = preds[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = convert_patches_to_image(dataset_val.imgs, preds[..., np.newaxis], 0, 128, 64, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.from_numpy(dataloader_train.dataset.weights).float().cuda()\n",
    "f_loss = nn.NLLLoss(weight=weights, ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gt_label_to_color(dataset_val.gt[0], dataset_val.colors)*255)\n",
    "plt.show()\n",
    "plt.imshow(gt_label_to_color(img[...,0], dataset_val.colors)*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = test(model, f_loss, dataloader_train, dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds_val = predict(model, dataloader_val)\n",
    "#preds_train = predict(model, dataloader_train)\n",
    "\n",
    "# save model\n",
    "state = {\n",
    "    'model': model.state_dict(),\n",
    "    'n_epochs': 60,\n",
    "    'loss_train': 0.0  # TODO change\n",
    "}\n",
    "torch.save(state, 'model_all_cl.pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load('model_all_cl.pytorch')\n",
    "model.load_state_dict(state['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
