\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nyt/global//global/global}
\providecommand\@newglossary[4]{}
\@newglossary{acronym}{alg}{acr}{acn}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{main.ist}
\@glsorder{word}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{List of Figures}{ii}{section*.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{List of Tables}{v}{section*.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{Acronyms}{vi}{section*.6}}
\abx@aux@cite{leibig2017}
\abx@aux@segm{0}{0}{leibig2017}
\abx@aux@cite{ronneberger2015u}
\abx@aux@segm{0}{0}{ronneberger2015u}
\abx@aux@cite{Levinson2011TowardsFA}
\abx@aux@segm{0}{0}{Levinson2011TowardsFA}
\abx@aux@cite{Volpi2017DenseSL}
\abx@aux@segm{0}{0}{Volpi2017DenseSL}
\abx@aux@cite{kampffmeyer}
\abx@aux@segm{0}{0}{kampffmeyer}
\abx@aux@cite{Zhu2017DeepLI}
\abx@aux@segm{0}{0}{Zhu2017DeepLI}
\abx@aux@cite{Shelhamer2015FullyCN}
\abx@aux@segm{0}{0}{Shelhamer2015FullyCN}
\abx@aux@cite{NguyenYC14}
\abx@aux@segm{0}{0}{NguyenYC14}
\abx@aux@cite{Goodfellow2014}
\abx@aux@segm{0}{0}{Goodfellow2014}
\abx@aux@cite{KendallG17}
\abx@aux@segm{0}{0}{KendallG17}
\abx@aux@cite{Gal2016Uncertainty}
\abx@aux@segm{0}{0}{Gal2016Uncertainty}
\abx@aux@cite{Rupprecht2017LearningIA}
\abx@aux@segm{0}{0}{Rupprecht2017LearningIA}
\abx@aux@segm{0}{0}{leibig2017}
\abx@aux@segm{0}{0}{leibig2017}
\abx@aux@cite{menderes_automatic_2015}
\abx@aux@segm{0}{0}{menderes_automatic_2015}
\abx@aux@cite{womble_automated_2007}
\abx@aux@segm{0}{0}{womble_automated_2007}
\abx@aux@cite{postadjian_investigating_2017}
\abx@aux@segm{0}{0}{postadjian_investigating_2017}
\abx@aux@cite{tuiaAL2011}
\abx@aux@segm{0}{0}{tuiaAL2011}
\abx@aux@cite{Tuia2011ASO}
\abx@aux@segm{0}{0}{Tuia2011ASO}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\abx@aux@segm{0}{0}{Gal2016Uncertainty}
\abx@aux@cite{Choi2017UncertaintyAwareLF}
\abx@aux@segm{0}{0}{Choi2017UncertaintyAwareLF}
\abx@aux@segm{0}{0}{KendallG17}
\abx@aux@cite{subramanya}
\abx@aux@segm{0}{0}{subramanya}
\abx@aux@segm{0}{0}{leibig2017}
\abx@aux@cite{Sun2018KSconfA}
\abx@aux@segm{0}{0}{Sun2018KSconfA}
\abx@aux@segm{0}{0}{KendallG17}
\abx@aux@cite{Hammer2007HowTP}
\abx@aux@segm{0}{0}{Hammer2007HowTP}
\abx@aux@segm{0}{0}{Gal2016Uncertainty}
\abx@aux@segm{0}{0}{Gal2016Uncertainty}
\abx@aux@segm{0}{0}{KendallG17}
\abx@aux@cite{mandelbaum17}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@cite{Bahat_2018}
\abx@aux@segm{0}{0}{Bahat_2018}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@cite{deMorsier2014thesis}
\abx@aux@segm{0}{0}{deMorsier2014thesis}
\abx@aux@cite{decisionForests-MSR}
\abx@aux@segm{0}{0}{decisionForests-MSR}
\abx@aux@segm{0}{0}{decisionForests-MSR}
\abx@aux@cite{mnist}
\abx@aux@segm{0}{0}{mnist}
\abx@aux@cite{Volpi2015SemanticSO}
\abx@aux@segm{0}{0}{Volpi2015SemanticSO}
\abx@aux@cite{ghahramani}
\abx@aux@segm{0}{0}{ghahramani}
\abx@aux@segm{0}{0}{NguyenYC14}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{ghahramani}
\abx@aux@segm{0}{0}{NguyenYC14}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{subramanya}
\abx@aux@segm{0}{0}{subramanya}
\abx@aux@segm{0}{0}{Bahat_2018}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{Sun2018KSconfA}
\abx@aux@segm{0}{0}{Bahat_2018}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@cite{Bishop1994NoveltyDA}
\abx@aux@segm{0}{0}{Bishop1994NoveltyDA}
\abx@aux@cite{Markou2003NoveltyDApt1}
\abx@aux@segm{0}{0}{Markou2003NoveltyDApt1}
\abx@aux@cite{Markou2003NoveltyDApt2}
\abx@aux@segm{0}{0}{Markou2003NoveltyDApt2}
\abx@aux@cite{Pimentel2014ARO}
\abx@aux@segm{0}{0}{Pimentel2014ARO}
\abx@aux@segm{0}{0}{deMorsier2014thesis}
\abx@aux@segm{0}{0}{Pimentel2014ARO}
\abx@aux@segm{0}{0}{deMorsier2014thesis}
\abx@aux@segm{0}{0}{Pimentel2014ARO}
\abx@aux@segm{0}{0}{Bahat_2018}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Literature Review}{3}{section.2}}
\newlabel{sec:lit}{{2}{3}{Literature Review}{section.2}{}}
\abx@aux@segm{0}{0}{decisionForests-MSR}
\abx@aux@cite{Schmidhuber2015DeepLI}
\abx@aux@segm{0}{0}{Schmidhuber2015DeepLI}
\abx@aux@segm{0}{0}{Zhu2017DeepLI}
\abx@aux@cite{Kampffmeyer2016SemanticSO}
\abx@aux@segm{0}{0}{Kampffmeyer2016SemanticSO}
\abx@aux@cite{HendrycksG16c}
\abx@aux@segm{0}{0}{HendrycksG16c}
\abx@aux@cite{zaragoza}
\abx@aux@segm{0}{0}{zaragoza}
\abx@aux@cite{ouerghemmi_two-step_2017}
\abx@aux@segm{0}{0}{ouerghemmi_two-step_2017}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{zaragoza}
\abx@aux@segm{0}{0}{zaragoza}
\abx@aux@segm{0}{0}{Sun2018KSconfA}
\abx@aux@segm{0}{0}{Sun2018KSconfA}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Methods Based on Network Output}{4}{subsection.2.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Single-Pass Methods}{4}{subsubsection.2.1.1}}
\newlabel{eq:net_msr}{{3}{4}{Single-Pass Methods}{equation.2.3}{}}
\newlabel{eq:net_margin}{{4}{4}{Single-Pass Methods}{equation.2.4}{}}
\newlabel{eq:net_entropy}{{5}{4}{Single-Pass Methods}{equation.2.5}{}}
\abx@aux@segm{0}{0}{Bahat_2018}
\abx@aux@segm{0}{0}{Bahat_2018}
\abx@aux@segm{0}{0}{Bahat_2018}
\abx@aux@segm{0}{0}{Bahat_2018}
\abx@aux@segm{0}{0}{Bahat_2018}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Invariance to Image Transformations}{5}{subsubsection.2.1.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Invariance To Image Transformation: Schema for \gls {MNIST} digit example \cite {Bahat_2018}.\relax }}{5}{figure.caption.8}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:schema-baseline1}{{1}{5}{Invariance To Image Transformation: Schema for \gls {MNIST} digit example \cite {Bahat_2018}.\relax }{figure.caption.8}{}}
\abx@aux@segm{0}{0}{ghahramani}
\abx@aux@segm{0}{0}{ghahramani}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{leibig2017}
\abx@aux@cite{Lakshminarayanan16}
\abx@aux@segm{0}{0}{Lakshminarayanan16}
\abx@aux@segm{0}{0}{subramanya}
\abx@aux@segm{0}{0}{Kampffmeyer2016SemanticSO}
\abx@aux@segm{0}{0}{ghahramani}
\abx@aux@segm{0}{0}{ghahramani}
\abx@aux@segm{0}{0}{NguyenYC14}
\abx@aux@segm{0}{0}{NguyenYC14}
\abx@aux@segm{0}{0}{Goodfellow2014}
\abx@aux@segm{0}{0}{Goodfellow2014}
\abx@aux@segm{0}{0}{subramanya}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{Bishop1994NoveltyDA}
\abx@aux@segm{0}{0}{subramanya}
\abx@aux@segm{0}{0}{subramanya}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{Bishop1994NoveltyDA}
\abx@aux@segm{0}{0}{Bishop1994NoveltyDA}
\abx@aux@cite{Gal2015Dropout}
\abx@aux@segm{0}{0}{Gal2015Dropout}
\abx@aux@segm{0}{0}{ghahramani}
\abx@aux@segm{0}{0}{KendallG17}
\abx@aux@segm{0}{0}{Gal2015Dropout}
\abx@aux@segm{0}{0}{ghahramani}
\abx@aux@segm{0}{0}{Gal2015Dropout}
\abx@aux@segm{0}{0}{ghahramani}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}MC-Dropout}{6}{subsubsection.2.1.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Methods Based on Network Activations}{6}{subsection.2.2}}
\newlabel{subsec:pre-softmax}{{2.2}{6}{Methods Based on Network Activations}{subsection.2.2}{}}
\abx@aux@segm{0}{0}{Goodfellow2014}
\abx@aux@segm{0}{0}{Goodfellow2014}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{Lakshminarayanan16}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{Lakshminarayanan16}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{Pimentel2014ARO}
\abx@aux@segm{0}{0}{Markou2003NoveltyDApt1}
\abx@aux@segm{0}{0}{Markou2003NoveltyDApt2}
\abx@aux@cite{Reynolds2009GaussianMM}
\abx@aux@segm{0}{0}{Reynolds2009GaussianMM}
\abx@aux@segm{0}{0}{Pimentel2014ARO}
\abx@aux@cite{Wang2004AnomalyID}
\abx@aux@segm{0}{0}{Wang2004AnomalyID}
\abx@aux@cite{Beghi2014AOS}
\abx@aux@segm{0}{0}{Beghi2014AOS}
\abx@aux@cite{Szymanski2011VisualisingKS}
\abx@aux@segm{0}{0}{Szymanski2011VisualisingKS}
\abx@aux@segm{0}{0}{deMorsier2014thesis}
\abx@aux@segm{0}{0}{Pimentel2014ARO}
\abx@aux@cite{Breunig2000LOFID}
\abx@aux@segm{0}{0}{Breunig2000LOFID}
\abx@aux@cite{Liu2008IsolationF}
\abx@aux@segm{0}{0}{Liu2008IsolationF}
\abx@aux@segm{0}{0}{HendrycksG16c}
\abx@aux@segm{0}{0}{HendrycksG16c}
\abx@aux@segm{0}{0}{Bahat_2018}
\abx@aux@segm{0}{0}{Bahat_2018}
\abx@aux@segm{0}{0}{ghahramani}
\abx@aux@segm{0}{0}{ghahramani}
\abx@aux@segm{0}{0}{Sun2018KSconfA}
\abx@aux@segm{0}{0}{Sun2018KSconfA}
\abx@aux@segm{0}{0}{subramanya}
\abx@aux@segm{0}{0}{subramanya}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{Goodfellow2014}
\abx@aux@segm{0}{0}{Goodfellow2014}
\abx@aux@segm{0}{0}{KendallG17}
\abx@aux@segm{0}{0}{KendallG17}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{mandelbaum17}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Network Architectures for Confidence Estimation}{7}{subsection.2.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Comparison to Novelty Detection Methods}{7}{subsection.2.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Summary}{7}{subsection.2.5}}
\abx@aux@segm{0}{0}{decisionForests-MSR}
\abx@aux@segm{0}{0}{decisionForests-MSR}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary of reviewed confidence measures for neural networks. Implemented baselines are indicated in bold.\relax }}{8}{table.caption.9}}
\newlabel{table:summary_literature}{{1}{8}{Summary of reviewed confidence measures for neural networks. Implemented baselines are indicated in bold.\relax }{table.caption.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{8}{section.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Overview: \acrlongpl {DF}}{8}{subsection.3.1}}
\abx@aux@segm{0}{0}{decisionForests-MSR}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Workflow Diagram\relax }}{9}{figure.caption.10}}
\newlabel{fig:schema}{{2}{9}{Workflow Diagram\relax }{figure.caption.10}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Decision Trees}{9}{subsection.3.2}}
\abx@aux@segm{0}{0}{decisionForests-MSR}
\abx@aux@segm{0}{0}{decisionForests-MSR}
\abx@aux@cite{Breiman2001}
\abx@aux@segm{0}{0}{Breiman2001}
\abx@aux@cite{Zoubir2007BootstrapMA}
\abx@aux@segm{0}{0}{Zoubir2007BootstrapMA}
\abx@aux@cite{Breiman1996BaggingP}
\abx@aux@segm{0}{0}{Breiman1996BaggingP}
\abx@aux@cite{CW2017}
\abx@aux@segm{0}{0}{CW2017}
\abx@aux@segm{0}{0}{decisionForests-MSR}
\abx@aux@segm{0}{0}{decisionForests-MSR}
\newlabel{eq:ig}{{6}{10}{Decision Trees}{equation.3.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Random Forests}{10}{subsection.3.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}\acrlongpl {DF}}{10}{subsection.3.4}}
\abx@aux@segm{0}{0}{decisionForests-MSR}
\abx@aux@segm{0}{0}{decisionForests-MSR}
\abx@aux@cite{scipy}
\abx@aux@segm{0}{0}{scipy}
\newlabel{eq:proba_density}{{10}{11}{\acrlongpl {DF}}{equation.3.10}{}}
\newlabel{eq:mv-gaussian}{{11}{11}{\acrlongpl {DF}}{equation.3.11}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of tree growth for a fictive dataset. Covariance ellipses are indicated in red lines and splitting lines in red dotted lines, indicating the dimension and value along which a node is split. Nodes are considered leaf nodes when no further split is necessary or possible, either because the Information Gain of a new split would be too low or because the maximum depth is reached. In a \acrlong {DF}, every tree would gets to see a subset of all points and fit slightly different leaf nodes.\relax }}{12}{figure.caption.11}}
\newlabel{fig:DF-split-visu}{{3}{12}{Illustration of tree growth for a fictive dataset. Covariance ellipses are indicated in red lines and splitting lines in red dotted lines, indicating the dimension and value along which a node is split. Nodes are considered leaf nodes when no further split is necessary or possible, either because the Information Gain of a new split would be too low or because the maximum depth is reached. In a \acrlong {DF}, every tree would gets to see a subset of all points and fit slightly different leaf nodes.\relax }{figure.caption.11}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \acrlong {DF} parameters and suggested parameter ranges\relax }}{13}{table.caption.12}}
\newlabel{table:df-parameters}{{2}{13}{\acrlong {DF} parameters and suggested parameter ranges\relax }{table.caption.12}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Datasets}{13}{section.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Synthetic Datasets}{13}{subsection.4.1}}
\newlabel{subsec:data-synthetic}{{4.1}{13}{Synthetic Datasets}{subsection.4.1}{}}
\abx@aux@segm{0}{0}{mnist}
\abx@aux@segm{0}{0}{mnist}
\abx@aux@segm{0}{0}{mnist}
\abx@aux@segm{0}{0}{Volpi2017DenseSL}
\abx@aux@segm{0}{0}{Volpi2015SemanticSO}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Synthetic datasets\relax }}{14}{figure.caption.13}}
\newlabel{fig:synthetic-datasets}{{4}{14}{Synthetic datasets\relax }{figure.caption.13}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\gls {MNIST} Dataset}{14}{subsection.4.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Sample images of the \gls {MNIST} dataset for true y labels 0 to 9 \cite {mnist}\relax }}{14}{figure.caption.14}}
\newlabel{fig:MNIST-Im}{{5}{14}{Sample images of the \gls {MNIST} dataset for true y labels 0 to 9 \cite {mnist}\relax }{figure.caption.14}{}}
\abx@aux@segm{0}{0}{Volpi2015SemanticSO}
\abx@aux@segm{0}{0}{Volpi2015SemanticSO}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Zurich Dataset}{15}{subsection.4.3}}
\newlabel{subsec:data-zurich}{{4.3}{15}{Zurich Dataset}{subsection.4.3}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Sample pair of images and ground truth for the Zurich dataset \cite {Volpi2015SemanticSO}\relax }}{15}{figure.caption.15}}
\newlabel{fig:zh-im-gt-pair}{{6}{15}{Sample pair of images and ground truth for the Zurich dataset \cite {Volpi2015SemanticSO}\relax }{figure.caption.15}{}}
\abx@aux@segm{0}{0}{mnist}
\abx@aux@cite{Srivastava2014DropoutAS}
\abx@aux@segm{0}{0}{Srivastava2014DropoutAS}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Label distribution in the Zurich dataset\relax }}{16}{figure.caption.16}}
\newlabel{fig:label-dist-zurich}{{7}{16}{Label distribution in the Zurich dataset\relax }{figure.caption.16}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Setup}{16}{section.5}}
\newlabel{sec:experimental-setup}{{5}{16}{Experimental Setup}{section.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Network Architectures}{16}{subsection.5.1}}
\newlabel{sec:network-architectures}{{5.1}{16}{Network Architectures}{subsection.5.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{\gls {MNIST} dataset}{16}{section*.17}}
\abx@aux@segm{0}{0}{ronneberger2015u}
\abx@aux@segm{0}{0}{ronneberger2015u}
\abx@aux@cite{Alom2018RecurrentRC}
\abx@aux@segm{0}{0}{Alom2018RecurrentRC}
\abx@aux@cite{Dong2017AutomaticBT}
\abx@aux@segm{0}{0}{Dong2017AutomaticBT}
\abx@aux@segm{0}{0}{ronneberger2015u}
\abx@aux@segm{0}{0}{ronneberger2015u}
\abx@aux@segm{0}{0}{ronneberger2015u}
\abx@aux@segm{0}{0}{ronneberger2015u}
\abx@aux@cite{Marmanis2016SemanticSO}
\abx@aux@segm{0}{0}{Marmanis2016SemanticSO}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Architecture of the \gls {CNN} used for \gls {MNIST} digit classification. $|b|$ = batch size, $p$ = dropout probability.\relax }}{17}{table.caption.18}}
\newlabel{table:CNN_MNIST}{{3}{17}{Architecture of the \gls {CNN} used for \gls {MNIST} digit classification. $|b|$ = batch size, $p$ = dropout probability.\relax }{table.caption.18}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Zurich dataset}{17}{section*.19}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces U-Net architecture, according to \textcite {ronneberger2015u}\relax }}{17}{figure.caption.20}}
\newlabel{fig:u-net}{{8}{17}{U-Net architecture, according to \textcite {ronneberger2015u}\relax }{figure.caption.20}{}}
\abx@aux@segm{0}{0}{Volpi2017DenseSL}
\abx@aux@segm{0}{0}{Volpi2017DenseSL}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Padded image and patches for a given patch size and stride. The big blue rectangle delimits the image area outside of which the original image is padded using mirroring. The rainbow-colored rectangles show patches with an overlap between pairs of patches defined by the stride. The central $stride \times stride$ solid-filled rectangles within each patch show the patch areas which are kept after prediction. The red dotted line shows the size of the image that is obtained after merging all predictions within the central areas of the patches. Finally, the thus obtained prediction image is cropped to the big blue box, denoting the original image size. Final predictions are obtained using several strides and averaging the results.\relax }}{18}{figure.caption.21}}
\newlabel{fig:im_padding}{{9}{18}{Padded image and patches for a given patch size and stride. The big blue rectangle delimits the image area outside of which the original image is padded using mirroring. The rainbow-colored rectangles show patches with an overlap between pairs of patches defined by the stride. The central $stride \times stride$ solid-filled rectangles within each patch show the patch areas which are kept after prediction. The red dotted line shows the size of the image that is obtained after merging all predictions within the central areas of the patches. Finally, the thus obtained prediction image is cropped to the big blue box, denoting the original image size. Final predictions are obtained using several strides and averaging the results.\relax }{figure.caption.21}{}}
\abx@aux@cite{scikit-learn}
\abx@aux@segm{0}{0}{scikit-learn}
\abx@aux@cite{Hinneburg2000WhatIT}
\abx@aux@segm{0}{0}{Hinneburg2000WhatIT}
\abx@aux@cite{Hinneburg1999OptimalGT}
\abx@aux@segm{0}{0}{Hinneburg1999OptimalGT}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Novelty Detection Baselines}{19}{subsection.5.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{MSR, Margin, Entropy}{19}{section*.22}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{MC-Dropout}{19}{section*.23}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Pre-softmax methods}{19}{section*.24}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Dimensionality Reduction and Data Separability}{19}{subsection.5.3}}
\newlabel{subsec:methodology-dim-reduction}{{5.3}{19}{Dimensionality Reduction and Data Separability}{subsection.5.3}{}}
\abx@aux@cite{Maaten2008VisualizingDU}
\abx@aux@segm{0}{0}{Maaten2008VisualizingDU}
\abx@aux@segm{0}{0}{Maaten2008VisualizingDU}
\newlabel{fig:pca_components_mnist}{{10a}{20}{\gls {MNIST} dataset, left-out class 8\relax }{figure.caption.25}{}}
\newlabel{sub@fig:pca_components_mnist}{{a}{20}{\gls {MNIST} dataset, left-out class 8\relax }{figure.caption.25}{}}
\newlabel{fig:pca_components_zurich}{{10b}{20}{Zurich dataset, left-out class roads\relax }{figure.caption.25}{}}
\newlabel{sub@fig:pca_components_zurich}{{b}{20}{Zurich dataset, left-out class roads\relax }{figure.caption.25}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Explained variance as a function of the number of components\relax }}{20}{figure.caption.25}}
\newlabel{fig:pca_components}{{10}{20}{Explained variance as a function of the number of components\relax }{figure.caption.25}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \gls {t-SNE} schema with toy data. \gls {t-SNE} finds a mapping between the original, high-dimensional data (left) and the data in a lower dimensionality (right). Classes of data points are shown in blue, red and green. Assuming that data points of the same class are more similar to each other, they will be closer to each other in the \gls {t-SNE} visualization.\relax }}{21}{figure.caption.26}}
\newlabel{fig:t-SNE-schema}{{11}{21}{\gls {t-SNE} schema with toy data. \gls {t-SNE} finds a mapping between the original, high-dimensional data (left) and the data in a lower dimensionality (right). Classes of data points are shown in blue, red and green. Assuming that data points of the same class are more similar to each other, they will be closer to each other in the \gls {t-SNE} visualization.\relax }{figure.caption.26}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Evaluation}{21}{subsection.5.4}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \acrlong {CM}: UA = User's Accuracy = Precision, PA = Producer's Accuracy = Recall, OA = Overall Accuracy, $1,2,\mathinner {\ldotp \ldotp \ldotp },r$=classes. $n_{ij}$ counts the number of labels predicted as class $i$ and belonging to the true class $j$. Bullet indexes signify either the sum of the row (e.g., $n_{O\bullet }$), the sum of the column (e.g., $n_{\bullet O}$) or the sum of all elements of the \acrlong {CM} ($n_{\bullet \bullet }$).\relax }}{22}{table.caption.27}}
\newlabel{table:cm}{{4}{22}{\acrlong {CM}: UA = User's Accuracy = Precision, PA = Producer's Accuracy = Recall, OA = Overall Accuracy, $1,2,\hdots ,r$=classes. $n_{ij}$ counts the number of labels predicted as class $i$ and belonging to the true class $j$. Bullet indexes signify either the sum of the row (e.g., $n_{O\bullet }$), the sum of the column (e.g., $n_{\bullet O}$) or the sum of all elements of the \acrlong {CM} ($n_{\bullet \bullet }$).\relax }{table.caption.27}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Hyperparameter Search}{22}{subsection.5.5}}
\newlabel{subsec:hyperparameter-search}{{5.5}{22}{Hyperparameter Search}{subsection.5.5}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \acrlong {DF} parameters for each dataset\relax }}{22}{table.caption.28}}
\newlabel{table:synthetic-parameters}{{5}{22}{\acrlong {DF} parameters for each dataset\relax }{table.caption.28}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Parameter Search: Gray boxes show activations belonging to data points of seen classes, red boxes activations belonging to data points of the unseen class. In principle, the true class label is unknown for the test set. For each hyperparameter combination, the classifier is trained \texttt  {n} times on a subset of the training set activations belonging only to seen classes (blue arrow) and evaluated on a subset of the seen and unseen points of the validation set (green arrows). The best hyperparameter combination found that way is applied to the entire test set (gray arrows), predicting for every point whether it belongs to a seen or unseen class.\relax }}{23}{figure.caption.29}}
\newlabel{fig:hyperparameter-search}{{12}{23}{Parameter Search: Gray boxes show activations belonging to data points of seen classes, red boxes activations belonging to data points of the unseen class. In principle, the true class label is unknown for the test set. For each hyperparameter combination, the classifier is trained \texttt {n} times on a subset of the training set activations belonging only to seen classes (blue arrow) and evaluated on a subset of the seen and unseen points of the validation set (green arrows). The best hyperparameter combination found that way is applied to the entire test set (gray arrows), predicting for every point whether it belongs to a seen or unseen class.\relax }{figure.caption.29}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6}Results}{23}{section.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Experiments}{23}{subsection.6.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Synthetic Data}{24}{subsection.6.2}}
\newlabel{subsec:results-synthetic}{{6.2}{24}{Synthetic Data}{subsection.6.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Covariance ellipses of individual Density Tree (subset of all points shown)\relax }}{24}{figure.caption.30}}
\newlabel{fig:gen-data}{{13}{24}{Covariance ellipses of individual Density Tree (subset of all points shown)\relax }{figure.caption.30}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Splitting steps of a single node, showing the data, covariance ellipses and information gain of the parent node for dataset 2\relax }}{25}{figure.caption.31}}
\newlabel{fig:D2-covs-steps}{{14}{25}{Splitting steps of a single node, showing the data, covariance ellipses and information gain of the parent node for dataset 2\relax }{figure.caption.31}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Gaussian \gls {PDF} distribution according to single tree, and according to a \acrlong {DF} consisting of 20 trees\relax }}{26}{figure.caption.32}}
\newlabel{fig:gen-data-heatmap}{{15}{26}{Gaussian \gls {PDF} distribution according to single tree, and according to a \acrlong {DF} consisting of 20 trees\relax }{figure.caption.32}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}\gls {MNIST}}{26}{subsection.6.3}}
\newlabel{subsec:results-MNIST}{{6.3}{26}{\gls {MNIST}}{subsection.6.3}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Mean Overall Accuracy in \% for the \gls {CNN} models trained on $N-1$ classes\relax }}{27}{table.caption.33}}
\newlabel{table:mnist-nd-accuracy-mean}{{6}{27}{Mean Overall Accuracy in \% for the \gls {CNN} models trained on $N-1$ classes\relax }{table.caption.33}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Predicted labels for networks with left-out classes 4 and 8 (top) and 1 and 7 (bottom). While digits showing a 4 are mostly mislabeled as a 9, the digit 8 is mislabeled less homogeneously. While one could suppose that digits 1 and 7 look similar and might be confused, digits 7 are mostly classified as digit 9, and digit 1 mostly as digits 4 and 8.\relax }}{28}{figure.caption.34}}
\newlabel{fig:pred-count-mnist}{{16}{28}{Predicted labels for networks with left-out classes 4 and 8 (top) and 1 and 7 (bottom). While digits showing a 4 are mostly mislabeled as a 9, the digit 8 is mislabeled less homogeneously. While one could suppose that digits 1 and 7 look similar and might be confused, digits 7 are mostly classified as digit 9, and digit 1 mostly as digits 4 and 8.\relax }{figure.caption.34}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces \gls {t-SNE} of \gls {MNIST} activations, model with left-out class 7. Both before and after \gls {PCA}, activations of different classes seem well separable, including the unseen class. \relax }}{29}{figure.caption.35}}
\newlabel{fig:tsne-mnist}{{17}{29}{\gls {t-SNE} of \gls {MNIST} activations, model with left-out class 7. Both before and after \gls {PCA}, activations of different classes seem well separable, including the unseen class. \relax }{figure.caption.35}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces t-SNE of \gls {MNIST} activations, after \gls {PCA} transformations.\relax }}{30}{figure.caption.36}}
\newlabel{fig:tsne-mnist-miscl}{{18}{30}{t-SNE of \gls {MNIST} activations, after \gls {PCA} transformations.\relax }{figure.caption.36}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Mean \gls {AUROC} for each left-out class in the \gls {MNIST} dataset\relax }}{30}{table.caption.37}}
\newlabel{table:mnist-auroc-nd-mean}{{7}{30}{Mean \gls {AUROC} for each left-out class in the \gls {MNIST} dataset\relax }{table.caption.37}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Zurich Dataset}{31}{subsection.6.4}}
\newlabel{subsec:results-zurich}{{6.4}{31}{Zurich Dataset}{subsection.6.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Overall Results}{31}{subsection.6.5}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Test set accuracy for the UNET \gls {CNN} trained on all classes (Overall Accuracy: 77.59 \%)\relax }}{31}{table.caption.38}}
\newlabel{table:zurich-cnn-acc-all}{{8}{31}{Test set accuracy for the UNET \gls {CNN} trained on all classes (Overall Accuracy: 77.59 \%)\relax }{table.caption.38}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Accuracy measures for the UNET \gls {CNN} trained on $N-1$ classes.\relax }}{31}{table.caption.39}}
\newlabel{table:zurich-cnn-acc-nd}{{9}{31}{Accuracy measures for the UNET \gls {CNN} trained on $N-1$ classes.\relax }{table.caption.39}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Prediction and ground truth for the model trained without the roads class\relax }}{32}{figure.caption.40}}
\newlabel{fig:pred-gt-road}{{19}{32}{Prediction and ground truth for the model trained without the roads class\relax }{figure.caption.40}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Predictions for network with left-out class ``roads'' and ``trees''. While roads are mostly mislabelled as buildings, trees are mislabelled less homogeneously.\relax }}{32}{figure.caption.41}}
\newlabel{fig:pred-count-zurich}{{20}{32}{Predictions for network with left-out class ``roads'' and ``trees''. While roads are mostly mislabelled as buildings, trees are mislabelled less homogeneously.\relax }{figure.caption.41}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces \gls {t-SNE} of Zurich dataset activations, model with left-out class buildings. The same number of points are shown by class, although the real class distribution is imbalanced (cf. table \ref  {table:zurich-cnn-acc-all}).\relax }}{33}{figure.caption.42}}
\newlabel{fig:tsne-zurich}{{21}{33}{\gls {t-SNE} of Zurich dataset activations, model with left-out class buildings. The same number of points are shown by class, although the real class distribution is imbalanced (cf. table \ref {table:zurich-cnn-acc-all}).\relax }{figure.caption.42}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces \gls {t-SNE} of Zurich dataset activations after \gls {PCA}.\relax }}{34}{figure.caption.43}}
\newlabel{fig:tsne-zurich-miscl}{{22}{34}{\gls {t-SNE} of Zurich dataset activations after \gls {PCA}.\relax }{figure.caption.43}{}}
\abx@aux@cite{Gonzlez2012DigitalIP}
\abx@aux@segm{0}{0}{Gonzlez2012DigitalIP}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces \gls {AUROC} for each left-out class\relax }}{35}{table.caption.44}}
\newlabel{table:zurich-auroc-nd}{{10}{35}{\gls {AUROC} for each left-out class\relax }{table.caption.44}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Visual interpretation}{35}{subsection.6.6}}
\newlabel{subfig:original-im}{{23c}{36}{Original confidence image\relax }{figure.caption.45}{}}
\newlabel{sub@subfig:original-im}{{c}{36}{Original confidence image\relax }{figure.caption.45}{}}
\newlabel{subfig:equalized-im}{{23d}{36}{Equalized confidence image\relax }{figure.caption.45}{}}
\newlabel{sub@subfig:equalized-im}{{d}{36}{Equalized confidence image\relax }{figure.caption.45}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Original and equalized confidence distributions for \gls {DF}, using the left-out class ``Roads''. While outliers are visible in the original figure, smaller confidence differences between classes are better visible after histogram equalization.\relax }}{36}{figure.caption.45}}
\newlabel{fig:hist-eq}{{23}{36}{Original and equalized confidence distributions for \gls {DF}, using the left-out class ``Roads''. While outliers are visible in the original figure, smaller confidence differences between classes are better visible after histogram equalization.\relax }{figure.caption.45}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Visual uncertainty results for selected methods on left-out class ``Roads'' and corresponding ground truth. Contrast stretching and histogram equalization have been applied to \gls {OC-SVM} and \gls {DF} images for better visibility. Variance per \gls {PCA} component and \gls {ROC} curves are shown below the confidence images.\relax }}{37}{figure.caption.46}}
\newlabel{fig:im_cert_1}{{24}{37}{Visual uncertainty results for selected methods on left-out class ``Roads'' and corresponding ground truth. Contrast stretching and histogram equalization have been applied to \gls {OC-SVM} and \gls {DF} images for better visibility. Variance per \gls {PCA} component and \gls {ROC} curves are shown below the confidence images.\relax }{figure.caption.46}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Visual uncertainty results for selected methods on left-out class ``Trees'' and corresponding ground truth. Contrast stretching and histogram equalization have been applied to \gls {OC-SVM} and \gls {DF} images for better visibility. Variance per \gls {PCA} component and \gls {ROC} curves are shown below the confidence images.\relax }}{39}{figure.caption.47}}
\newlabel{fig:im_cert_3}{{25}{39}{Visual uncertainty results for selected methods on left-out class ``Trees'' and corresponding ground truth. Contrast stretching and histogram equalization have been applied to \gls {OC-SVM} and \gls {DF} images for better visibility. Variance per \gls {PCA} component and \gls {ROC} curves are shown below the confidence images.\relax }{figure.caption.47}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Confidence for the left-out class ``Trees'' according to different methods plotted onto \gls {t-SNE}, showing the $n$ points with the lowest confidence where $n$ is the number of points per class shown in the \gls {t-SNE} plot. Points of the unseen classes are indicated with a solid-edge circle. Ideally, all solid-edge circles should be red, and all other points green. The original \gls {t-SNE} plot and the \gls {ROC} curves for each method are shown for comparison.\relax }}{41}{figure.caption.48}}
\newlabel{fig:t-SNE-probas_3}{{26}{41}{Confidence for the left-out class ``Trees'' according to different methods plotted onto \gls {t-SNE}, showing the $n$ points with the lowest confidence where $n$ is the number of points per class shown in the \gls {t-SNE} plot. Points of the unseen classes are indicated with a solid-edge circle. Ideally, all solid-edge circles should be red, and all other points green. The original \gls {t-SNE} plot and the \gls {ROC} curves for each method are shown for comparison.\relax }{figure.caption.48}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.7}Particular Objects}{42}{subsection.6.7}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Swimming pool object with MSR and \gls {DF} confidence images\relax }}{43}{figure.caption.49}}
\newlabel{fig:obj-im1}{{27}{43}{Swimming pool object with MSR and \gls {DF} confidence images\relax }{figure.caption.49}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Soccer pitch object with MSR and \gls {DF} confidence scores\relax }}{44}{figure.caption.50}}
\newlabel{fig:obj-im4}{{28}{44}{Soccer pitch object with MSR and \gls {DF} confidence scores\relax }{figure.caption.50}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{45}{section.7}}
\newlabel{sec:discussion}{{7}{45}{Discussion}{section.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}\gls {MNIST} dataset}{45}{subsection.7.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Dimensionality reduction}{45}{section*.51}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Network accuracy}{45}{section*.52}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Zurich dataset}{45}{subsection.7.2}}
\abx@aux@segm{0}{0}{Hinneburg2000WhatIT}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Class confusion}{46}{section*.53}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Gaussianity in high dimensions}{46}{section*.54}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Data separability}{46}{section*.55}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{46}{section.8}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Main Contributions}{47}{subsection.8.1}}
\abx@aux@segm{0}{0}{ronneberger2015u}
\abx@aux@cite{Berkeley2015HypercolumnsFO}
\abx@aux@segm{0}{0}{Berkeley2015HypercolumnsFO}
\abx@aux@segm{0}{0}{mandelbaum17}
\abx@aux@segm{0}{0}{mandelbaum17}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Future Research}{48}{subsection.8.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Influence of network architectures}{48}{section*.56}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Datasets}{48}{section*.57}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Meta-Loss to increase class separability}{48}{section*.58}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Parameter sensitivity}{48}{section*.59}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Further research ideas}{48}{section*.60}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Alternative confidence measure scheme: red parts are to be retrieved and multiplied to measure the degree of agreement of the softmax inputs.\relax }}{49}{figure.caption.61}}
\newlabel{fig:nn_scheme}{{29}{49}{Alternative confidence measure scheme: red parts are to be retrieved and multiplied to measure the degree of agreement of the softmax inputs.\relax }{figure.caption.61}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Evaluation}{49}{section*.62}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Conclusion}{49}{section*.63}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9}Appendices}{54}{section.9}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {A}Code Documentation}{54}{subsection.9.1}}
\newlabel{subsec:implementation}{{A}{54}{Code Documentation}{subsection.9.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {B}Random Forest}{54}{subsection.9.2}}
\newlabel{app:rf}{{B}{54}{Random Forest}{subsection.9.2}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces Decision boundaries of a single Decision Tree on 2-dimensional synthetic data, splitting the data until every leaf node only contains data of one cluster. Left: decision boundaries with Data, right: decision boundaries only. The Decision Tree clearly overfits the data.\relax }}{54}{figure.caption.65}}
\newlabel{fig:decision-boundaries}{{B.1}{54}{Decision boundaries of a single Decision Tree on 2-dimensional synthetic data, splitting the data until every leaf node only contains data of one cluster. Left: decision boundaries with Data, right: decision boundaries only. The Decision Tree clearly overfits the data.\relax }{figure.caption.65}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Decision tree with unlimited depth on the training data for shown for illustrative purposes, with the split dimension and value at every non-leaf node and the class label at every leaf node. The tree clearly overfits the data and produces edgy decision boundaries\relax }}{55}{figure.caption.66}}
\newlabel{fig:decision_tree}{{B.2}{55}{Decision tree with unlimited depth on the training data for shown for illustrative purposes, with the split dimension and value at every non-leaf node and the class label at every leaf node. The tree clearly overfits the data and produces edgy decision boundaries\relax }{figure.caption.66}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces Decision boundaries of a Random Forest on 2-dimensional synthetic data. 1000 Decision Trees have been trained on a 30\% bootstrap sample of the original data. Left: decision boundaries with Data, right: decision boundaries only. The Random Forest manages to smooth out the class decision boundaries.\relax }}{55}{figure.caption.67}}
\newlabel{fig:rf}{{B.3}{55}{Decision boundaries of a Random Forest on 2-dimensional synthetic data. 1000 Decision Trees have been trained on a 30\% bootstrap sample of the original data. Left: decision boundaries with Data, right: decision boundaries only. The Random Forest manages to smooth out the class decision boundaries.\relax }{figure.caption.67}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {C}Data Structure}{56}{subsection.9.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {C.1}{\ignorespaces Implemented data structure for Decision Tree nodes. Every node saves a pointer to its parent, the unique labels contained at its split level, the split dimension and value, methods for tree descending and formatting as well information about its child nodes.\relax }}{56}{figure.caption.68}}
\newlabel{fig:decision-node}{{C.1}{56}{Implemented data structure for Decision Tree nodes. Every node saves a pointer to its parent, the unique labels contained at its split level, the split dimension and value, methods for tree descending and formatting as well information about its child nodes.\relax }{figure.caption.68}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {C.2}{\ignorespaces Implemented data structure for Density Tree nodes. Every node saves a pointer to its parent, the unique labels contained at its split level, the split dimension and value, methods for tree descending and formatting as well information about its child nodes. In addition, every root node pre-stores the inverse and determinant of the covariance matrix of both clusters situated to the right and left of the node for faster calculation of the Gaussian \gls {PDF}.\relax }}{56}{figure.caption.69}}
\newlabel{fig:density-node}{{C.2}{56}{Implemented data structure for Density Tree nodes. Every node saves a pointer to its parent, the unique labels contained at its split level, the split dimension and value, methods for tree descending and formatting as well information about its child nodes. In addition, every root node pre-stores the inverse and determinant of the covariance matrix of both clusters situated to the right and left of the node for faster calculation of the Gaussian \gls {PDF}.\relax }{figure.caption.69}{}}
\abx@aux@segm{0}{0}{ronneberger2015u}
\abx@aux@segm{0}{0}{ronneberger2015u}
\abx@aux@segm{0}{0}{ronneberger2015u}
\abx@aux@segm{0}{0}{ronneberger2015u}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {D}\gls {MNIST} Evaluation Metrics}{57}{subsection.9.4}}
\newlabel{subsec:app-eval-mnist}{{D}{57}{\gls {MNIST} Evaluation Metrics}{subsection.9.4}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {D.1}{\ignorespaces Accuracy metrics in \% for the \gls {CNN} trained on $N-1$ classes for the \gls {MNIST} dataset\relax }}{57}{table.caption.70}}
\newlabel{table:mnist-nd-accuracy}{{D.1}{57}{Accuracy metrics in \% for the \gls {CNN} trained on $N-1$ classes for the \gls {MNIST} dataset\relax }{table.caption.70}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {D.2}{\ignorespaces \gls {AUROC} for each left-out class in the \gls {MNIST} dataset\relax }}{57}{table.caption.71}}
\newlabel{table:mnist-auroc-nd}{{D.2}{57}{\gls {AUROC} for each left-out class in the \gls {MNIST} dataset\relax }{table.caption.71}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {E}Zurich Network Architecture}{58}{subsection.9.5}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {E.1}{\ignorespaces U-Net Architecture of the \gls {CNN} used for Zurich Dataset, according to \textcite {ronneberger2015u}. Conv. = convolution, filt = filters, str = stride, $p$ = dropout probability, dim = dimensions, Input dimensions $|b|, w, h, n_c$ = batch size, width, height, number of channels. A convolution or transpose convolution always takes the previous layer in the network as input.\relax }}{58}{table.caption.72}}
\newlabel{table:CNN_Zurich}{{E.1}{58}{U-Net Architecture of the \gls {CNN} used for Zurich Dataset, according to \textcite {ronneberger2015u}. Conv. = convolution, filt = filters, str = stride, $p$ = dropout probability, dim = dimensions, Input dimensions $|b|, w, h, n_c$ = batch size, width, height, number of channels. A convolution or transpose convolution always takes the previous layer in the network as input.\relax }{table.caption.72}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {F}\gls {MNIST} Dataset Figures}{59}{subsection.9.6}}
\newlabel{app:mnist-figures}{{F}{59}{\gls {MNIST} Dataset Figures}{subsection.9.6}{}}
\newlabel{subfig:MNIST-pred_count-0}{{F.1a}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{sub@subfig:MNIST-pred_count-0}{{a}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{subfig:MNIST-pred_count-1}{{F.1b}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{sub@subfig:MNIST-pred_count-1}{{b}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{subfig:MNIST-pred_count-2}{{F.1c}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{sub@subfig:MNIST-pred_count-2}{{c}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{subfig:MNIST-pred_count-3}{{F.1d}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{sub@subfig:MNIST-pred_count-3}{{d}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{subfig:MNIST-pred_count-4}{{F.1e}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{sub@subfig:MNIST-pred_count-4}{{e}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{subfig:MNIST-pred_count-5}{{F.1f}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{sub@subfig:MNIST-pred_count-5}{{f}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{subfig:MNIST-pred_count-6}{{F.1g}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{sub@subfig:MNIST-pred_count-6}{{g}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{subfig:MNIST-pred_count-7}{{F.1h}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{sub@subfig:MNIST-pred_count-7}{{h}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{subfig:MNIST-pred_count-8}{{F.1i}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{sub@subfig:MNIST-pred_count-8}{{i}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{subfig:MNIST-pred_count-9}{{F.1j}{59}{Class \class \relax }{figure.caption.73}{}}
\newlabel{sub@subfig:MNIST-pred_count-9}{{j}{59}{Class \class \relax }{figure.caption.73}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {F.1}{\ignorespaces Count of predictions for each left-out class\relax }}{59}{figure.caption.73}}
\newlabel{fig:MNIST_pred_count_all}{{F.1}{59}{Count of predictions for each left-out class\relax }{figure.caption.73}{}}
\newlabel{subfig:MNIST-tsne-0}{{F.2a}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{sub@subfig:MNIST-tsne-0}{{a}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{subfig:MNIST-tsne-1}{{F.2b}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{sub@subfig:MNIST-tsne-1}{{b}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{subfig:MNIST-tsne-2}{{F.2c}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{sub@subfig:MNIST-tsne-2}{{c}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{subfig:MNIST-tsne-3}{{F.2d}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{sub@subfig:MNIST-tsne-3}{{d}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{subfig:MNIST-tsne-4}{{F.2e}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{sub@subfig:MNIST-tsne-4}{{e}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{subfig:MNIST-tsne-5}{{F.2f}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{sub@subfig:MNIST-tsne-5}{{f}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{subfig:MNIST-tsne-6}{{F.2g}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{sub@subfig:MNIST-tsne-6}{{g}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{subfig:MNIST-tsne-7}{{F.2h}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{sub@subfig:MNIST-tsne-7}{{h}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{subfig:MNIST-tsne-8}{{F.2i}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{sub@subfig:MNIST-tsne-8}{{i}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{subfig:MNIST-tsne-9}{{F.2j}{60}{Class \class \relax }{figure.caption.74}{}}
\newlabel{sub@subfig:MNIST-tsne-9}{{j}{60}{Class \class \relax }{figure.caption.74}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {F.2}{\ignorespaces \gls {t-SNE} of \gls {MNIST} dataset activations after \gls {PCA} transformations for each left-out class\relax }}{60}{figure.caption.74}}
\newlabel{fig:tsne-mnist-all-cl}{{F.2}{60}{\gls {t-SNE} of \gls {MNIST} dataset activations after \gls {PCA} transformations for each left-out class\relax }{figure.caption.74}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {G}Zurich Dataset Figures}{61}{subsection.9.7}}
\newlabel{app:zurich-figures}{{G}{61}{Zurich Dataset Figures}{subsection.9.7}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {G.1}{\ignorespaces Explained variance by first \gls {PCA} components, for activations of each left-out class and for activations of the model trained on all classes. The number of \gls {PCA} components was chosen such as to explain more than 95\% of the variance.\relax }}{61}{figure.caption.75}}
\newlabel{fig:pca-zurich-all-cl}{{G.1}{61}{Explained variance by first \gls {PCA} components, for activations of each left-out class and for activations of the model trained on all classes. The number of \gls {PCA} components was chosen such as to explain more than 95\% of the variance.\relax }{figure.caption.75}{}}
\newlabel{subfig:tsne-1}{{G.2a}{62}{\classname {}\relax }{figure.caption.76}{}}
\newlabel{sub@subfig:tsne-1}{{a}{62}{\classname {}\relax }{figure.caption.76}{}}
\newlabel{subfig:tsne-2}{{G.2b}{62}{\classname {}\relax }{figure.caption.76}{}}
\newlabel{sub@subfig:tsne-2}{{b}{62}{\classname {}\relax }{figure.caption.76}{}}
\newlabel{subfig:tsne-3}{{G.2c}{62}{\classname {}\relax }{figure.caption.76}{}}
\newlabel{sub@subfig:tsne-3}{{c}{62}{\classname {}\relax }{figure.caption.76}{}}
\newlabel{subfig:tsne-4}{{G.2d}{62}{\classname {}\relax }{figure.caption.76}{}}
\newlabel{sub@subfig:tsne-4}{{d}{62}{\classname {}\relax }{figure.caption.76}{}}
\newlabel{subfig:tsne-5}{{G.2e}{62}{\classname {}\relax }{figure.caption.76}{}}
\newlabel{sub@subfig:tsne-5}{{e}{62}{\classname {}\relax }{figure.caption.76}{}}
\newlabel{subfig:tsne-6}{{G.2f}{62}{\classname {}\relax }{figure.caption.76}{}}
\newlabel{sub@subfig:tsne-6}{{f}{62}{\classname {}\relax }{figure.caption.76}{}}
\newlabel{subfig:tsne-7}{{G.2g}{62}{\classname {}\relax }{figure.caption.76}{}}
\newlabel{sub@subfig:tsne-7}{{g}{62}{\classname {}\relax }{figure.caption.76}{}}
\newlabel{subfig:tsne-8}{{G.2h}{62}{\classname {}\relax }{figure.caption.76}{}}
\newlabel{sub@subfig:tsne-8}{{h}{62}{\classname {}\relax }{figure.caption.76}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {G.2}{\ignorespaces \gls {t-SNE} of Zurich dataset activations after \gls {PCA} transformations for each left-out class and for the activations of the network trained on all classes. The same number of points are shown by class to show class separability, although the real class distribution is imbalanced (cf. table \ref  {table:zurich-cnn-acc-all}).\relax }}{62}{figure.caption.76}}
\newlabel{fig:tsne-zurich-all-cl}{{G.2}{62}{\gls {t-SNE} of Zurich dataset activations after \gls {PCA} transformations for each left-out class and for the activations of the network trained on all classes. The same number of points are shown by class to show class separability, although the real class distribution is imbalanced (cf. table \ref {table:zurich-cnn-acc-all}).\relax }{figure.caption.76}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {G.3}{\ignorespaces Image and ground truth for visualized novelty detection methods\relax }}{63}{figure.caption.77}}
\newlabel{fig:app-im-gt-methods}{{G.3}{63}{Image and ground truth for visualized novelty detection methods\relax }{figure.caption.77}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {G.4}{\ignorespaces Ground truth and visual results for left-out class ``roads''.\relax }}{64}{figure.caption.78}}
\newlabel{fig:zurich-im-uncert-1}{{G.4}{64}{Ground truth and visual results for left-out class ``\classname ''.\relax }{figure.caption.78}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {G.5}{\ignorespaces Ground truth and visual results for left-out class ``buildings''.\relax }}{65}{figure.caption.79}}
\newlabel{fig:zurich-im-uncert-2}{{G.5}{65}{Ground truth and visual results for left-out class ``\classname ''.\relax }{figure.caption.79}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {G.6}{\ignorespaces Ground truth and visual results for left-out class ``trees''.\relax }}{66}{figure.caption.80}}
\newlabel{fig:zurich-im-uncert-3}{{G.6}{66}{Ground truth and visual results for left-out class ``\classname ''.\relax }{figure.caption.80}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {G.7}{\ignorespaces Ground truth and visual results for left-out class ``grass''.\relax }}{67}{figure.caption.81}}
\newlabel{fig:zurich-im-uncert-4}{{G.7}{67}{Ground truth and visual results for left-out class ``\classname ''.\relax }{figure.caption.81}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {G.8}{\ignorespaces Ground truth and visual results for left-out class ``bare soil''.\relax }}{68}{figure.caption.82}}
\newlabel{fig:zurich-im-uncert-5}{{G.8}{68}{Ground truth and visual results for left-out class ``\classname ''.\relax }{figure.caption.82}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {G.9}{\ignorespaces Ground truth and visual results for left-out class ``water''.\relax }}{69}{figure.caption.83}}
\newlabel{fig:zurich-im-uncert-6}{{G.9}{69}{Ground truth and visual results for left-out class ``\classname ''.\relax }{figure.caption.83}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {G.10}{\ignorespaces Ground truth and visual results for left-out class ``railways''.\relax }}{70}{figure.caption.84}}
\newlabel{fig:zurich-im-uncert-7}{{G.10}{70}{Ground truth and visual results for left-out class ``\classname ''.\relax }{figure.caption.84}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {G.11}{\ignorespaces Ground truth and visual results for left-out class ``swimming pools''.\relax }}{71}{figure.caption.85}}
\newlabel{fig:zurich-im-uncert-8}{{G.11}{71}{Ground truth and visual results for left-out class ``\classname ''.\relax }{figure.caption.85}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {G.12}{\ignorespaces \gls {ROC} curves of confidence measures for novelty detection and for error detection\relax }}{72}{figure.caption.86}}
\newlabel{fig:zurich-nd-roc}{{G.12}{72}{\gls {ROC} curves of confidence measures for novelty detection and for error detection\relax }{figure.caption.86}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {H}Hyperparameter Search Results}{73}{subsection.9.8}}
\newlabel{app:hyperparameters}{{H}{73}{Hyperparameter Search Results}{subsection.9.8}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {H.1}{\ignorespaces Best hyperparameters for the \gls {MNIST} Dataset\relax }}{73}{table.caption.87}}
\newlabel{table:hyperparameters-results-mnist}{{H.1}{73}{Best hyperparameters for the \gls {MNIST} Dataset\relax }{table.caption.87}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {H.2}{\ignorespaces Best hyperparameters for the Zurich Dataset\relax }}{74}{table.caption.88}}
\newlabel{table:hyperparameters-results-zurich}{{H.2}{74}{Best hyperparameters for the Zurich Dataset\relax }{table.caption.88}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {H.1}{\ignorespaces RBF Kernel visualizations for One-Class SVM in Zurich dataset. Kernels were applied to a class-balanced subsample of training activations belonging to the seen classes. Best kernels found using hyperparameter search are labelled in bold.\relax }}{74}{figure.caption.89}}
\newlabel{fig:oc-svm-vis-rbf}{{H.1}{74}{RBF Kernel visualizations for One-Class SVM in Zurich dataset. Kernels were applied to a class-balanced subsample of training activations belonging to the seen classes. Best kernels found using hyperparameter search are labelled in bold.\relax }{figure.caption.89}{}}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{Alom2018RecurrentRC}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Bahat_2018}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Beghi2014AOS}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Bishop1994NoveltyDA}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Breiman1996BaggingP}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Breiman2001}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Breunig2000LOFID}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Berkeley2015HypercolumnsFO}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Choi2017UncertaintyAwareLF}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{decisionForests-MSR}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Gonzlez2012DigitalIP}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Dong2017AutomaticBT}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Gal2016Uncertainty}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Gal2015Dropout}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{ghahramani}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Goodfellow2014}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Hammer2007HowTP}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{HendrycksG16c}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Hinneburg2000WhatIT}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Hinneburg1999OptimalGT}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{kampffmeyer}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Kampffmeyer2016SemanticSO}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{KendallG17}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Lakshminarayanan16}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{mnist}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{leibig2017}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Levinson2011TowardsFA}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Liu2008IsolationF}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Maaten2008VisualizingDU}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{mandelbaum17}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Markou2003NoveltyDApt1}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Markou2003NoveltyDApt2}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Marmanis2016SemanticSO}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{menderes_automatic_2015}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{deMorsier2014thesis}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{NguyenYC14}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{ouerghemmi_two-step_2017}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{scikit-learn}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Pimentel2014ARO}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{postadjian_investigating_2017}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Reynolds2009GaussianMM}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{ronneberger2015u}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Rupprecht2017LearningIA}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Schmidhuber2015DeepLI}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{scipy}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Shelhamer2015FullyCN}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Srivastava2014DropoutAS}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{subramanya}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Sun2018KSconfA}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Szymanski2011VisualisingKS}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{tuiaAL2011}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Tuia2011ASO}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Volpi2015SemanticSO}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Volpi2017DenseSL}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Wang2004AnomalyID}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{CW2017}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{womble_automated_2007}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{zaragoza}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Zhu2017DeepLI}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{Zoubir2007BootstrapMA}{nyt/global//global/global}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {H.2}{\ignorespaces Polynomial kernel visualizations for One-Class SVM in Zurich dataset with best degree $r$. Kernels were applied to a class-balanced subsample of training activations belonging to the seen classes. Contrast stretching has been applied to the images of the polynomial kernels to highlight more local variation. Best kernels found using hyperparameter search are labelled in bold.\relax }}{75}{figure.caption.90}}
\newlabel{fig:oc-svm-vis-poly}{{H.2}{75}{Polynomial kernel visualizations for One-Class SVM in Zurich dataset with best degree $r$. Kernels were applied to a class-balanced subsample of training activations belonging to the seen classes. Contrast stretching has been applied to the images of the polynomial kernels to highlight more local variation. Best kernels found using hyperparameter search are labelled in bold.\relax }{figure.caption.90}{}}
