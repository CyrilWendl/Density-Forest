{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "import os\n",
    "from skimage import io\n",
    "from skimage import exposure\n",
    "import natsort as ns\n",
    "import multiprocessing\n",
    "from tqdm import tqdm_notebook\n",
    "from skimage.util import *\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "\n",
    "im_dir = r''+ path + '/Zurich_dataset/images_tif/'\n",
    "gt_dir = r''+ path + '/Zurich_dataset/groundtruth/'\n",
    "\n",
    "im_names = ns.natsorted(os.listdir(im_dir))\n",
    "gt_names = ns.natsorted(os.listdir(gt_dir))\n",
    "print(\"images: %i \" % len(im_names))\n",
    "print(\"ground truth images: %i \" % len(gt_names))\n",
    "\n",
    "def im_load(path, max_size = 600): # for now, only return highest [max_size] pixels, multiple of patch_size\n",
    "    \"\"\"load a TIF image\"\"\"\n",
    "    image = np.asarray(io.imread(path)).astype(float)\n",
    "    #print(image.shape)\n",
    "    return np.asarray(image[:max_size,:max_size,:])\n",
    "\n",
    "\n",
    "def get_im_patches(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = 512\n",
    "imgs = np.asarray([im_load(im_dir + im_name, max_size = max_size) for im_name in im_names])\n",
    "gt = np.asarray([im_load(gt_dir + gt_name, max_size = max_size) for gt_name in gt_names])\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaded a set of images\n",
    "def imgs_stretch_eq(imgs):\n",
    "    imgs_stretch, imgs_eq = [], []\n",
    "    for im in imgs:\n",
    "        # Contrast stretching\n",
    "        p2, p98 = np.percentile(im, (2, 98))\n",
    "        img_stretch = im.copy()\n",
    "        img_eq = im.copy()\n",
    "        for band in range(im.shape[-1]):\n",
    "            img_stretch[:,:,band] = exposure.rescale_intensity(im[:,:,band], in_range=(p2, p98))\n",
    "            img_eq[:,:,band] = exposure.equalize_hist(img_stretch[:,:,band])\n",
    "        \n",
    "        # append images\n",
    "        imgs_stretch.append(img_stretch)\n",
    "        imgs_eq.append(img_eq)\n",
    "        \n",
    "    # convert to np arrays\n",
    "    imgs_stretch = np.asarray(imgs_stretch)\n",
    "    imgs_eq = np.asarray(imgs_eq)\n",
    "    return imgs_stretch, imgs_eq \n",
    "\n",
    "imgs_stretch, imgs_eq = imgs_stretch_eq(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show image, its groundtruth image and overlay (to verify matching)\n",
    "i = 1\n",
    "alpha=.6\n",
    "overlay = imgs_eq[i][:,:,:3]*alpha+gt[i]/255*(1-alpha)\n",
    "\n",
    "fig, axes = plt.subplots(1,3)\n",
    "fig.set_size_inches(15,7)\n",
    "axes[0].imshow(imgs_eq[i][:,:,:3], cmap='Greys_r')\n",
    "axes[1].imshow(gt[i]/255, cmap='Greys_r')\n",
    "axes[2].imshow(overlay, cmap='Greys_r')\n",
    "axes[0].set_title(\"Stretched image\")\n",
    "axes[1].set_title(\"Ground truth\")\n",
    "axes[2].set_title(\"Ground truth (overlay)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = imgs_eq # continue using stretched image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(imgs))\n",
    "print(np.shape(gt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GT to labels\n",
    "Next, we need to convert the ground truth (colors) to labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get label corresponding to each color\n",
    "from collections import OrderedDict\n",
    "\n",
    "legend = OrderedDict((('Background',[255, 255, 255]),\n",
    "          ('Roads',[0, 0, 0]),\n",
    "          ('Buildings',[100, 100, 100]),\n",
    "          ('Trees',[0, 125, 0]),\n",
    "          ('Grass',[0, 255, 0]),\n",
    "          ('Bare Soil',[150, 80, 0]),\n",
    "          ('Water',[0, 0, 150]),\n",
    "          ('Railways',[255, 255, 0]),\n",
    "          ('Swimming Pools',[150, 150, 255])))\n",
    "\n",
    "# get class names by increasing value (as done above)\n",
    "names, colors = [], []\n",
    "for name, color in legend.items():\n",
    "    names.append(name)\n",
    "    colors.append(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_color_to_label(gt, maj = False):\n",
    "    \"\"\"\n",
    "    Transform a set of GT image in value range [0, 255] of shape (n_images, width, height, 3) \n",
    "    to a set of GT labels of shape (n_images, width, height)\n",
    "    \"\"\"\n",
    "    \n",
    "    # sum of distinct color values\n",
    "    gt_new = np.zeros(np.asarray(gt).shape[:-1])\n",
    "\n",
    "    # replace colors by new values\n",
    "    for i in range(len(colors)):\n",
    "        gt_new[np.all(gt==colors[i],axis=-1)] = i #np.argsort(colors)[i]    \n",
    "        \n",
    "    if maj:\n",
    "        # return only majority label for each patch\n",
    "        gt_maj_label = []\n",
    "        for i in range(len(gt)):\n",
    "            counts = np.bincount(gt_new[i].flatten())\n",
    "            gt_maj_label.append(np.argmax(counts))\n",
    "\n",
    "        gt_new = np.asarray([gt_maj_label]).T\n",
    "        \n",
    "    return gt_new\n",
    "\n",
    "def gt_label_to_color(gt):\n",
    "    \"\"\"\n",
    "    Transform a set of GT labels of shape (n_images, width, height)\n",
    "    to a set of GT images in value range [0,1] of shape (n_images, width, height, 3) \"\"\"\n",
    "    gt_new = np.zeros(gt.shape+(3,))\n",
    "    for i in range(len(colors)): # loop colors\n",
    "        gt_new[gt == i,:] = np.divide(colors[i],255)\n",
    "    return gt_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_maj_label = gt_color_to_label(gt)\n",
    "print(\"Unique Labels in GT: \", np.unique(gt_maj_label))\n",
    "print(np.shape(gt_maj_label))\n",
    "gt = gt_maj_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_colors = gt_label_to_color(gt_maj_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gt_colors[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_patches(images, patch_size = 16, window_size = 64):\n",
    "    \"\"\"\n",
    "    get padded (mirror) patches for all images\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    for im in tqdm_notebook(images):\n",
    "        patches_im = np.zeros([int(im.shape[0]/patch_size),int(im.shape[0]/patch_size), window_size, window_size, im.shape[2]])    \n",
    "        for i in range(np.shape(im)[2]):\n",
    "            padded = np.lib.pad(im[:,:,i], int(np.floor((window_size-patch_size)/2)), 'reflect')    \n",
    "            patches_im[:,:,:,:,i] = view_as_windows(padded, window_size, step=patch_size)\n",
    "            \n",
    "        n_patches = int((im.shape[0]/patch_size)**2) # 25*25 = 625 per image\n",
    "        patches_im = np.reshape(patches_im, (n_patches, window_size, window_size, im.shape[2]))\n",
    "\n",
    "        # TODO reshape patches (first dim)\n",
    "        patches.append(patches_im)\n",
    "    patches = np.array(patches)\n",
    "    patches =  np.asarray([patches[i][j] for i in range(len(patches)) for j in range(len(patches[i]))])\n",
    "    #patches = np.concatenate(patches, axis = 0)\n",
    "    return patches\n",
    "\n",
    "def get_gt_patches(images_gt, patch_size = 16):\n",
    "    \"\"\"\n",
    "    get ground truth patches for all images\n",
    "    \"\"\"\n",
    "    gt_patches = []\n",
    "    for im in tqdm_notebook(images_gt):\n",
    "        patches_im_gt = view_as_blocks(im, block_shape=(patch_size, patch_size))         \n",
    "        n_patches = int((im.shape[0]/patch_size)**2) # 25*25 = 625 per image\n",
    "        patches_im_gt = np.reshape(patches_im_gt, (n_patches, patch_size, patch_size))\n",
    "\n",
    "        gt_patches.append(patches_im_gt)\n",
    "    gt_patches = np.array(gt_patches)\n",
    "    gt_patches =  np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "    #patches = np.concatenate(patches, axis = 0)\n",
    "    return np.asarray(gt_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imgs.shape)\n",
    "print(gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patch_size = 64\n",
    "im_patches = get_padded_patches(imgs, patch_size = patch_size, window_size = patch_size)\n",
    "gt_patches = get_gt_patches(gt, patch_size = patch_size)\n",
    "print(im_patches.shape)\n",
    "print(gt_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_patches_color=gt_label_to_color(gt_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_row = 8\n",
    "fig, axes = plt.subplots(3,imgs_row)\n",
    "fig.set_size_inches(20,8)\n",
    "offset = 10\n",
    "alpha=.3\n",
    "for i in range(offset, offset + imgs_row):\n",
    "    axes[0][i-offset].imshow(im_patches[i][:,:,:3])\n",
    "    axes[1][i-offset].imshow(gt_label_to_color(gt_patches[i]))\n",
    "    axes[2][i-offset].imshow(gt_label_to_color(gt_patches[i])*alpha+im_patches[i][:,:,:3]*(1-alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print some statistics for the class distribution\n",
    "unique, counts = np.unique(gt,return_counts=True)\n",
    "print(unique, counts)\n",
    "fig, ax = plt.subplots(1,1)\n",
    "fig.set_size_inches(10,5)\n",
    "ax.scatter(unique,counts)\n",
    "ax.set_title('number of samples per class')\n",
    "ax.set_xticks(np.arange(len(names)))\n",
    "ax.set_xticklabels(names)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "import keras.losses\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from keras.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val  = train_test_split(im_patches, gt_patches, test_size=.2, random_state = 42)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train.flatten()), y_train.flatten())    \n",
    "class_weights[0] = 0 # give less weight to background label class\n",
    "class_weights[5] = 1 # give less weight to swimming pool class\n",
    "class_weights[8] = 1 # give less weight to swimming pool class\n",
    "print(\"Class weights:\")\n",
    "for i, w in enumerate(class_weights):\n",
    "    print(\"%15s: %3.3f\"%(names[i],w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_classes = 9\n",
    "\n",
    "# convert to numpy arrays\n",
    "x_train = np.asarray(x_train)\n",
    "x_val = np.asarray(x_val)\n",
    "\n",
    "# make y data categorical\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_val = to_categorical(y_val, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print shapes of variables\n",
    "for var in x_train, y_train, x_val, y_val:\n",
    "    print(np.shape(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(n_classes = 9,input_shape=x_train.shape[1:]):\n",
    "    \"\"\"get UNET model instance\"\"\"\n",
    "    \n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Dropout(.1)(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Dropout(.1)(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Dropout(.1)(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Dropout(.1)(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Dropout(.1)(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Dropout(.1)(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Dropout(.1)(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = Dropout(.1)(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = Dropout(.1)(conv9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "    conv10 = Conv2D(n_classes, (1, 1), activation='softmax')(conv9)\n",
    "    #conv10 = core.Activation('softmax')(conv10)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[conv10])\n",
    "    \n",
    "    sgd = optimizers.SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.acc = []\n",
    "        self.val_acc = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))\n",
    "\n",
    "# callbacks (evaluated every epoch)\n",
    "# keep track of the accuracy over epochs\n",
    "callback_history = AccuracyHistory()\n",
    "\n",
    "# stop early if after several epochs the accuracy doesn't improive\n",
    "callback_earlystop = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=24, verbose=1, mode='auto')\n",
    "\n",
    "# decrease learning rate when accuracy stops improving\n",
    "callback_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=12, verbose=1, mode='auto', \n",
    "                                epsilon=1e-4, cooldown=0, min_lr=1e-8)\n",
    "\n",
    "# checkpoint to save weights at every epoch (in case of interruption)\n",
    "filepath=\"weights-improvement.hdf5\"\n",
    "callback_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will do preprocessing and realtime data augmentation:\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,              # Set input mean to 0 over the dataset, feature-wise.\n",
    "    featurewise_std_normalization=True,   # Divide inputs by std of the dataset, feature-wise.\n",
    "    #shear_range=0.2,\n",
    "    #zoom_range=0.2,\n",
    "    rotation_range=.5,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "\n",
    "batch_size = 10\n",
    "epochs = 300\n",
    "\n",
    "def model_train(model, data_augmentation = True):\n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              class_weight = class_weights,\n",
    "              validation_data = (x_val, y_val),\n",
    "              callbacks=[callback_history, \n",
    "                         callback_earlystop, \n",
    "                         callback_lr,\n",
    "                         callback_checkpoint])\n",
    "    else:\n",
    "        print('Using real-time data augmentation.')\n",
    "\n",
    "        # Fit the model on the batches generated by datagen.flow().\n",
    "        model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                         batch_size=batch_size),\n",
    "                            steps_per_epoch=int(np.ceil(x_train.shape[0] / float(batch_size))),\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            class_weight = class_weights, # weights for loss function\n",
    "                            validation_data=(x_val, y_val),\n",
    "                            callbacks=[callback_history, \n",
    "                                       callback_earlystop,\n",
    "                                       callback_lr,\n",
    "                                       callback_checkpoint],\n",
    "                            workers=multiprocessing.cpu_count()-1,\n",
    "                            use_multiprocessing=True)\n",
    "        # TODO ignore background\n",
    "\n",
    "# train the model\n",
    "model = get_unet()\n",
    "#print(model.summary())\n",
    "model.load_weights('weights-improvement.hdf5')\n",
    "#model_train(model, data_augmentation = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy evolution over epochs\n",
    "def plot_train_model(epochs):\n",
    "    plt.plot(range(1,epochs+1), callback_history.acc, '.-', range(1,epochs+1), callback_history.val_acc, '.-')\n",
    "    plt.xlabel('Epochs', )\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['accuracy', 'validation accuracy'])\n",
    "    plt.show()\n",
    "\n",
    "plot_train_model(275)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('model_unet.h5')  # save model, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_val, y_val, batch_size = 1, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_val, batch_size = 5, verbose = 1)\n",
    "y_pred_label = np.argmax(y_pred,axis=-1)\n",
    "y_val_labels = np.argmax(y_val,axis=-1) # test gt, converting back to labels for figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy as margin between highest and second highest class\n",
    "y_pred_rank = np.sort(y_pred,axis=-1) # for every pixel, get the rank\n",
    "y_pred_max1 = y_pred_rank[:,:,:,-1] # second highest proba\n",
    "y_pred_max2 = y_pred_rank[:,:,:,-2] # second highest proba\n",
    "y_pred_acc = y_pred_max1 - y_pred_max2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot prediction results\n",
    "im_idx = 5\n",
    "alpha = .3 # for overlay\n",
    "fig, axes = plt.subplots(1,6)\n",
    "fig.set_size_inches(20,20)\n",
    "fig_im = x_val[im_idx][:,:,:3]*(1-alpha)\n",
    "fig_val = gt_label_to_color(y_val_labels[im_idx])\n",
    "fig_pred = gt_label_to_color(y_pred_label[im_idx])\n",
    "\n",
    "# plots\n",
    "axes[0].imshow(fig_im)\n",
    "axes[1].imshow(fig_val)\n",
    "axes[2].imshow(fig_val*alpha+fig_im*(1-alpha))\n",
    "axes[3].imshow(fig_pred)\n",
    "axes[4].imshow(fig_pred*alpha+fig_im*(1-alpha))\n",
    "axes[5].imshow(fig_im*0+1)\n",
    "\n",
    "# titles\n",
    "axes[0].set_title(\"Test image\")\n",
    "axes[1].set_title(\"Ground truth\")\n",
    "axes[2].set_title(\"Ground truth (overlay)\")\n",
    "axes[3].set_title(\"Predicted Image\")\n",
    "axes[4].set_title(\"Predicted Image (overlay)\")\n",
    "axes[5].set_title(\"Legend\")\n",
    "\n",
    "# legend\n",
    "legend_data = [[l[0],l[1]] for l in legend.items()]\n",
    "handles = [Rectangle((0,0),1,1, color = (v/255 for v in c)) for n,c in legend_data]\n",
    "labels = [n for n,c in legend_data]\n",
    "axes[5].legend(handles,labels)\n",
    "\n",
    "# show certitude by network\n",
    "fig = plt.figure()\n",
    "plt.imshow(y_pred_acc[im_idx], cmap='gray')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation accuracy\n",
    "print(\"Validation accuracy: %.3f\" % sklearn.metrics.accuracy_score(y_pred_label.flatten(),y_val_labels.flatten()))\n",
    "print(sklearn.metrics.classification_report(y_pred_label.flatten(),y_val_labels.flatten(),target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certainty using Density Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/keras-team/keras/issues/41\n",
    "from keras import backend as K\n",
    "\n",
    "def get_activations(model, layer, X_batch):\n",
    "    get_activations = K.function([model.layers[0].input, K.learning_phase()], [model.layers[layer].output,])\n",
    "    activations = get_activations([X_batch,0])\n",
    "    return np.asarray(activations[0])\n",
    "\n",
    "activations = get_activations(model, -2, x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some activations\n",
    "fig, axes = plt.subplots(4,8)\n",
    "fig.set_size_inches(20,10)\n",
    "for i in range(4):\n",
    "    for j in range(8):\n",
    "        axes[i][j].imshow(activations[im_idx][:,:,i*8+j])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(x_val[im_idx][:,:,:3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Density Tree\n",
    "Feed activation weigths into density tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/cyrilwendl/SIE-Master/Code') # Path to density Tree package\n",
    "sys.path.append('/home/cyrilwendl/SIE-Master/Code/density_tree') # Path to density Tree package\n",
    "\n",
    "from density_tree import density_tree\n",
    "from density_tree.density_tree_create import create_density_tree\n",
    "from sklearn import decomposition\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_2D = np.reshape(activations, (activations.shape[0]*activations.shape[1]*activations.shape[2],activations.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create density tree for activation weights of training data\n",
    "\n",
    "dimensions = np.shape(activations_2D)[-1]\n",
    "nclusters = 9\n",
    "\n",
    "# PCA\n",
    "n_components = 8\n",
    "plt.cla()\n",
    "pca = decomposition.PCA(n_components=n_components)\n",
    "pca.fit(activations_2D)\n",
    "print(\"Explained variance ratio by first components:\")\n",
    "print(pca.explained_variance_ratio_)  \n",
    "print(\"Variance explained by first %i components: %.2f\" % (n_components, np.sum(pca.explained_variance_ratio_[:n_components])))\n",
    "plt.scatter(np.arange(n_components), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cumulative sum of explained variance\")\n",
    "plt.grid()\n",
    "plt.savefig(\"../Figures/pca_components.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = pca.transform(activations_2D)\n",
    "#X_seen = pca.transform(activations_seen_classes)\n",
    "#X_unseen = pca.transform(activations_unseen_class)\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "#root = create_density_tree(X_seen, dimensions = dimensions, clusters = nclusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_1D = np.reshape(y_val_labels,(y_val_labels.shape[0]*y_val_labels.shape[1]*y_val_labels.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook # for interactive Jupyter Notebook\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "n_points = 2000\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_all[:n_points,0], X_all[:n_points,1], zs=X_all[:n_points,2], c=y_val_1D[:n_points], cmap=plt.get_cmap('rainbow'), s=15)\n",
    "#ax.scatter(X_unseen[:,0], X_unseen[:,1], zs=X_unseen[:,2], c='black', s=30, marker='x', depthshade=False) \n",
    "ax.legend(['seen classes','unseen class'])\n",
    "\n",
    "#plt.savefig(\"../Figures/pca_components_3d.pdf\", bbox_inches='tight', pad_inches=0)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variance explained by first 3 components: %.2f\" % np.sum(pca.explained_variance_ratio_[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "root = create_density_tree(X_all, dimensions = n_components, clusters = nclusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "02f1a23c8fd947ae80614472e5d29f76": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "19fc97a440c5439ba7f4e5b53638c7e6": {
     "views": []
    },
    "1e4d1071b9aa42c3b3e7ee1080d42b75": {
     "views": []
    },
    "31364b1586914b1f8304b44af16dde6b": {
     "views": []
    },
    "3636765aeb4849fba391a21442b10fe3": {
     "views": []
    },
    "376269c1ebaa4f3aa7781a88b9fb7737": {
     "views": []
    },
    "3d50bf1756834694a9b13878ab5862b7": {
     "views": []
    },
    "59dfe880d74a41ecbc61a569a9deabf4": {
     "views": []
    },
    "5be46b35b4f54acfae4dba6723d2b960": {
     "views": []
    },
    "631f90513383421991559799e1313e4e": {
     "views": []
    },
    "6b21fbab71cd4f809d9b141a85551fb8": {
     "views": []
    },
    "6b7b28877bdf4a2db41c41ca3d5eae5b": {
     "views": []
    },
    "742adc8b93ab40e0855d57b91405da5c": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "75f8855e4b03422ba7053746db529e8b": {
     "views": []
    },
    "7f6bea79e11244699b30fa0a57441217": {
     "views": []
    },
    "83d53c3827f14ba596b6617cb358e568": {
     "views": []
    },
    "96bed79dcf394798ba93ead6f1486d29": {
     "views": []
    },
    "a42c164a85864f5cbf42db870a96a24e": {
     "views": []
    },
    "a4bf03f6f0734efe8c5b41512124b56c": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "aadbaf84bc714856a87965f804472920": {
     "views": []
    },
    "b8ffa2c830b5466ba1b7906a43d5326c": {
     "views": []
    },
    "d0809adb30474329b33020ca476d236d": {
     "views": []
    },
    "d5b90b8879484282ba08d045e9ad4ac9": {
     "views": []
    },
    "e6864e4c85664f92ae0f56d87e6ebc24": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
