{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zurich Land Cover Classification\n",
    "\n",
    "This script presents a visualization of training a U-Net classifier on 7 out of 8 available land cover classes of the Zurich dataset, and detecting the unseen class using a Density Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14344233999815580467\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10919205274\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10593941837647837258\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# python libraries\n",
    "import os, sys\n",
    "from multiprocessing import cpu_count\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "from sklearn import decomposition, svm, preprocessing\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn import metrics\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# custom libraries\n",
    "# base_dir = '/Users/cyrilwendl/Documents/EPFL'\n",
    "base_dir = '/raid/home/cwendl'  # for guanabana\n",
    "sys.path.append(base_dir + '/SIE-Master/Code')  # Path to density Tree package\n",
    "\n",
    "from helpers.helpers import *\n",
    "from helpers.plots import *\n",
    "from helpers.data_augment import *\n",
    "from helpers.data_loader import *\n",
    "from helpers.parameter_search import *\n",
    "from density_forest.density_forest import *\n",
    "from density_forest.helpers import *\n",
    "from baselines.helpers import *\n",
    "from keras_helpers.unet import *\n",
    "from keras_helpers.callbacks import *\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # sys.argv[2]\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "class_to_remove = 1  # int(sys.argv[1])\n",
    "paramsearch = False  # search for best hyperparameters\n",
    "\n",
    "my_dpi=255 # dpi of my screen, for image exporting\n",
    "\n",
    "# data frame with previously found optimal hyperparameters\n",
    "df_ps = pd.read_csv('models_out/hyperparams.csv', index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gmm_n_components</th>\n",
       "      <th>oc-svm_k</th>\n",
       "      <th>oc-svm_deg</th>\n",
       "      <th>oc_svm_nu</th>\n",
       "      <th>df_depth</th>\n",
       "      <th>df_min_ig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Roads</th>\n",
       "      <td>3</td>\n",
       "      <td>poly</td>\n",
       "      <td>2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Buildings</th>\n",
       "      <td>5</td>\n",
       "      <td>poly</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trees</th>\n",
       "      <td>4</td>\n",
       "      <td>poly</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grass</th>\n",
       "      <td>3</td>\n",
       "      <td>poly</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bare Soil</th>\n",
       "      <td>9</td>\n",
       "      <td>poly</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Water</th>\n",
       "      <td>3</td>\n",
       "      <td>poly</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Swimming Pools</th>\n",
       "      <td>3</td>\n",
       "      <td>rbf</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Railways</th>\n",
       "      <td>9</td>\n",
       "      <td>poly</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500</td>\n",
       "      <td>3</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                gmm_n_components oc-svm_k  oc-svm_deg  oc_svm_nu  df_depth  \\\n",
       "Roads                          3     poly           2      0.001         1   \n",
       "Buildings                      5     poly           1      0.001         3   \n",
       "Trees                          4     poly           1      0.010         1   \n",
       "Grass                          3     poly           1      0.001         3   \n",
       "Bare Soil                      9     poly           1      0.500         1   \n",
       "Water                          3     poly           1      0.001         1   \n",
       "Swimming Pools                 3      rbf           1      0.500         3   \n",
       "Railways                       9     poly           3      0.500         3   \n",
       "\n",
       "                df_min_ig  \n",
       "Roads                -inf  \n",
       "Buildings            -inf  \n",
       "Trees            0.700000  \n",
       "Grass            0.000000  \n",
       "Bare Soil        0.000000  \n",
       "Water            0.000000  \n",
       "Swimming Pools   0.700000  \n",
       "Railways         0.700000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "loading data with overlap\n",
      "classes to keep: ['Buildings' 'Trees' 'Grass' 'Bare Soil' 'Water' 'Railways']\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "\n",
    "# data without overlap\n",
    "print(\"loading data\")\n",
    "data_train = ZurichLoader(path, 'train', class_to_remove=class_to_remove)\n",
    "data_val = ZurichLoader(path, 'val', class_to_remove=class_to_remove)\n",
    "data_test = ZurichLoader(path, 'test', class_to_remove=class_to_remove)\n",
    "\n",
    "print(\"loading data with overlap\")\n",
    "# data with overlap, for prediction\n",
    "data_train_overlap = ZurichLoader(path, 'train', stride=32, inherit_loader=data_train)\n",
    "data_val_overlap = ZurichLoader(path, 'val', stride=32, inherit_loader=data_val)\n",
    "data_test_overlap = ZurichLoader(path, 'test', stride=32, inherit_loader=data_test)\n",
    "\n",
    "# class names and colors\n",
    "names = data_train.names\n",
    "colors = data_train.colors\n",
    "n_classes = 8\n",
    "classes_to_keep = np.asarray([x for x in range(1, n_classes) if x != class_to_remove])\n",
    "names_keep = np.asarray(names)[classes_to_keep]\n",
    "print(\"classes to keep: \" + str(names_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, offset in zip([data_train, data_val, data_test], [0, 10, 15]):\n",
    "    for im_idx, im in enumerate(dataset.imgs):\n",
    "        im = im[..., :3]\n",
    "        f_name = \"../Figures/Zurich/Im/Im_\" + str(im_idx + offset) + \".jpg\"\n",
    "        export_figure_matplotlib(im, f_name, dpi=my_dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, offset in zip([data_train, data_val, data_test], [0, 10, 15]):\n",
    "    for gt_idx, gt in enumerate(dataset.gt):\n",
    "        gt_col = gt_label_to_color(gt, colors)*255\n",
    "        f_name = \"../Figures/Zurich/Im/GT_\" + str(gt_idx + offset) + \".jpg\"\n",
    "        export_figure_matplotlib(gt_col, f_name, dpi=my_dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_labels_tr, cnt_tr = np.unique(data_train.gt_patches.astype('int'), return_counts=True)\n",
    "pred_labels_val, cnt_val = np.unique(data_val.gt_patches.astype('int'), return_counts=True)\n",
    "pred_labels_te, cnt_te = np.unique(data_test.gt_patches.astype('int'), return_counts=True)\n",
    "\n",
    "cnt_tr = cnt_tr / np.sum(cnt_tr) * 100\n",
    "cnt_val = np.concatenate((cnt_val / np.sum(cnt_val) * 100, [0]))\n",
    "cnt_te = cnt_te / np.sum(cnt_te) * 100\n",
    "\n",
    "df = pd.DataFrame({'Train': cnt_tr, 'Val': cnt_val, 'Test': cnt_te}, index=names[pred_labels_tr])\n",
    "\n",
    "axis = df[::-1].plot.barh(figsize=(7, 6), colormap='winter')\n",
    "plt.xlim([0, 100])\n",
    "plt.xlabel(\"Count [%]\")\n",
    "plt.grid(alpha=.3)\n",
    "axis.spines['right'].set_visible(False)\n",
    "axis.spines['top'].set_visible(False)\n",
    "plt.savefig(\"../Figures/Zurich/Pred_count/ZH_dist.pdf\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training\n",
    "\n",
    "Data Split: \n",
    "- Training: 12 images\n",
    "- Validation: 4 images\n",
    "- Test: 4 images\n",
    "\n",
    "Tested Architectures: \n",
    "\n",
    "| Model | Patch Size | Data Augmentations | Number of Parameters | Testing Precision (avg) | Testing Recall (avg) | Testing f1 score (avg) | Validation / Test accuracy |\n",
    "| ------- | ------- | ------- | ------- | ------- | ------- |\n",
    "| U-Net | 64 | Rot 90°, Flipping  | 7,828,200 | 0.87 | 0.858 | 0.86 | t |\n",
    "| U-Net | 128 | Rot 90°, Flipping  | 7,828,200 | 0.69 | 0.61 | 0.64 | t |\n",
    "| U-Net | 128 | Rot 90°, Flipping  | 7,828,200 | 0.90 | 0.89 | 0.89 | v |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# create copies of original data\\ny_train_label = y_train.copy()\\ny_val_label = y_val.copy()\\ny_test_label = y_test.copy()\\n\\n# get class weights\\nlabels_unique = np.unique(y_train.flatten())\\nprint(labels_unique)\\nclass_weights = class_weight.compute_class_weight(\\'balanced\\', labels_unique, y_train.flatten())\\nclass_weights[0] = 0  # give less weight to background label class\\nclass_weights[5] = 7  # give less weight to bare soil class\\nclass_weights[8] = 7  # give less weight to swimming pool class\\n\\nprint(\"Class weights:\")\\nfor i, w in enumerate(class_weights):\\n    print(\"%15s: %3.3f\" % (names[i], w))\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# create copies of original data\n",
    "y_train_label = y_train.copy()\n",
    "y_val_label = y_val.copy()\n",
    "y_test_label = y_test.copy()\n",
    "\n",
    "# get class weights\n",
    "labels_unique = np.unique(y_train.flatten())\n",
    "print(labels_unique)\n",
    "class_weights = class_weight.compute_class_weight('balanced', labels_unique, y_train.flatten())\n",
    "class_weights[0] = 0  # give less weight to background label class\n",
    "class_weights[5] = 7  # give less weight to bare soil class\n",
    "class_weights[8] = 7  # give less weight to swimming pool class\n",
    "\n",
    "print(\"Class weights:\")\n",
    "for i, w in enumerate(class_weights):\n",
    "    print(\"%15s: %3.3f\" % (names[i], w))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# convert to numpy arrays\\nx_train = np.asarray(x_train)\\nx_val = np.asarray(x_val)\\nx_test = np.asarray(x_test)\\n\\n# make y data categorical\\ny_train = to_categorical(y_train_label, n_classes)\\ny_val = to_categorical(y_val_label, n_classes)\\n\\ny_train = y_train[..., classes_to_keep]\\ny_val = y_val[..., classes_to_keep]\\nn_classes = len(classes_to_keep)\\nclass_weights = class_weights[classes_to_keep]\\n\\n# print shapes of variables\\nfor var in x_train, y_train, x_val, y_val:\\n    print(np.shape(var))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# convert to numpy arrays\n",
    "x_train = np.asarray(x_train)\n",
    "x_val = np.asarray(x_val)\n",
    "x_test = np.asarray(x_test)\n",
    "\n",
    "# make y data categorical\n",
    "y_train = to_categorical(y_train_label, n_classes)\n",
    "y_val = to_categorical(y_val_label, n_classes)\n",
    "\n",
    "y_train = y_train[..., classes_to_keep]\n",
    "y_val = y_val[..., classes_to_keep]\n",
    "n_classes = len(classes_to_keep)\n",
    "class_weights = class_weights[classes_to_keep]\n",
    "\n",
    "# print shapes of variables\n",
    "for var in x_train, y_train, x_val, y_val:\n",
    "    print(np.shape(var))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# callbacks (evaluated every epoch)\\n# show loss and accuracy figures after each epoch\\ncallback_plot = PlotLosses()\\n\\n# stop early if after several epochs the accuracy doesn\\'t improve\\ncallback_earlystop = EarlyStopping(monitor=\\'val_loss\\', min_delta=1e-4, patience=24, verbose=1, mode=\\'auto\\')\\n\\n# decrease learning rate when accuracy stops improving\\ncallback_lr = ReduceLROnPlateau(monitor=\\'val_loss\\', factor=0.5, patience=12, verbose=1, mode=\\'auto\\',\\n                                epsilon=1e-4, cooldown=0, min_lr=1e-8)\\n\\n# checkpoint to save weights at every epoch (in case of interruption)\\nfile_path = \"weights-improvement.hdf5\"\\ncallback_checkpoint = ModelCheckpoint(file_path, monitor=\\'val_acc\\', verbose=0, save_best_only=True, mode=\\'max\\')\\n\\ncallback_tensorboard = TensorBoard(log_dir=\\'./tensorboard\\', histogram_freq=0, write_graph=True, write_images=True)\\n\\n# model setup\\nbatch_size = 20\\nepochs = 300\\n\\n\\ndef model_train(model, data_augmentation):\\n    # Fit the model on the batches generated by datagen.flow().\\n    model.fit_generator(batch_generator(x_train, y_train,\\n                                        batch_size=batch_size, data_augmentation=data_augmentation),\\n                        steps_per_epoch=int(np.ceil(x_train.shape[0] / float(batch_size))),\\n                        epochs=epochs,\\n                        verbose=1,\\n                        class_weight=class_weights,  # weights for loss function\\n                        validation_data=(x_val, y_val),\\n                        callbacks=[callback_earlystop,\\n                                   callback_lr,\\n                                   # callback_checkpoint,\\n                                   callback_plot,\\n                                   callback_tensorboard],\\n                        workers=cpu_count(),\\n                        use_multiprocessing=True)\\n                        \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# callbacks (evaluated every epoch)\n",
    "# show loss and accuracy figures after each epoch\n",
    "callback_plot = PlotLosses()\n",
    "\n",
    "# stop early if after several epochs the accuracy doesn't improve\n",
    "callback_earlystop = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=24, verbose=1, mode='auto')\n",
    "\n",
    "# decrease learning rate when accuracy stops improving\n",
    "callback_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=12, verbose=1, mode='auto',\n",
    "                                epsilon=1e-4, cooldown=0, min_lr=1e-8)\n",
    "\n",
    "# checkpoint to save weights at every epoch (in case of interruption)\n",
    "file_path = \"weights-improvement.hdf5\"\n",
    "callback_checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "\n",
    "callback_tensorboard = TensorBoard(log_dir='./tensorboard', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "# model setup\n",
    "batch_size = 20\n",
    "epochs = 300\n",
    "\n",
    "\n",
    "def model_train(model, data_augmentation):\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(batch_generator(x_train, y_train,\n",
    "                                        batch_size=batch_size, data_augmentation=data_augmentation),\n",
    "                        steps_per_epoch=int(np.ceil(x_train.shape[0] / float(batch_size))),\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        class_weight=class_weights,  # weights for loss function\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        callbacks=[callback_earlystop,\n",
    "                                   callback_lr,\n",
    "                                   # callback_checkpoint,\n",
    "                                   callback_plot,\n",
    "                                   callback_tensorboard],\n",
    "                        workers=cpu_count(),\n",
    "                        use_multiprocessing=True)\n",
    "                        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train or load model\n",
    "# train the model\n",
    "# model_unet = get_unet(n_classes, x_train.shape[1:])\n",
    "# model_train(model_unet, data_augmentation=True)\n",
    "# model_unet.save('models_out/model_unet_64_flip_rot90_wo_cl_' + str(names[class_to_remove]).lower() + '_2.h5')  # save model, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "name_model = path + '/models_out/model_unet_64_flip_rot90_wo_cl_' + str(names[class_to_remove]).lower().replace(\" \", \"\") + '.h5'    \n",
    "model_unet = load_model(name_model, custom_objects={'fn': ignore_background_class_accuracy(0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15640/15640 [==============================] - 9s 576us/step\n",
      "5165/5165 [==============================] - 3s 501us/step\n",
      "5223/5223 [==============================] - 3s 503us/step\n"
     ]
    }
   ],
   "source": [
    "# get all predictions in training and test set\n",
    "# training set\n",
    "y_pred_tr = model_unet.predict(data_train_overlap.im_patches, verbose=1)\n",
    "y_pred_tr = remove_overlap(data_train.imgs, y_pred_tr, 64, 32)\n",
    "y_pred_label_tr = get_y_pred_labels(y_pred_tr, class_to_remove=class_to_remove, background=True)\n",
    "\n",
    "# validation set\n",
    "y_pred_val = model_unet.predict(data_val_overlap.im_patches, verbose=1)\n",
    "y_pred_val = remove_overlap(data_val.imgs, y_pred_val, 64, 32)\n",
    "y_pred_label_val = get_y_pred_labels(y_pred_val, class_to_remove=class_to_remove, background=True)\n",
    "\n",
    "# test set\n",
    "y_pred_te = model_unet.predict(data_test_overlap.im_patches, verbose=1)\n",
    "y_pred_te = remove_overlap(data_test.imgs, y_pred_te, 64, 32)\n",
    "y_pred_label_te = get_y_pred_labels(y_pred_te, class_to_remove=class_to_remove, background=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of correctly / incorrectly predicted pixels\n",
    "# train\n",
    "#pred_t_tr = (data_train.gt_patches != class_to_remove) & (data_train.gt_patches != 0)\n",
    "pred_t_tr = y_pred_label_tr == data_train.gt_patches\n",
    "pred_f_tr = data_train.gt_patches == class_to_remove\n",
    "\n",
    "# val\n",
    "#pred_t_val = (data_val.gt_patches != class_to_remove) & (data_val.gt_patches != 0)\n",
    "pred_t_val = y_pred_label_val == data_val.gt_patches\n",
    "pred_f_val = data_val.gt_patches == class_to_remove\n",
    "\n",
    "# test\n",
    "#pred_t_te = (data_test.gt_patches != class_to_remove) & (data_test.gt_patches != 0)\n",
    "pred_t_te = y_pred_label_te == data_test.gt_patches\n",
    "pred_f_te = data_test.gt_patches == class_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export predicted images\n",
    "offset = 0\n",
    "for dataset, preds in zip([data_train, data_val, data_test], [y_pred_label_tr, y_pred_label_val, y_pred_label_te]):\n",
    "    imgs = convert_patches_to_image(dataset.imgs, preds[..., np.newaxis], 64, 64)\n",
    "    for im_idx, im in enumerate(imgs):\n",
    "        im_color = gt_label_to_color(im, colors) * 255\n",
    "        f_name = \"../Figures/Zurich/Im_pred/cl_\" + str(class_to_remove)\n",
    "        f_name = f_name + \"/Im_\" + str(im_idx + offset) + \"_wo_cl_\" + str(class_to_remove) + \".jpg\"\n",
    "        export_figure_matplotlib(im_color, f_name, dpi=my_dpi)\n",
    "        \n",
    "    offset += len(dataset.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metrics (Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93.75 90.3 ]\n",
      "[91.67 79.76]\n",
      "[88.1  76.71]\n"
     ]
    }
   ],
   "source": [
    "# Get oa, aa for train, val, test\n",
    "# train\n",
    "y_pred_tr_flattened = np.asarray(y_pred_label_tr.flatten()).astype('int')\n",
    "y_tr_flattened = np.asarray(data_train.gt_patches.flatten()).astype('int')\n",
    "filter_items = (y_tr_flattened != 0) & (y_tr_flattened != class_to_remove)\n",
    "oa_tr = oa(y_tr_flattened[filter_items], y_pred_tr_flattened[filter_items])\n",
    "aa_tr, aa_tr_cl = aa(y_tr_flattened[filter_items], y_pred_tr_flattened[filter_items])\n",
    "\n",
    "# val\n",
    "y_pred_val_flattened = np.asarray(y_pred_label_val.flatten()).astype('int')\n",
    "y_val_flattened = np.asarray(data_val.gt_patches.flatten()).astype('int')\n",
    "filter_items = (y_val_flattened != 0) & (y_val_flattened != class_to_remove)\n",
    "oa_val = oa(y_val_flattened[filter_items], y_pred_val_flattened[filter_items])\n",
    "aa_val, aa_val_cl = aa(y_val_flattened[filter_items], y_pred_val_flattened[filter_items])\n",
    "\n",
    "# test\n",
    "y_pred_te_flattened = np.asarray(y_pred_label_te.flatten()).astype('int')\n",
    "y_te_flattened = np.asarray(data_test.gt_patches.flatten()).astype('int')\n",
    "filter_items = (y_te_flattened != 0) & (y_te_flattened != class_to_remove)\n",
    "oa_te = oa(y_te_flattened[filter_items], y_pred_te_flattened[filter_items])\n",
    "aa_te, aa_te_cl = aa(y_te_flattened[filter_items], y_pred_te_flattened[filter_items])\n",
    "\n",
    "print(np.round(np.multiply([oa_tr, aa_tr], 100), 2))\n",
    "print(np.round(np.multiply([oa_val, aa_val], 100), 2))\n",
    "print(np.round(np.multiply([oa_te, aa_te], 100), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write metrics to CSV files\n",
    "df_metrics = pd.read_csv('models_out/metrics_ND.csv', index_col=0)\n",
    "df2 = pd.DataFrame({str(names[class_to_remove]):[oa_tr, aa_tr, oa_val, aa_val, oa_te, aa_te]},\n",
    "                    index = ['OA Train', 'AA Train', 'OA Val', 'AA Val', 'OA Test', 'AA Test']).T\n",
    "df_metrics = df_metrics.append(df2)\n",
    "df_metrics = df_metrics[~df_metrics.index.duplicated(keep='last')]  # avoid duplicates\n",
    "df_metrics.to_csv('models_out/metrics_ND.csv')\n",
    "# print((df_metrics*100).round(2).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 7, does not match size of target_names, 6\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  Buildings      0.824     0.978     0.894    778708\n",
      "      Trees      0.895     0.866     0.880    693092\n",
      "      Grass      0.953     0.841     0.894    689620\n",
      "  Bare Soil      0.901     0.711     0.794     88088\n",
      "      Water      0.984     0.888     0.933    284293\n",
      "   Railways      0.047     0.082     0.060     19190\n",
      "\n",
      "avg / total      0.889     0.881     0.881   2563448\n",
      "\n",
      "88.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Accuracy metrics\n",
    "y_pred_flattened = np.asarray(y_pred_label_te.flatten()).astype('int')\n",
    "y_test_flattened = np.asarray(data_test.gt_patches.flatten()).astype('int')\n",
    "\n",
    "# mask background and removed classes for evaluation metrics\n",
    "filter_items = (y_test_flattened != 0) & (y_test_flattened != class_to_remove)\n",
    "\n",
    "# Class accuracy, average accuracy\n",
    "print(metrics.classification_report(\n",
    "    y_test_flattened[filter_items],\n",
    "    y_pred_flattened[filter_items],\n",
    "    target_names=names_keep,\n",
    "    digits=3))\n",
    "\n",
    "\n",
    "# Overall accuracy\n",
    "print(np.round(oa_te * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of predictions in unseen class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution of predicted label\n",
    "pred_labels, pred_counts = np.unique(y_pred_label_te[pred_f_te], return_counts=True)\n",
    "pred_counts = pred_counts / sum(pred_counts) * 100\n",
    "\n",
    "# visualization\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "plt.bar(pred_labels, pred_counts)\n",
    "plt.xticks(np.arange(0, 10))\n",
    "plt.ylim([0,100])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"Count [%]\")\n",
    "plt.grid(alpha=.3)\n",
    "fig.axes[0].spines['right'].set_visible(False)\n",
    "fig.axes[0].spines['top'].set_visible(False)\n",
    "plt.title(\"Misclassified labels (mean MSR=%.2f)\" % np.mean(get_acc_net_msr(y_pred_te[pred_f_te])))\n",
    "plt.xticks(pred_labels_te, names, rotation=20)\n",
    "plt.savefig(\"../Figures/Zurich/Pred_count/ZH_pred-count_wo_cl\" + str(class_to_remove) + \".pdf\",\n",
    "            bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novelty Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_msr = get_acc_net_msr(y_pred_te).flatten()\n",
    "probas_margin = get_acc_net_max_margin(y_pred_te).flatten()\n",
    "probas_entropy = get_acc_net_entropy(y_pred_te).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.60, PR AUC: 0.17\n",
      "AUROC: 0.59, PR AUC: 0.17\n",
      "AUROC: 0.61, PR AUC: 0.18\n"
     ]
    }
   ],
   "source": [
    "# precision-recall curves\n",
    "\n",
    "# msr\n",
    "y_scores = -probas_msr\n",
    "y_true = pred_f_te.flatten()\n",
    "precision_msr, recall_msr, _ = metrics.precision_recall_curve(y_true, y_scores)\n",
    "pr_auc_msr = metrics.average_precision_score(y_true, y_scores)\n",
    "auroc_msr = metrics.roc_auc_score(y_true, y_scores)\n",
    "fpr_msr, tpr_msr, _ = metrics.roc_curve(y_true, y_scores)\n",
    "\n",
    "# margin\n",
    "y_scores = -probas_margin\n",
    "precision_margin, recall_margin, _ = metrics.precision_recall_curve(y_true, y_scores)\n",
    "pr_auc_margin = metrics.average_precision_score(y_true, y_scores)\n",
    "auroc_margin = metrics.roc_auc_score(y_true, y_scores)\n",
    "fpr_margin, tpr_margin, _ = metrics.roc_curve(y_true, y_scores)\n",
    "\n",
    "# entropy\n",
    "y_scores = -probas_entropy\n",
    "precision_entropy, recall_entropy, _ = metrics.precision_recall_curve(y_true, y_scores)\n",
    "pr_auc_entropy = metrics.average_precision_score(y_true, y_scores)\n",
    "auroc_entropy = metrics.roc_auc_score(y_true, y_scores)\n",
    "fpr_entropy, tpr_entropy, _ = metrics.roc_curve(y_true, y_scores)\n",
    "\n",
    "print(\"AUROC: %.2f, PR AUC: %.2f\" % (auroc_msr, pr_auc_msr))\n",
    "print(\"AUROC: %.2f, PR AUC: %.2f\" % (auroc_margin, pr_auc_margin))\n",
    "print(\"AUROC: %.2f, PR AUC: %.2f\" % (auroc_entropy, pr_auc_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "# MSR\n",
    "probas_patches_msr = np.reshape(probas_msr, np.shape(data_test.gt_patches))\n",
    "probas_patches_msr -= np.min(probas_patches_msr)\n",
    "probas_patches_msr /= np.max(probas_patches_msr)\n",
    "\n",
    "# margin\n",
    "probas_patches_margin = np.reshape(probas_margin, np.shape(data_test.gt_patches))\n",
    "probas_patches_margin -= np.min(probas_patches_margin)\n",
    "probas_patches_margin /= np.max(probas_patches_margin)\n",
    "\n",
    "# entropy\n",
    "probas_patches_entropy = np.reshape(probas_entropy, np.shape(data_test.gt_patches))\n",
    "probas_patches_entropy -= np.min(probas_patches_entropy)\n",
    "probas_patches_entropy /= np.max(probas_patches_entropy)\n",
    "\n",
    "base_folder = \"../Figures/Zurich/Im_cert/cl_\" + str(class_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_im_msr = convert_patches_to_image(data_test.imgs, probas_patches_msr[..., np.newaxis], 64, 64)\n",
    "acc_im_margin = convert_patches_to_image(data_test.imgs, probas_patches_margin[..., np.newaxis], 64, 64)\n",
    "acc_im_entropy = convert_patches_to_image(data_test.imgs, probas_patches_entropy[..., np.newaxis], 64, 64)\n",
    "\n",
    "# export images\n",
    "for img_idx in range(len(data_test.imgs)):\n",
    "    # msr\n",
    "    acc_im_msr_ = exposure.equalize_hist(acc_im_msr[img_idx])\n",
    "    f_name = base_folder + \"/ZH_wo_cl_\" + str(class_to_remove) + \"_net_msr_im_\" + str(img_idx) + \".jpg\"\n",
    "    export_figure_matplotlib(acc_im_msr_, f_name, dpi=my_dpi)\n",
    "    \n",
    "    # margin\n",
    "    \n",
    "    acc_im_margin_ = exposure.equalize_hist(acc_im_margin[img_idx])\n",
    "    f_name = base_folder + \"/ZH_wo_cl_\" + str(class_to_remove) + \"_net_margin_im_\" + str(img_idx) + \".jpg\"\n",
    "    export_figure_matplotlib(acc_im_margin_, f_name, dpi=my_dpi)\n",
    "    \n",
    "    # entropy\n",
    "    \n",
    "    acc_im_entropy_ = exposure.equalize_hist(acc_im_entropy[img_idx])\n",
    "    f_name = base_folder + \"/ZH_wo_cl_\" + str(class_to_remove) + \"_net_entropy_im_\" + str(img_idx) + \".jpg\"\n",
    "    export_figure_matplotlib(acc_im_entropy_, f_name, dpi=my_dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAABLCAYAAACr45hhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAAfhJREFUeJzt1k2Om0AUhdFXkL1kydlClpJNUS8DGzemkZ0MrlpqnSMhXH82YvDJo7sLIGH56gcAvi+BAWIEBogRGCBGYIAYgQFiBAaIERggRmCAmB//s/n3+rOXpepxjcPni2uM8XL9+B1Vx/lRYx01lvu1ftyXp7nl077l4sxYlhrLqGV9XqvH/fYgYx23BzrM7/vqcKbuv3ve+7xn/77zi3r/eYyr+Tfnxv0ljuVjvF/n8dh/Z3297+Jcj6ruWbNndZ3uPWvWvFjf3qxfne+a+736efxp7Th/GteLte7aug732yvcumt2Pea27przNH6s92G+as7T+L6+ddc293u/Hj/m6rB2Gl+cn7OqZ39c3c/jq+vdnov1qqr+9Wf8SzP8gwFiBAaIERggRmCAGIEBYgQGiBEYIEZggBiBAWIEBogRGCBGYIAYgQFiBAaIERggRmCAGIEBYgQGiBEYIEZggBiBAWIEBogRGCBGYIAYgQFiBAaIERggRmCAGIEBYgQGiBEYIEZggBiBAWIEBogRGCBGYIAYgQFiBAaIERggRmCAGIEBYgQGiBEYIEZggBiBAWIEBogRGCBGYIAYgQFiBAaIERggRmCAGIEBYkZ3f/UzAN+UfzBAjMAAMQIDxAgMECMwQIzAADECA8QIDBAjMECMwAAxfwFz5mOgoT6MYwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 648x108 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# export colorbar\n",
    "a = np.array([[0,1]])\n",
    "plt.figure(figsize=(9, 1.5))\n",
    "img = plt.imshow(a, cmap=\"RdYlGn\")\n",
    "plt.gca().set_visible(False)\n",
    "cax = plt.axes([0.1, 0.1, 0.4, 0.5])\n",
    "cb = plt.colorbar(orientation=\"horizontal\", cax=cax)\n",
    "cb.outline.set_linewidth(0)\n",
    "plt.axis('off')\n",
    "plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "plt.savefig(\"../Figures/Zurich/Im_cert/colorbar.pdf\", bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_dropout_imgs(model, dataset, batch_size=300, n_iter=10):\n",
    "    \"\"\"\n",
    "    make all predictions per batch for image data\n",
    "    :param model: model to use for predictions\n",
    "    :param x: image patches\n",
    "    :param imgs: original entire images\n",
    "    :param batch_size: size of prediction batch\n",
    "    :param n_iter: number of predictions to make with dropout\n",
    "    :param ids: ids of images for which to make predictions\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    x = dataset.im_patches\n",
    "    imgs = dataset.imgs\n",
    "    n_steps = int(np.ceil(len(x) / batch_size))\n",
    "    preds_it = []\n",
    "    f = k.function([model.layers[0].input, k.learning_phase()], [model.layers[-1].output])\n",
    "\n",
    "    for _ in tqdm(range(n_iter)):\n",
    "        preds = []\n",
    "        for i in range(n_steps):\n",
    "            idx_start = i * batch_size\n",
    "            idx_end = (i + 1) * batch_size\n",
    "            pred = np.concatenate(f([x[idx_start:idx_end], 1]))\n",
    "            preds.append(pred)\n",
    "\n",
    "        preds = np.concatenate(preds)\n",
    "        preds = remove_overlap(dataset.imgs, preds, dataset.patch_size, dataset.stride)\n",
    "        preds_it.append(preds)\n",
    "\n",
    "    return preds_it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:28<00:00,  2.87s/it]\n"
     ]
    }
   ],
   "source": [
    "# get predictions\n",
    "y_preds = predict_with_dropout_imgs(model_unet, data_test_overlap, batch_size=500, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction and confidence\n",
    "prediction = np.mean(y_preds, 0)\n",
    "probas_dropout = -get_acc_net_entropy(prediction)\n",
    "del y_preds # free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.59, PR AUC: 0.17\n"
     ]
    }
   ],
   "source": [
    "# dropout metrics\n",
    "y_scores = probas_dropout.flatten()\n",
    "precision_dropout, recall_dropout, _ = metrics.precision_recall_curve(y_true, y_scores)\n",
    "pr_auc_dropout = metrics.average_precision_score(y_true, y_scores)\n",
    "auroc_dropout = metrics.roc_auc_score(y_true, y_scores)\n",
    "fpr_dropout, tpr_dropout, _ = metrics.roc_curve(y_true, y_scores)\n",
    "print(\"AUROC: %.2f, PR AUC: %.2f\" % (auroc_dropout, pr_auc_dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "probas_patches_dropout = np.reshape(probas_dropout, np.shape(data_test.gt_patches))\n",
    "probas_patches_dropout -= np.min(probas_patches_dropout)\n",
    "probas_patches_dropout /= np.max(probas_patches_dropout)\n",
    "\n",
    "# show image of DF uncertainty vs. max margin uncertainty\n",
    "acc_im_dropout = convert_patches_to_image(data_test.imgs, -probas_patches_dropout[..., np.newaxis], 64, 64)\n",
    "for img_idx in range(len(data_test.imgs)):\n",
    "    acc_im_dropout_ = exposure.equalize_hist(acc_im_dropout[img_idx])\n",
    "    f_name = base_folder + \"/ZH_wo_cl_\" + str(class_to_remove) + \"_dropout_im_\" + str(img_idx) + \".jpg\"\n",
    "    export_figure_matplotlib(acc_im_dropout_, f_name, dpi=my_dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Activations, PCA, t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [00:15<00:00, 51.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# get activations for training Density Forest\n",
    "act_train_all = get_activations_batch(model_unet, -2, data_train_overlap.im_patches, 20, verbose=True)\n",
    "\n",
    "# retain only activation weights for which there is a ground truth\n",
    "act_train_all = remove_overlap(data_train.imgs, act_train_all, patch_size=64, stride=32)\n",
    "act_train = act_train_all[pred_t_tr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get activations\n",
    "act_val_all = get_activations_batch(model_unet, -2, data_val_overlap.im_patches, 20, verbose=True)\n",
    "\n",
    "# retain only activation weights for which there is a ground truth\n",
    "act_val_all = remove_overlap(data_val.imgs, act_val_all, patch_size=64, stride=32)\n",
    "act_val = act_val_all[pred_t_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get activations for testing Density Forest\n",
    "act_test = get_activations_batch(model_unet, -2, data_test_overlap.im_patches, 20, verbose=True)\n",
    "\n",
    "# remove test activations overlap\n",
    "act_test = remove_overlap(data_test.imgs, act_test, patch_size=64, stride=32)\n",
    "act_test = np.concatenate(np.concatenate(act_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get balanced data subset to show in figure\n",
    "tsne_pts_per_class = 200\n",
    "dataset_subset_indices = get_balanced_subset_indices(data_test.gt_patches.flatten(), \n",
    "                                                     np.arange(1, 9), pts_per_class=tsne_pts_per_class)\n",
    "dataset_subset_indices = np.concatenate(dataset_subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=500)\n",
    "tsne_all = tsne.fit_transform(act_test[dataset_subset_indices])\n",
    "tsne_y = data_test.gt_patches.flatten()[dataset_subset_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "_, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ax.set_axis_off()\n",
    "plot_pts_2d(tsne_all, tsne_y, ax, classes_to_keep, colors, class_to_remove=class_to_remove)\n",
    "plt.savefig(\"../Figures/Zurich/tSNE/t-SNE_wo_cl\" + str(class_to_remove) + \"_before_PCA.pdf\",\n",
    "            bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create density tree for activation weights of training data\n",
    "# PCA\n",
    "pca = decomposition.PCA(n_components=.95)\n",
    "pca.fit(act_test)  # fit on training set without background pixels\n",
    "n_components = np.alen(pca.explained_variance_ratio_)\n",
    "print(\"Variance explained by first %i components: %.2f\" % (\n",
    "    n_components, sum(pca.explained_variance_ratio_)))\n",
    "\n",
    "# transform training activations\n",
    "act_train_all = pca.transform(np.concatenate(np.concatenate(act_train_all)))\n",
    "act_train = pca.transform(act_train)\n",
    "\n",
    "act_val_all = pca.transform(np.concatenate(np.concatenate(act_val_all)))\n",
    "act_val = pca.transform(act_val)\n",
    "\n",
    "\n",
    "# transform test set activations\n",
    "act_test = pca.transform(act_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative explained variance\n",
    "fig = plt.figure()\n",
    "plt.scatter(np.arange(n_components), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cumulative sum of explained variance\")\n",
    "plt.grid(alpha=.3)\n",
    "fig.axes[0].spines['right'].set_visible(False)\n",
    "fig.axes[0].spines['top'].set_visible(False)\n",
    "plt.savefig(\"../Figures/Zurich/PCA/ZH_pca_components_wo_cl_\" + str(class_to_remove) + \".pdf\",\n",
    "            bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization after PCA\n",
    "tsne_all = tsne.fit_transform(act_test[dataset_subset_indices])\n",
    "# tsne without unseen class\n",
    "tsne_train = tsne_all[tsne_y != class_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "_, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "plot_pts_2d(tsne_all, tsne_y, ax, classes_to_keep, colors, class_to_remove=class_to_remove)\n",
    "ax.set_axis_off()\n",
    "plt.savefig(\"../Figures/Zurich/tSNE/t-SNE_wo_cl\" + str(class_to_remove) + \"_after_PCA.pdf\",\n",
    "            bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first 3 PCA components\n",
    "plot_pts_3d(act_test[:, :3], data_test.gt_patches.flatten(), classes_to_keep, colors,\n",
    "            class_to_remove=class_to_remove, subsample_pct=.0003,\n",
    "            s_name='../Figures/Zurich/PCA/pca_components_3d_' + str(names[class_to_remove]) + '.pdf')\n",
    "\n",
    "print(\"Variance explained by first 3 components: %.2f\" % np.sum(pca.explained_variance_ratio_[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot first 2 PCA components\n",
    "_, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.set_axis_off()\n",
    "plot_pts_2d(act_test[:, :2], data_test.gt_patches.flatten(), ax, classes_to_keep, colors,\n",
    "            class_to_remove=class_to_remove, subsample_pct=.0005,\n",
    "            s_name='../Figures/Zurich/PCA/pca_components_2d_' + str(names[class_to_remove]) + '.pdf')\n",
    "print(\"Variance explained by first 2 components: %.2f\" % np.sum(pca.explained_variance_ratio_[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_degrees = [3, 5, 4, 3, 9, 5, 9, 3]\n",
    "\n",
    "if paramsearch:\n",
    "    tuned_parameters = [{'n_components': np.arange(3, 10), 'max_iter': [10000]}]\n",
    "    # do parameter search\n",
    "    ps_gmm = ParameterSearch(GaussianMixture, tuned_parameters, act_train, act_train_all,\n",
    "                             pred_f_tr.flatten(), scorer_roc_probas_gmm, \n",
    "                             n_iter=3, verbosity=10, n_jobs=-1, subsample_train=.01, subsample_test=.01)\n",
    "    ps_gmm.fit()\n",
    "    best_params_gmm = ps_gmm.best_params\n",
    "else:\n",
    "    best_params_gmm = {'n_components': df_ps.loc[str(names[class_to_remove])]['gmm_n_components'], 'max_iter': 10000}\n",
    "print(best_params_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GMM\n",
    "gmm = GaussianMixture(**best_params_gmm)\n",
    "gmm.fit(draw_subsamples(act_train, .1))\n",
    "\n",
    "# Predict\n",
    "probas_gmm = gmm.predict_proba(act_test)\n",
    "probas_gmm = get_acc_net_entropy(probas_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision-recall curve\n",
    "y_scores = -probas_gmm\n",
    "precision_gmm, recall_gmm, _ = metrics.precision_recall_curve(y_true, y_scores)\n",
    "pr_auc_gmm = metrics.auc(recall_gmm, precision_gmm)\n",
    "\n",
    "# ROC\n",
    "fpr_gmm, tpr_gmm, _ = metrics.roc_curve(y_true, y_scores)\n",
    "auroc_gmm = metrics.roc_auc_score(y_true, y_scores)\n",
    "plt.step(recall_gmm, precision_gmm)\n",
    "print(\"AUROC: %.2f, PR AUC: %.2f\" % (auroc_gmm, pr_auc_gmm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_exp(im, n_iter):\n",
    "    \"\"\"ad-hoc contrast enhancement for GMM\"\"\"\n",
    "    for _ in range(n_iter):\n",
    "        im = np.exp(im)\n",
    "        im = exposure.equalize_hist(im)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "probas_patches_gmm = np.reshape(probas_gmm, np.shape(data_test.gt_patches))\n",
    "probas_patches_gmm -= np.min(probas_patches_gmm)\n",
    "probas_patches_gmm /= np.max(probas_patches_gmm)\n",
    "\n",
    "# show image of DF uncertainty vs. max margin uncertainty\n",
    "acc_im_gmm = convert_patches_to_image(data_test.imgs, probas_patches_gmm[..., np.newaxis], 64, 64)\n",
    "for img_idx in range(len(data_test.imgs)):\n",
    "    acc_im_gmm_ = acc_im_gmm[img_idx]\n",
    "    acc_im_gmm_ = equalize_exp(acc_im_gmm_, 10)\n",
    "    f_name = base_folder + \"/ZH_wo_cl_\" + str(class_to_remove) + \"_gmm_im_\" + str(img_idx) + \".jpg\"\n",
    "    export_figure_matplotlib(acc_im_gmm_, f_name, dpi=my_dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_svm = preprocessing.scale(act_train)\n",
    "act_train_all_svm = preprocessing.scale(act_train_all)\n",
    "act_val_all_svm = preprocessing.scale(act_val_all)\n",
    "act_test_svm = preprocessing.scale(act_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter search\n",
    "\n",
    "if paramsearch:\n",
    "    tuned_parameters = [{'kernel': ['rbf'],\n",
    "                         'nu': [1e-4, 1e-3, 1e-2, 1e-1, 5e-1],\n",
    "                         'degree':[1]\n",
    "                         },\n",
    "                        {'kernel': ['poly'],\n",
    "                         'degree': np.arange(1, 4),\n",
    "                         'nu': [1e-4, 1e-3, 1e-2, 1e-1, 5e-1],\n",
    "                         'max_iter': [10000]}]\n",
    "\n",
    "    # do parameter search\n",
    "    ps_svm = ParameterSearch(svm.OneClassSVM, tuned_parameters, act_train_svm, act_train_all,\n",
    "                             pred_f_tr.flatten(), scorer_roc_probas_svm, n_iter=5,\n",
    "                             verbosity=11, n_jobs=-1, subsample_train=.0001, subsample_test=.001)\n",
    "    ps_svm.fit()\n",
    "    best_params_svm = ps_svm.best_params\n",
    "else:\n",
    "    best_params_svm = {'kernel': df_ps.loc[str(names[class_to_remove])]['oc-svm_k'], \n",
    "                       'degree': df_ps.loc[str(names[class_to_remove])]['oc-svm_deg'], \n",
    "                       'nu': df_ps.loc[str(names[class_to_remove])]['oc_svm_nu'],\n",
    "                       'max_iter':10000}\n",
    "    \n",
    "print(best_params_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SVM\n",
    "clf_svm = svm.OneClassSVM(**best_params_svm)\n",
    "clf_svm.fit(draw_subsamples(act_train_svm, .001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "probas_svm = clf_svm.decision_function(act_test_svm[dataset_subset_indices])\n",
    "probas_svm -= np.min(probas_svm)\n",
    "probas_svm /= np.max(probas_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_svm = clf_svm.decision_function(act_test_svm)\n",
    "probas_svm -= np.min(probas_svm)\n",
    "probas_svm /= np.max(probas_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "\n",
    "y_scores = -probas_svm[:]\n",
    "# PR\n",
    "precision_svm, recall_svm, _ = metrics.precision_recall_curve(y_true, y_scores)\n",
    "pr_auc_svm = metrics.auc(recall_svm, precision_svm)\n",
    "\n",
    "# ROC\n",
    "fpr_svm, tpr_svm, _ = metrics.roc_curve(y_true, y_scores)\n",
    "auroc_svm = metrics.roc_auc_score(y_true, y_scores)\n",
    "\n",
    "print(\"AUROC: %.2f, PR AUC: %.2f\" % (auroc_svm, pr_auc_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "probas_patches_svm = np.reshape(probas_svm, np.shape(data_test.gt_patches))\n",
    "\n",
    "# show image of DF uncertainty vs. max margin uncertainty\n",
    "acc_im_svm = convert_patches_to_image(data_test.imgs, probas_patches_svm[..., np.newaxis], 64, 64)\n",
    "for img_idx in range(len(data_test.imgs)):\n",
    "    acc_im_svm_ = exposure.equalize_hist(acc_im_svm[img_idx])\n",
    "    f_name = base_folder + \"/ZH_wo_cl_\" + str(class_to_remove) + \"_svm_im_\" + str(img_idx) + \".jpg\"\n",
    "    export_figure_matplotlib(acc_im_svm_, f_name, dpi=my_dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_ind = get_balanced_subset_indices(data_train.gt_patches.flatten(), classes_to_keep, pts_per_class=150)\n",
    "\n",
    "subsample = act_train_all_svm[np.concatenate(subset_ind)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF\n",
    "K = RBF()\n",
    "K_X = K.__call__(subsample)\n",
    "K_X = exposure.equalize_hist(K_X)\n",
    "f_name = \"../Figures/Zurich/Kernels/Kernel_RBF_wo_cl_\" + str(class_to_remove) + \".jpg\" \n",
    "export_figure_matplotlib(K_X, f_name, dpi=my_dpi)\n",
    "\n",
    "# polynomial\n",
    "for deg in [1, 2, 3]:\n",
    "    K_X = metrics.pairwise.polynomial_kernel(subsample, degree=deg)\n",
    "    # contrast stretching\n",
    "    p2, p98 = np.percentile(K_X, (2, 98))\n",
    "    K_X = exposure.rescale_intensity(K_X, in_range=(p2, p98))\n",
    "\n",
    "    f_name = \"../Figures/Zurich/Kernels/Kernel_poly_wo_cl_\" + str(class_to_remove) + \"_deg_\" + str(deg) + \".jpg\"\n",
    "    export_figure_matplotlib(K_X, f_name, dpi=my_dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DensityForest instance\n",
    "clf_df = DensityForest(max_depth=2, min_subset=.1, n_trees=100,\n",
    "                       subsample_pct=.1, n_jobs=-1, verbose=10,\n",
    "                       ig_improvement=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit to training data\n",
    "clf_df.fit(tsne_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show ellipses on plot\n",
    "_, axes = plt.subplots(2, 2, figsize=(15, 15)) \n",
    "for i in range(4):\n",
    "    plot_pts_2d(tsne_all, tsne_y, axes[int(i/2)][np.mod(i, 2)], classes_to_keep, \n",
    "                colors, class_to_remove=class_to_remove)\n",
    "    axes[int(i/2)][np.mod(i, 2)].set_axis_off()\n",
    "    covs, means = get_clusters(clf_df.root_nodes[i], [], [])\n",
    "    plot_ellipses(axes[int(i / 2)][np.mod(i, 2)], means, covs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export some ellipses for GIF\n",
    "\n",
    "for i in range(10):\n",
    "    _, ax = plt.subplots(1, 1, figsize=(8, 8)) \n",
    "    plt.xlim([-50, 50])\n",
    "    plt.ylim([-50, 50])\n",
    "    plot_pts_2d(tsne_all, tsne_y, ax, classes_to_keep, colors, \n",
    "                class_to_remove=class_to_remove, names=names)\n",
    "    covs, means = get_clusters(clf_df.root_nodes[i], [], [])\n",
    "    plot_ellipses(ax, means, covs)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"../Figures/Zurich/GIF/TSNE_act_wo_cl\" + str(class_to_remove) + \"_\"+str(i)+\".pdf\", \n",
    "                bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "# get probabilities for all images\n",
    "probas_df = np.log(clf_df.predict(tsne_all))\n",
    "\n",
    "# indices of correctly / wrongly predicted points\n",
    "pred_f = pred_f_te.flatten()[dataset_subset_indices]\n",
    "pred_t = pred_t_te.flatten()[dataset_subset_indices]\n",
    "\n",
    "# plot colors\n",
    "probas_df_c = imgs_stretch_eq(probas_df[np.newaxis, ..., np.newaxis])[0, ..., 0]\n",
    "colors_plt = plt.cm.YlOrRd(1 - probas_df_c)[..., :3]\n",
    "\n",
    "# threshold for second plot\n",
    "c_thresh_t = plt.cm.GnBu((probas_df < np.sort(probas_df)[tsne_pts_per_class])*255)[:, :3]\n",
    "\n",
    "c_thresh_f = plt.cm.GnBu((probas_df > np.sort(probas_df)[tsne_pts_per_class])*255)[:, :3]\n",
    "\n",
    "# plot correctly predicted points (o marker)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10)) \n",
    "axes[0].scatter(tsne_all[:, 0][pred_t], tsne_all[:, 1][pred_t], c=colors_plt[pred_t])\n",
    "axes[1].scatter(tsne_all[:, 0][pred_t], tsne_all[:, 1][pred_t], c=c_thresh_t[pred_t])\n",
    "\n",
    "# plot incorrectly predicted points (x marker)\n",
    "axes[0].scatter(tsne_all[:, 0][pred_f], tsne_all[:, 1][pred_f], c=colors_plt[pred_f], marker='x')\n",
    "axes[1].scatter(tsne_all[:, 0][pred_f], tsne_all[:, 1][pred_f], c=c_thresh_f[pred_f], marker='x')\n",
    "[axes[i].legend(['Seen points', 'Novel points']) for i in range(2)]\n",
    "[axes[i].set_axis_off() for i in range(2)]\n",
    "extent = axes[0].get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "plt.savefig(\"../Figures/Zurich/GIF/probas.pdf\", bbox_inches=extent, pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter search\n",
    "default_params = {'n_trees': 5, 'n_max_dim': 0, 'n_jobs': -1, \n",
    "                  'verbose': 0, 'subsample_pct': .0002, 'min_subset': 0}\n",
    "\n",
    "if paramsearch:\n",
    "    \"\"\"search for best hyperparameters\"\"\"\n",
    "    tuned_params = [{'max_depth': [1, 2, 3],\n",
    "                     'ig_improvement': [-np.infty, 0, .4, .7]\n",
    "                    }]\n",
    "\n",
    "    # do parameter search\n",
    "    ps_df = ParameterSearch(DensityForest, tuned_params, act_train, act_train_all,\n",
    "                            pred_f_tr.flatten(), scorer_roc_probas_df,\n",
    "                            n_iter=3, verbosity=11, n_jobs=1, subsample_train=1, \n",
    "                            subsample_test=.001, default_params=default_params)\n",
    "\n",
    "    print(\"Testing %i combinations %i times\" % (len(ps_df.combinations), ps_df.n_iter))\n",
    "    print(ps_df.combinations)\n",
    "    ps_df.fit()\n",
    "    print(ps_df.best_params)\n",
    "    \n",
    "    # Create DensityForest instance\n",
    "    best_params_df = ps_df.best_params\n",
    "    \n",
    "else:\n",
    "    \"\"\"use previously found hyperparameters\"\"\"\n",
    "    best_params_df = {'max_depth': df_ps.loc[str(names[class_to_remove])]['df_depth'],\n",
    "                      'ig_improvement': df_ps.loc[str(names[class_to_remove])]['df_min_ig']}\n",
    "    \n",
    "    \n",
    "print(best_params_df)\n",
    "default_params['verbose'] = 1\n",
    "default_params['batch_size'] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit DF with best found parameters\n",
    "clf_df = DensityForest(**best_params_df, **default_params)\n",
    "clf_df.fit(act_train)\n",
    "\n",
    "# get probabilities for all images\n",
    "probas_df = clf_df.predict(act_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape probas to (n_patches, patch_size, patch_size)\n",
    "probas_patches_df = np.reshape(probas_df, np.shape(data_test.gt_patches))\n",
    "\n",
    "# transformations\n",
    "probas_patches_df -= np.nanmin(probas_patches_df)\n",
    "probas_patches_df /= np.nanmax(probas_patches_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "y_scores = -probas_df\n",
    "\n",
    "# PR\n",
    "precision_df, recall_df, _ = metrics.precision_recall_curve(y_true, y_scores)\n",
    "pr_auc_df = metrics.auc(recall_df, precision_df)\n",
    "\n",
    "# ROC\n",
    "fpr_df, tpr_df, _ = metrics.roc_curve(y_true, y_scores)\n",
    "auroc_df = metrics.roc_auc_score(y_true, y_scores)\n",
    "\n",
    "print(\"AUROC: %.2f, PR AUC: %.2f\" % (auroc_df, pr_auc_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "probas_patches_df = np.reshape(probas_df, np.shape(data_test.gt_patches))\n",
    "probas_patches_df -= np.min(probas_patches_df)\n",
    "probas_patches_df /= np.max(probas_patches_df)\n",
    "\n",
    "# show image of DF uncertainty vs. max margin uncertainty\n",
    "acc_im_df = convert_patches_to_image(data_test.imgs, probas_patches_df[..., np.newaxis], 64, 64)\n",
    "for img_idx in range(len(data_test.imgs)):\n",
    "    #acc_im_df_ = exposure.equalize_hist(acc_im_df[img_idx])\n",
    "    # TODO try log for contrast enhancement\n",
    "    acc_im_df_= exposure.equalize_hist(np.log(acc_im_df[img_idx]+1e-10))\n",
    "    f_name = base_folder + \"/ZH_wo_cl_\" + str(class_to_remove) + \"_df_im_\" + str(img_idx) + \".jpg\"\n",
    "    export_figure_matplotlib(acc_im_df_, f_name, dpi=my_dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "# order according to increasing score\n",
    "scores_pr = [pr_auc_msr, pr_auc_margin, pr_auc_entropy, pr_auc_dropout, pr_auc_gmm, pr_auc_svm, pr_auc_df]\n",
    "\n",
    "recalls = [recall_msr, recall_margin, recall_entropy, recall_dropout, recall_gmm, recall_svm, recall_df]\n",
    "precisions = [precision_msr, precision_margin, precision_entropy, precision_dropout, \n",
    "              precision_gmm, precision_svm, precision_df]\n",
    "\n",
    "names_methods = np.array(['MSR', 'Margin', 'Entropy', 'Dropout', 'GMM', 'OC SVM', 'DF'])\n",
    "scores_order = np.argsort(scores_pr)\n",
    "colors_lines = plt.cm.rainbow(np.linspace(0, 1, len(scores_pr)))[:, :3]\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "for i in scores_order:\n",
    "    plt.step(recalls[i], precisions[i], where='post', c=colors_lines[i])\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.grid(alpha=.3)\n",
    "fig.axes[0].spines['right'].set_visible(False)\n",
    "fig.axes[0].spines['top'].set_visible(False)\n",
    "plt.legend([str.format('%s: %.2f') % (names_methods[i], scores_pr[i]) for i in scores_order], title=\"PR AUC\")\n",
    "plt.savefig(\"../Figures/Zurich/Metrics/PR_pred_wo_cl_\" + str(class_to_remove) + \".pdf\", \n",
    "            bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "# order according to increasing score\n",
    "scores_auc = [auroc_msr, auroc_margin, auroc_entropy, auroc_dropout, auroc_gmm, auroc_svm, auroc_df]\n",
    "fprs = [fpr_msr, fpr_margin, fpr_entropy, fpr_dropout, fpr_gmm, fpr_svm, fpr_df]\n",
    "tprs = [tpr_msr, tpr_margin, tpr_entropy, tpr_dropout, tpr_gmm, tpr_svm, tpr_df]\n",
    "scores_order = np.argsort(scores_auc)\n",
    "colors_lines = plt.cm.rainbow(np.linspace(0, 1, len(scores_auc)))[:, :3]\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "for i in scores_order:\n",
    "    plt.plot(fprs[i], tprs[i], c=colors_lines[i])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', c='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.grid(alpha=.3)\n",
    "fig.axes[0].spines['right'].set_visible(False)\n",
    "fig.axes[0].spines['top'].set_visible(False)\n",
    "plt.legend([str.format('%s: %.2f') % (names_methods[i], scores_auc[i]) for i in scores_order], title=\"AUROC\")\n",
    "plt.savefig(\"../Figures/Zurich/Metrics/ROC_pred_wo_cl_\" + str(class_to_remove) + \".pdf\", \n",
    "            bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write best hyperparameters to CSV file\n",
    "best_params = {'gmm_n_components': best_params_gmm['n_components'], \n",
    "               'oc-svm_k': best_params_svm['kernel'], \n",
    "               'oc-svm_deg': best_params_svm['degree'], \n",
    "               'oc_svm_nu': best_params_svm['nu'], \n",
    "               'df_depth': best_params_df['max_depth'], \n",
    "               'df_min_ig': best_params_df['ig_improvement']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to CSV files\n",
    "# hyperparameters\n",
    "df_ps = pd.read_csv('models_out/hyperparams.csv', index_col=0)\n",
    "df2 = pd.DataFrame(best_params, index=[str(names[class_to_remove])])\n",
    "df_ps = df_ps.append(df2)\n",
    "df_ps = df_ps[~df_ps.index.duplicated(keep='last')]  # avoid duplicates\n",
    "df_ps.to_csv('models_out/hyperparams.csv')\n",
    "\n",
    "# AUROC\n",
    "df_auroc = pd.read_csv('models_out/auroc_all.csv', index_col=0)\n",
    "df2 = pd.DataFrame({str(names[class_to_remove]): scores_auc}, index = names_methods).T\n",
    "df_auroc = df_auroc.append(df2)\n",
    "df_auroc = df_auroc[~df_auroc.index.duplicated(keep='last')]  # avoid duplicates\n",
    "df_auroc.to_csv('models_out/auroc_all.csv')\n",
    "\n",
    "\n",
    "# PR AUC\n",
    "df_aucpr = pd.read_csv('models_out/aucpr_all.csv', index_col=0)\n",
    "df2 = pd.DataFrame({str(names[class_to_remove]): scores_pr}, index = names_methods).T\n",
    "df_aucpr = df_aucpr.append(df2)\n",
    "df_aucpr = df_aucpr[~df_aucpr.index.duplicated(keep='last')]  # avoid duplicates\n",
    "df_aucpr.to_csv('models_out/aucpr_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load auroc df with previously saved results\n",
    "df_auroc = pd.read_csv('models_out/auroc_all.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder by class names\n",
    "df_auroc = df_auroc.reindex(names[1:])\n",
    "df_auroc.to_csv('models_out/auroc_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show latex table of AUROC metrics for all methods\n",
    "print(df_auroc.round(2).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show best method for each left-out class\n",
    "df_auroc.T.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show mean aourc for each method\n",
    "df_auroc.mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show mean aourc for each method\n",
    "print(df_ps.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dpi = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_figure_matplotlib(arr, f_name=None, dpi=255, resize_fact=1, plt_show=False, rect=None):\n",
    "    \"\"\"\n",
    "    Export array as figure in original resolution\n",
    "    :param arr: array of image to save in original resolution\n",
    "    :param f_name: name of file where to save figure\n",
    "    :param resize_fact: resize facter wrt shape of arr, in (0, np.infty)\n",
    "    :param dpi: dpi of your screen\n",
    "    :param plt_show: show plot or not\n",
    "    :param rect: rectangle to overlay over image\n",
    "    \"\"\"\n",
    "    # plot\n",
    "    fig = plt.figure(frameon=False)\n",
    "    fig.set_size_inches(arr.shape[1] / dpi, arr.shape[0] / dpi)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    ax.imshow(arr, cmap='RdYlGn')\n",
    "\n",
    "    # add rectangle overlay\n",
    "    if rect is not None:\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # save figure\n",
    "    if f_name is not None:\n",
    "        plt.savefig(f_name, dpi=(dpi * resize_fact))\n",
    "\n",
    "    if plt_show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_particularity(dataset, img_idx, x, y, width, height, confs_patches=None, names=None, dir_out=None, idx=0):\n",
    "    \"\"\"\n",
    "    export detail view of interesting image: image with blue rectangle, cropped region, cropped GT, cropped conf images\n",
    "    \"\"\"\n",
    "    # Image\n",
    "    class_to_remove = dataset.class_to_remove\n",
    "    im = dataset.imgs[img_idx][..., :3]\n",
    "    max_x = np.mod(im.shape[0], 32)\n",
    "    max_y = np.mod(im.shape[1], 32)\n",
    "    im = im[:-max_x, :-max_y]\n",
    "    rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='b', facecolor='b', alpha=.5)\n",
    "    f_name = dir_out+ 'im_' + str(img_idx) + '_obj_' + str(idx) + '_im.jpg'\n",
    "    fig = export_figure_matplotlib(im, dpi=my_dpi, rect=rect, f_name=f_name)\n",
    "    plt.close()\n",
    "    \n",
    "    # Cropped Image\n",
    "    im_crop = im[y:(y + height), x:(x + width)]\n",
    "    f_name = dir_out+ 'im_' + str(img_idx) + '_obj_' + str(idx) + '_im_crop.jpg'\n",
    "    fig = export_figure_matplotlib(im_crop, dpi=my_dpi, f_name=f_name)\n",
    "    plt.close()\n",
    "    \n",
    "    # GT\n",
    "    rect = patches.Rectangle((x, y), width, height, linewidth=3, edgecolor='b', facecolor='None')\n",
    "    im_gt = gt_label_to_color(dataset.gt[img_idx], colors)*255\n",
    "    max_x = np.mod(im_gt.shape[0], 32)\n",
    "    max_y = np.mod(im_gt.shape[1], 32)\n",
    "    im_gt = im_gt[:-max_x, :-max_y]\n",
    "    f_name = dir_out+ 'im_' + str(img_idx) + '_obj_' + str(idx) + '_gt.jpg'\n",
    "    fig = export_figure_matplotlib(im_gt, dpi=my_dpi, rect=rect, f_name=f_name)\n",
    "    plt.close()\n",
    "\n",
    "    if confs_patches is not None:\n",
    "        for idx_conf,conf_patches in enumerate(confs_patches):\n",
    "            conf_im = convert_patches_to_image(dataset.imgs, conf_patches[..., np.newaxis], 64, 64)\n",
    "            conf_im = exposure.equalize_hist(conf_im[img_idx])\n",
    "            rect = patches.Rectangle((x, y), width, height, linewidth=2, edgecolor='b', facecolor='None')\n",
    "            f_name = dir_out+ 'im_' + str(img_idx) + '_obj_' + str(idx) + '_' + names[idx_conf] + '_wo_cl_'+ str(class_to_remove) + '.jpg'\n",
    "            export_figure_matplotlib(conf_im, dpi=my_dpi, rect=rect, f_name=f_name)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_confs_patches = ['msr', 'df']\n",
    "confs_patches = [probas_patches_msr, probas_patches_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_out = '../Figures/Zurich/Detail/im1/'\n",
    "export_particularity(data_test, 1, 270, 660, 200, 200, dir_out=dir_out, confs_patches=confs_patches, names=names_confs_patches, idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_out = '../Figures/Zurich/Detail/im3/'\n",
    "confs_patches = [probas_patches_msr, probas_patches_df] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_out = '../Figures/Zurich/Detail/im4/'\n",
    "export_particularity(data_test, 4, 670, 550, 450, 320, dir_out=dir_out, confs_patches=confs_patches, names=names_confs_patches, idx=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot confidence in t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_f_tsne = data_test.gt_patches == class_to_remove\n",
    "pred_t_tsne = (data_test.gt_patches != class_to_remove) & (data_test.gt_patches != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of correctly / wrongly predicted points\n",
    "pred_f = pred_f_tsne.flatten()[dataset_subset_indices]\n",
    "pred_t = pred_t_tsne.flatten()[dataset_subset_indices]\n",
    "probas_methods = [probas_msr, probas_margin, probas_entropy, probas_dropout.flatten(), probas_gmm, \n",
    "                  np.squeeze(probas_svm), np.squeeze(probas_df)]\n",
    "base_dir = \"../Figures/Zurich/Im_cert/cl_\" + str(class_to_remove) + \"/\"\n",
    "\n",
    "names_methods = ['net_msr', 'net_margin', 'net_entropy', 'dropout', 'gmm', 'svm', 'df']\n",
    "for proba, name in zip(probas_methods, names_methods):\n",
    "    probas_tsne = proba[dataset_subset_indices]\n",
    "\n",
    "    # colors\n",
    "    probas_df_c = probas_tsne\n",
    "    probas_df_c = exposure.equalize_hist(probas_tsne)\n",
    "    probas_df_c -= np.min(probas_df_c)\n",
    "    probas_df_c /= np.max(probas_df_c)\n",
    "    colors_plt = plt.cm.RdYlGn(probas_df_c * 255)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10)) \n",
    "    # indicators of least confident points    \n",
    "    # plot correctly predicted points (o marker)\n",
    "\n",
    "    ax.scatter(tsne_all[:, 0][pred_t], tsne_all[:, 1][pred_t], c=colors_plt[pred_t], alpha=.1)\n",
    "\n",
    "    # plot incorrectly predicted points (x marker)\n",
    "    ax.scatter(tsne_all[:, 0][pred_f], tsne_all[:, 1][pred_f], c=colors_plt[pred_f], edgecolors='black',\n",
    "               linewidths=.5, s=90)\n",
    "    ax.set_axis_off()\n",
    "    extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "\n",
    "    f_name = base_dir + \"t-SNE_wo_cl_\" + str(class_to_remove) + \"_\" + str(name) + \".pdf\"\n",
    "    plt.savefig(f_name, bbox_inches=extent, pad_inches=0)\n",
    "    ax.legend(['Seen points', 'Novel points'])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices of correctly / wrongly predicted points\n",
    "probas_methods = [probas_msr, probas_margin, probas_entropy, probas_dropout.flatten(), probas_gmm, \n",
    "                  np.squeeze(probas_svm), np.squeeze(probas_df)]\n",
    "base_dir = \"../Figures/Zurich/Im_cert/cl_\" + str(class_to_remove) + \"/\"\n",
    "\n",
    "names_methods = ['net_msr', 'net_margin', 'net_entropy', 'dropout', 'gmm', 'svm', 'df']\n",
    "for proba, name in zip(probas_methods, names_methods):\n",
    "    pred_f = pred_f_tsne.flatten()[dataset_subset_indices]\n",
    "    pred_t = pred_t_tsne.flatten()[dataset_subset_indices]\n",
    "    probas_tsne = proba[dataset_subset_indices]\n",
    "\n",
    "    # colors\n",
    "    colors_plt = plt.cm.RdYlGn((probas_tsne > np.sort(probas_tsne)[tsne_pts_per_class]) * 255)\n",
    "\n",
    "    # plot correctly predicted points (o marker)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10)) \n",
    "    ax.scatter(tsne_all[:, 0][pred_t], tsne_all[:, 1][pred_t], c=colors_plt[pred_t], alpha=.2)\n",
    "\n",
    "    # plot incorrectly predicted points (x marker)\n",
    "    ax.scatter(tsne_all[:, 0][pred_f], tsne_all[:, 1][pred_f], c=colors_plt[pred_f], edgecolors='black',\n",
    "               linewidths=.5, s=90)\n",
    "    \n",
    "    ax.set_axis_off()\n",
    "    extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    f_name = base_dir + \"t-SNE_wo_cl_\" + str(class_to_remove) + \"_\" + str(name) + \"_leastcert.pdf\"\n",
    "    plt.savefig(f_name, bbox_inches=extent, pad_inches=0)\n",
    "    ax.legend(['Seen points', 'Novel points'])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_remove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
