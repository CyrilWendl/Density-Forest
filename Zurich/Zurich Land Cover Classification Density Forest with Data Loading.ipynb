{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zurich Land Cover Classification\n",
    "\n",
    "This script presents a visualization of training a U-Net classifier on 7 out of 8 available land cover classes of the Zurich dataset, and detecting the unseen class using a Density Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9121097056035684139\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10974989517\n",
      "locality {\n",
      "  bus_id: 2\n",
      "}\n",
      "incarnation: 2997318934205998475\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# python libraries\n",
    "from matplotlib.patches import Rectangle\n",
    "from multiprocessing import cpu_count\n",
    "from sklearn.manifold import TSNE\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# custom libraries\n",
    "from helpers.helpers import *\n",
    "from helpers.data_augment import *\n",
    "from keras_helpers.unet import *\n",
    "from keras_helpers.helpers import *\n",
    "from keras_helpers.callbacks import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.utils import class_weight\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "# custom libraries\n",
    "base_dir = '/raid/home/cwendl'  # for guanabana\n",
    "sys.path.append(base_dir + '/SIE-Master/Code') # Path to density Tree package\n",
    "sys.path.append(base_dir + '/SIE-Master/Code/density_tree') # Path to density Tree package\n",
    "from density_tree.density_forest import *\n",
    "from density_tree.plots import plot_tsne, plot_pca\n",
    "from helpers.plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_remove = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: 20 \n",
      "ground truth images: 20 \n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "\n",
    "imgs, gt = load_data(path)\n",
    "\n",
    "# gt to labels\n",
    "# Next, we need to convert the ground truth (colors) to labels \n",
    "legend = OrderedDict((('Background', [255, 255, 255]),\n",
    "                      ('Roads', [0, 0, 0]),\n",
    "                      ('Buildings', [100, 100, 100]),\n",
    "                      ('Trees', [0, 125, 0]),\n",
    "                      ('Grass', [0, 255, 0]),\n",
    "                      ('Bare Soil', [150, 80, 0]),\n",
    "                      ('Water', [0, 0, 150]),\n",
    "                      ('Railways', [255, 255, 0]),\n",
    "                      ('Swimming Pools', [150, 150, 255])))\n",
    "\n",
    "# get class names by increasing value (as done above)\n",
    "names, colors = [], []\n",
    "for name, color in legend.items():\n",
    "    names.append(name)\n",
    "    colors.append(color)\n",
    "\n",
    "gt = gt_color_to_label(gt, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Get patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get patches\n",
    "patch_size = 64\n",
    "stride_train = 64  # has to be <= patch_size\n",
    "stride_test = 32  # has to be <= patch_size\n",
    "\n",
    "# ids for training, validation and test sets (0-19)\n",
    "ids_train = np.arange(0, 12)\n",
    "ids_val = np.arange(12, 16)\n",
    "ids_test = np.arange(16, 20)\n",
    "\n",
    "# get training, test and validation sets\n",
    "x_train = get_padded_patches(imgs[ids_train], patch_size=patch_size, stride=stride_train)\n",
    "x_val = get_padded_patches(imgs[ids_val], patch_size=patch_size, stride=stride_train)\n",
    "x_test = get_padded_patches(imgs[ids_test], patch_size=patch_size, stride=stride_test)\n",
    "x_test_nostride = get_padded_patches(imgs[ids_test], patch_size=patch_size, stride=patch_size)\n",
    "\n",
    "y_train = get_gt_patches(gt[ids_train], patch_size=patch_size, stride=stride_train)\n",
    "y_val = get_gt_patches(gt[ids_val], patch_size=patch_size, stride=stride_train)\n",
    "y_test = get_gt_patches(gt[ids_test], patch_size=patch_size, stride=stride_test)\n",
    "y_test_nostride = get_gt_patches(gt[ids_test], patch_size=patch_size, stride=patch_size)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(x_test_nostride.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize some patches \n",
    "imgs_row = 8\n",
    "fig, axes = plt.subplots(3, imgs_row)\n",
    "fig.set_size_inches(20, 8)\n",
    "offset = 0\n",
    "alpha = .6\n",
    "for i in range(offset, offset + imgs_row):\n",
    "    axes[0][i - offset].imshow(x_test[i][..., :3])  # images\n",
    "    axes[1][i - offset].imshow(\n",
    "        gt_label_to_color(y_test[i], colors) * alpha + x_test[i][..., :3] * (1 - alpha))  # ground truth (overlay)\n",
    "    axes[2][i - offset].imshow(gt_label_to_color(y_test[i], colors))  # ground truth\n",
    "\n",
    "# corresponding part of image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(imgs[16][:64, :64 * 8, :3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras CNN\n",
    "\n",
    "Data Split: \n",
    "- Training: 12 images\n",
    "- Validation: 4 images\n",
    "- Test: 4 images\n",
    "\n",
    "Tested Architectures: \n",
    "\n",
    "| Model | Patch Size | Data Augmentations | Number of Parameters | Testing Precision (avg) | Testing Recall (avg) | Testing f1 score (avg) | Validation / Test accuracy |\n",
    "| ------- | ------- | ------- | ------- | ------- | ------- |\n",
    "| U-Net | 64 | Rot 90°, Flipping  | 7,828,200 | 0.87 | 0.858 | 0.86 | t |\n",
    "| U-Net | 128 | Rot 90°, Flipping  | 7,828,200 | 0.69 | 0.61 | 0.64 | t |\n",
    "| U-Net | 128 | Rot 90°, Flipping  | 7,828,200 | 0.90 | 0.89 | 0.89 | v |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copies of original data\n",
    "y_train_label = y_train.copy()\n",
    "y_val_label = y_val.copy()\n",
    "y_test_label = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get class weights\n",
    "labels_unique = np.unique(y_train.flatten())\n",
    "print(labels_unique)\n",
    "class_weights = class_weight.compute_class_weight('balanced', labels_unique, y_train.flatten())\n",
    "class_weights[0] = 0  # give less weight to background label class\n",
    "class_weights[5] = 7  # give less weight to bare soil class\n",
    "class_weights[8] = 7  # give less weight to swimming pool class\n",
    "\n",
    "print(\"Class weights:\")\n",
    "for i, w in enumerate(class_weights):\n",
    "    print(\"%15s: %3.3f\" % (names[i], w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_classes = 9\n",
    "\n",
    "# convert to numpy arrays\n",
    "x_train = np.asarray(x_train)\n",
    "x_val = np.asarray(x_val)\n",
    "x_test = np.asarray(x_test)\n",
    "\n",
    "# make y data categorical\n",
    "y_train = to_categorical(y_train_label, n_classes)\n",
    "y_val = to_categorical(y_val_label, n_classes)\n",
    "\n",
    "# remove class\n",
    "classes_to_keep = np.asarray([x for x in range(1, n_classes) if x != class_to_remove])\n",
    "\n",
    "names_keep = np.asarray(names)[classes_to_keep]\n",
    "names_keep = names_keep.tolist()\n",
    "print(\"classes to keep: \" + str(names_keep))\n",
    "\n",
    "y_train = y_train[..., classes_to_keep]\n",
    "y_val = y_val[..., classes_to_keep]\n",
    "n_classes = len(classes_to_keep)\n",
    "class_weights = class_weights[classes_to_keep]\n",
    "\n",
    "# print shapes of variables\n",
    "for var in x_train, y_train, x_val, y_val:\n",
    "    print(np.shape(var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "img_idx = 14\n",
    "im_vis, gt_vis = augment_images_and_gt(x_train[img_idx], y_train_label[img_idx], rf_h=True,\n",
    "                                                   rf_v=True, rot=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "fig.set_size_inches((10, 5))\n",
    "axes[0].imshow(x_train[img_idx][..., :3])\n",
    "axes[1].imshow(im_vis[..., :3])\n",
    "axes[2].imshow(gt_vis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks (evaluated every epoch)\n",
    "# show loss and accuracy figures after each epoch\n",
    "callback_plot = PlotLosses()\n",
    "\n",
    "# stop early if after several epochs the accuracy doesn't improve\n",
    "callback_earlystop = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=24, verbose=1, mode='auto')\n",
    "\n",
    "# decrease learning rate when accuracy stops improving\n",
    "callback_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=12, verbose=1, mode='auto',\n",
    "                                epsilon=1e-4, cooldown=0, min_lr=1e-8)\n",
    "\n",
    "# checkpoint to save weights at every epoch (in case of interruption)\n",
    "file_path = \"weights-improvement.hdf5\"\n",
    "callback_checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "\n",
    "callback_tensorboard = TensorBoard(log_dir='./tensorboard', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "# model setup\n",
    "batch_size = 20\n",
    "epochs = 300\n",
    "\n",
    "\n",
    "def model_train(model, data_augmentation):\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(batch_generator(x_train, y_train,\n",
    "                                        batch_size=batch_size, data_augmentation=data_augmentation),\n",
    "                        steps_per_epoch=int(np.ceil(x_train.shape[0] / float(batch_size))),\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        class_weight=class_weights,  # weights for loss function\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        callbacks=[callback_earlystop,\n",
    "                                   callback_lr,\n",
    "                                   #callback_checkpoint,\n",
    "                                   callback_plot,\n",
    "                                   callback_tensorboard],\n",
    "                        workers=cpu_count(),\n",
    "                        use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train or load model\n",
    "# train the model\n",
    "#model_unet = get_unet(n_classes, x_train.shape[1:])\n",
    "#model_train(model_unet, data_augmentation=True)\n",
    "#model_unet.save('models_out/model_unet_64_flip_rot90_wo_cl_' + str(names[class_to_remove]).lower() + '_2.h5')  # save model, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "name_model = path + '/models_out/model_unet_64_flip_rot90_wo_cl_' + str(names[class_to_remove]).lower() + '.h5'    \n",
    "model_unet = load_model(name_model, custom_objects={'fn': ignore_background_class_accuracy(0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2. Prediction on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction\n",
    "y_pred = model_unet.predict(x_test, batch_size=20, verbose=1)\n",
    "\n",
    "# prediction patches without overlapping patches\n",
    "y_pred = np.concatenate(remove_overlap(imgs, y_pred, ids_test, 64, 32))\n",
    "\n",
    "# get label\n",
    "y_pred_label = get_y_pred_labels(y_pred, class_to_remove=class_to_remove)\n",
    "\n",
    "# get accuracy as softmax pseudo-probability\n",
    "y_pred_acc = np.max(y_pred, axis=-1)\n",
    "\n",
    "# Get accuracy as margin between highest and second highest class\n",
    "y_pred_acc_margin = get_acc_net_max_margin(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction image\n",
    "y_pred_acc_imgs = [convert_patches_to_image(imgs, y_pred_acc[..., np.newaxis],\n",
    "                                       img_idx=idx_im, img_start=ids_test[0], patch_size=64,\n",
    "                                       stride=64) for idx_im in ids_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot prediction results\n",
    "im_idx = 15\n",
    "alpha = .3  # for overlay\n",
    "fig, axes = plt.subplots(1, 6)\n",
    "fig.set_size_inches(20, 20)\n",
    "fig_im = x_test[im_idx][..., :3] * (1 - alpha)\n",
    "fig_test = gt_label_to_color(y_test_label[im_idx], colors)\n",
    "fig_pred = gt_label_to_color(y_pred_label[im_idx], colors)\n",
    "\n",
    "# plots\n",
    "axes[0].imshow(fig_im)\n",
    "axes[1].imshow(fig_test)\n",
    "axes[2].imshow(fig_test * alpha + fig_im * (1 - alpha))\n",
    "axes[3].imshow(fig_pred)\n",
    "axes[4].imshow(fig_pred * alpha + fig_im * (1 - alpha))\n",
    "axes[5].imshow(fig_im * 0 + 1)\n",
    "\n",
    "# titles\n",
    "axes[0].set_title(\"Test image\")\n",
    "axes[1].set_title(\"Ground truth\")\n",
    "axes[2].set_title(\"Ground truth (overlay)\")\n",
    "axes[3].set_title(\"Predicted Image\")\n",
    "axes[4].set_title(\"Predicted Image (overlay)\")\n",
    "axes[5].set_title(\"Legend\")\n",
    "\n",
    "# legend\n",
    "legend_data = [[l[0], l[1]] for l in legend.items()]\n",
    "handles = [Rectangle((0, 0), 1, 1, color=[v / 255 for v in c]) for n, c in legend_data]\n",
    "labels = np.asarray([n for n, c in legend_data])\n",
    "axes[5].legend(handles, labels)\n",
    "\n",
    "# show certitude by network\n",
    "fig = plt.figure()\n",
    "plt.imshow(y_pred_acc[im_idx], cmap='gray')\n",
    "plt.title(\"Network confidence\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_im = [convert_patches_to_image(imgs, gt_label_to_color(y_pred_label, colors), img_idx=i, img_start=16, patch_size=64,\n",
    "                             stride=64) for i in ids_test]\n",
    "\n",
    "for img_idx in ids_test:\n",
    "    # Pred\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(y_pred_im[img_idx-16])  # prediction\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"../Figures/Pred/im_\" + str(img_idx+1) + \"_pred_wo_cl_\" + str(class_to_remove) + \".pdf\", bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    # GT\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.imshow(gt_label_to_color(gt[img_idx],colors))  # gt stitched together\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"../Figures/Im/gt_\" + str(img_idx+1) + \".pdf\", bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "\n",
    "    # show also original image\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(imgs[img_idx][:, :, :3])\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"../Figures/Im/im_\" + str(img_idx+1) + \".pdf\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.title(\"original image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Accuracy Metrics (Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metrics\n",
    "y_pred_flattened= np.asarray(y_pred_label.flatten()).astype('int')\n",
    "y_test_flattened= np.asarray(y_test_nostride.flatten()).astype('int')\n",
    "\n",
    "# mask background and removed classes for evaluation metrics\n",
    "filter_items = (y_test_flattened != 0) & (y_test_flattened != class_to_remove)\n",
    "\n",
    "# Class accuracy, average accuracy\n",
    "print(metrics.classification_report(\n",
    "    y_test_flattened[filter_items],\n",
    "    y_pred_flattened[filter_items],\n",
    "    target_names=names_keep,\n",
    "    digits=3))\n",
    "\n",
    "\n",
    "# Overall accuracy\n",
    "OA = metrics.accuracy_score(y_test_flattened[filter_items], y_pred_flattened[filter_items])\n",
    "print(\"Overall accuracy: %.3f %%\" % (OA*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to log file\n",
    "precision, recall, fscore, support = metrics.precision_recall_fscore_support(y_test_flattened[filter_items], y_pred_flattened[filter_items])\n",
    "df = pd.DataFrame(data={'Precision':precision,\n",
    "                        'Recall':recall,\n",
    "                       'f1-score':fscore,\n",
    "                       'support':support}, index=names_keep)\n",
    "\n",
    "df.index.name = 'Class'\n",
    "with open(\"models_out/acc_class_\" + str(class_to_remove) + \".csv\", 'w') as f:\n",
    "    print(df.to_latex(float_format='%.3f'), file=f)  # Python 3.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Certainty using Density Forest\n",
    "### 4.1. Retrieve Activations, PCA, t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# image, layer indexes\n",
    "layer_idx = -2\n",
    "img_idx = 2\n",
    "batch_size = 20\n",
    "\n",
    "# get activations for training Density Forest\n",
    "act_train = get_activations(imgs, model_unet, layer_idx, x_train, ids_train, batch_size=160, patch_size=64, stride=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get activations for seen classes\n",
    "\n",
    "# retain only activation weights for which there is a ground truth\n",
    "filter_seen = (y_train_label != 0) & (y_train_label != class_to_remove)\n",
    "act_train_seen = np.concatenate(act_train)[filter_seen] \n",
    "\n",
    "# all but those belonging to background\n",
    "act_train = np.concatenate(act_train)[y_train_label != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get activations for testing Density Forest\n",
    "act_test = get_activations(imgs, model_unet, layer_idx, x_test, ids_test, batch_size=160, patch_size=64, stride=32)\n",
    "\n",
    "# remove test activations overlap\n",
    "act_test = remove_overlap(imgs, np.concatenate(act_test), ids_test, patch_size=64, stride=32) \n",
    "\n",
    "# all labels, including background\n",
    "act_test = np.concatenate(act_test, axis=0)[y_test_nostride < np.infty] # convert to 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get balanced data subset to show in figure\n",
    "pts_per_class = 100  # same number of points per class\n",
    "dataset_subset_indices = []\n",
    "for class_label in range(1, 9):\n",
    "    ds_subset_ind = np.where(y_test_nostride[y_test_nostride<np.infty]==class_label)[0]\n",
    "    dataset_subset_indices.append(np.random.choice(ds_subset_ind, size=pts_per_class, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=500)\n",
    "tsne_all = tsne.fit_transform(act_test[np.concatenate(dataset_subset_indices)])\n",
    "\n",
    "plot_tsne(tsne_all, class_to_remove, classes_to_keep, pts_per_class, colors, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create density tree for activation weights of training data\n",
    "# PCA\n",
    "pca = decomposition.PCA(n_components=5)\n",
    "pca.fit(act_train)  # fit on training set without background pixels\n",
    "n_components = np.alen(pca.explained_variance_ratio_)\n",
    "print(\"Variance explained by first %i components: %.2f\" % (\n",
    "    n_components, sum(pca.explained_variance_ratio_)))\n",
    "\n",
    "# transform training activations\n",
    "act_train_seen = pca.transform(act_train_seen)\n",
    "\n",
    "# transform test set activations\n",
    "act_test = pca.transform(act_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization after PCA\n",
    "tsne_all = tsne.fit_transform(act_test[np.concatenate(dataset_subset_indices)])\n",
    "\n",
    "# plot\n",
    "plot_tsne(tsne_all, class_to_remove, classes_to_keep, pts_per_class, colors, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PCA point\n",
    "plot_pca(act_test, class_to_remove, classes_to_keep, names, dataset_subset_indices, colors)\n",
    "\n",
    "print(\"Variance explained by first 3 components: %.2f\" % np.sum(pca.explained_variance_ratio_[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Train Density Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters importance:**\n",
    "- `n_trees`: minor relevance (10 is enough)\n",
    "- `min_subset`: important, smaller number <-> more clusters (requires `subset_data` to be high enough)\n",
    "- `subset_data`: irrelevant, but should be higher if `min_subset` is lower\n",
    "- `max_depth`: important, greater depth <-> more clusters\n",
    "- `fact_improvement`: important, smaller minimum factor <-> more clusters\n",
    "- `n_max_dim`: unimportant, better to set to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 10\n",
    "max_depth = 3\n",
    "subsample_pct = .005\n",
    "min_subset = .001\n",
    "fact_improvement = .1\n",
    "n_max_dim = -1\n",
    "n_jobs=-1\n",
    "\n",
    "root_nodes_seen = df_create(act_train_seen, max_depth=max_depth, min_subset=min_subset, n_trees=n_trees, \n",
    "                                        n_max_dim=n_max_dim, subsample_pct=subsample_pct, n_jobs=n_jobs, verbose=10, \n",
    "                                        fact_improvement=fact_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilities for all images\n",
    "probas = df_traverse_batch(act_test, root_nodes_seen, n_jobs=-1, batch_size=10000, verbosity = 10, standardize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Post-Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape probas to (n_patches, patch_size, patch_size)\n",
    "patches_start = get_offset(imgs, 64, 64, 16, 16) # idx of first patch in image\n",
    "patches_end = get_offset(imgs, 64, 64, 16, 20) # idx of first patch in image\n",
    "n_patches = patches_end - patches_start\n",
    "probas_seen_im = np.reshape(probas, (n_patches,patch_size,patch_size))\n",
    "\n",
    "# transformations\n",
    "#probas_seen_im[probas_seen_im==0]=1e-5  # for log\n",
    "#probas_seen_im = np.log(probas_seen_im)\n",
    "probas_seen_im -= np.nanmin(probas_seen_im)\n",
    "probas_seen_im /= np.nanmax(probas_seen_im)\n",
    "#probas_seen_im = 1-probas_seen_im\n",
    "\n",
    "# remove outliers\n",
    "#probas_seen_im[abs(probas_seen_im - np.nanmean(probas_seen_im)) > m * np.nanstd(probas_seen_im)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save probabilities corresponding to an image in an array\n",
    "probas_imgs = [] # (n_imgs, n_patches, patch_size, patch_size)\n",
    "for idx_im in ids_test:\n",
    "    patches_start = get_offset(imgs, 64, 64, 16, idx_im) # idx of first patch in image\n",
    "    patches_end = get_offset(imgs, 64, 64, 16, idx_im+1) # idx of last patch in image\n",
    "    probas_im = np.asarray(probas_seen_im[patches_start:patches_end])\n",
    "    probas_imgs.append(probas_im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Figures, Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx_im in ids_test:\n",
    "    im_cert_out = convert_patches_to_image(imgs, probas_imgs[idx_im-16][..., np.newaxis],\n",
    "                                           img_idx=idx_im, patch_size=64,\n",
    "                                           stride=64, img_start=idx_im)\n",
    "\n",
    "    \n",
    "    #im_cert_out = imgs_stretch_eq([im_cert_out])\n",
    "    #im_cert_out = im_cert_out[...,0][0]\n",
    "    im_cert_out = im_cert_out[...,0]\n",
    "    \n",
    "    plt.figure(figsize=(16, 20))\n",
    "    plt.imshow(im_cert_out, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"../Figures/DF/cl_\" +  str(class_to_remove) + \"/im_\" + str(idx_im + 1) + \"_confidence_cl_\" + str(class_to_remove) +\"_DF.pdf\", bbox_inches='tight', pad_inches=0);\n",
    "\n",
    "    plt.figure(figsize=(16, 20))\n",
    "    plt.imshow(y_pred_acc_imgs[idx_im-16][...,0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\"../Figures/DF/cl_\" +  str(class_to_remove) + \"/im_\" + str(idx_im + 1) + \"_confidence_cl_\" + str(class_to_remove) +\"_Net.pdf\", bbox_inches='tight', pad_inches=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert patches to image\n",
    "idx_im = 16\n",
    "\n",
    "# show certitude by network\n",
    "# image, overlay\n",
    "im_cert_out = convert_patches_to_image(imgs, probas_imgs[idx_im-16][..., np.newaxis],\n",
    "                                       img_idx=idx_im, patch_size=64,\n",
    "                                       stride=64, img_start=idx_im)[...,0]\n",
    "\n",
    "#im_cert_out = imgs_stretch_eq([im_cert_out])[0]\n",
    "\n",
    "def fig_uncertainty(thresh_2, thresh_3=.3, save=False, show=True):\n",
    "    \n",
    "    # part of image overlapping with uncertainty image\n",
    "    img_part = imgs[idx_im][:im_cert_out.shape[0],:im_cert_out.shape[1],:3]\n",
    "    fact_mult = 1\n",
    "    im_overlay = get_fig_overlay_fusion(img_part, im_cert_out*fact_mult, y_pred_acc_imgs[idx_im-16][...,0],\n",
    "                             thresh_2=thresh_2, thresh_3=thresh_3, opacity=.5)\n",
    "    \n",
    "    im_overlay_1 = get_fig_overlay(img_part, im_cert_out*fact_mult,\n",
    "                             thresh=thresh_2, opacity=.5)\n",
    "    \n",
    "    im_overlay_2 = get_fig_overlay(img_part, y_pred_acc_imgs[idx_im-16][...,0],\n",
    "                                   thresh=thresh_3, opacity=.5)\n",
    "    # y_pred_acc_imgs[idx_im-16][...,0]\n",
    "\n",
    "    fig = plt.figure(figsize=(10,12))\n",
    "    plt.imshow(im_overlay)\n",
    "    plt.axis('off')\n",
    "    if save:\n",
    "        plt.savefig(\"../Figures/DF/cl_\" +  str(class_to_remove) + \"/both_thresh_im_\" + str(idx_im+1) + \"wo_cl_\" +\n",
    "                    str(names[class_to_remove]).lower() + \".pdf\", bbox_inches='tight', pad_inches=0)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10,12))\n",
    "    axes[0].imshow(im_overlay_1)\n",
    "    axes[0].set_axis_off()\n",
    "    \n",
    "    axes[1].imshow(im_overlay_2)\n",
    "    axes[1].set_axis_off()\n",
    "\n",
    "        \n",
    "\n",
    "min_data, max_data = 0, 1  #  for normal\n",
    "n_steps = 20\n",
    "range_1 = (min_data,max_data,(max_data-min_data)/n_steps)\n",
    "\n",
    "min_data, max_data = .8, 1  #  for net\n",
    "n_steps = 20\n",
    "range_2 = (min_data,max_data,(max_data-min_data)/n_steps)\n",
    "interact(fig_uncertainty, thresh_2=range_1, thresh_3=range_2)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(gt_label_to_color(gt[idx_im], colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average certainty by Density Forest\n",
    "# support for all labels in y_true\n",
    "support = np.unique(y_test_flattened, return_counts=True)[1][1:]\n",
    "\n",
    "probas_patches = np.concatenate(probas_imgs) # all patches concatenated (like y_test)\n",
    "av_cert = []\n",
    "nans = []\n",
    "for label in np.arange(1, 9):\n",
    "    av_cert.append(np.nanmean(probas_patches[y_test_nostride==label]))\n",
    "    nans.append(np.sum(np.isnan(probas_patches[y_test_nostride==label]))/np.sum(np.ones(np.shape(probas_patches[y_test_nostride==label]))))\n",
    "\n",
    "av_cert_w = (av_cert*support)/sum(support)\n",
    "av_cert = np.asarray(av_cert)\n",
    "nans = np.asarray(nans)\n",
    "\n",
    "print(\"Average certainty within class:\")\n",
    "for idx, w in enumerate(av_cert):\n",
    "    print(\"%15s: %3.5f, nans: %.2f%%\" % (names[idx + 1], w*1e5, nans[idx]*100))\n",
    "\n",
    "\n",
    "# ratio unseen class / seen classes\n",
    "cert_unseen = av_cert[class_to_remove - 1]\n",
    "cert_seen = np.nanmean(np.asarray(av_cert)[av_cert != cert_unseen])\n",
    "\n",
    "av_cert_w = (av_cert*support)/sum(support)\n",
    "cert_unseen_w = av_cert_w[class_to_remove - 1]\n",
    "cert_seen_w = np.nanmean(np.asarray(av_cert)[av_cert_w != cert_unseen])\n",
    "\n",
    "print(\"Average certainty unseen class:\\t%.5f\" % cert_unseen)\n",
    "print(\"Average certainty seen classes:\\t%.5f\" % cert_seen)\n",
    "print(\"Ratio between support-weighted cert. of seen classes / unseen class:\\t%.3f\" % (cert_seen_w / cert_unseen_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to log file\n",
    "df = pd.DataFrame(data={'Average Certainty':av_cert,\n",
    "                        'Nans':nans}, index=names[1:])\n",
    "\n",
    "df.index.name = 'Class'\n",
    "with open(\"models_out/acc_DF_class_\" + str(class_to_remove) + \".csv\", 'w') as f:\n",
    "    print(df.to_latex(float_format='%.3f'), file=f)  # Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average certainty by Network\n",
    "av_cert = []\n",
    "nans = []\n",
    "for label in np.arange(1, 9):\n",
    "    av_cert.append(np.nanmean(y_pred_acc[y_test_nostride==label]))\n",
    "av_cert = np.asarray(av_cert)\n",
    "nans = np.asarray(nans)\n",
    "\n",
    "print(\"Average certainty within class:\")\n",
    "for idx, w in enumerate(av_cert):\n",
    "    print(\"%15s: %3.5f\" % (names[idx + 1], w))\n",
    "\n",
    "# ratio unseen class / seen classes\n",
    "cert_unseen = av_cert[class_to_remove - 1]\n",
    "cert_seen = np.nanmean(np.asarray(av_cert)[av_cert != cert_unseen])\n",
    "\n",
    "av_cert_w = (av_cert*support)/sum(support)\n",
    "cert_unseen_w = av_cert_w[class_to_remove - 1]\n",
    "cert_seen_w = np.nanmean(np.asarray(av_cert)[av_cert_w != cert_unseen])\n",
    "\n",
    "print(\"Average certainty unseen class:\\t%.5f\" % cert_unseen)\n",
    "print(\"Average certainty seen classes:\\t%.5f\" % cert_seen)\n",
    "print(\"Ratio between support-weighted cert. of seen classes / unseen class:\\t%.3f\" % (cert_seen_w / cert_unseen_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Accuracy ratios\n",
    "\n",
    "| Accuracy indicator | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | \n",
    "| ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- |\n",
    "| 1A | 4.689 |  | |  | | | | |\n",
    "| 1B | 4.980 | 6.792 | 6.038 | 3.489 | 53.949 | 165.043 | 153.863 | 179.750 |\n",
    "| 2A | 123.389 | 475.483 | 16.604 | 5.188 | 91.749 | 12332.699 | 43.022 | 79770.728 |\n",
    "| 2B | 4.840 | 4.512 | 4.234 | 3.256 | 108.226 | 409.266 | 104.420 | 8511.800 |\n",
    "\n",
    "**Abbreviations**\n",
    "1. Network\n",
    "  1. `softmax` pseudo-probability ratio weighted\n",
    "  2. `max-margin` ratio weighted\n",
    "2. Density Forest\n",
    "  1. average weighted accuracy ratios (`DF with randomization`)\n",
    "  2. average weighted accuracy ratios (`DF std. with randomization`)\n",
    "  \n",
    "#### Observations\n",
    "- *Higher number of splits* (greater depth, min subset of data per leaf smaller): better ratio, but roads and buildings both very low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net_msr = [4.689, 5.581, 5.007,]\n",
    "net_margin = [4.980 , 6.792, 6.038 , 3.489 , 53.949 , 165.043 , 153.863 , 179.750]\n",
    "df = [123.389 , 475.483 , 16.604 , 5.188, 91.749 , 12332.699 , 43.022 , 79770.728]\n",
    "df_std = [4.840 , 4.512 , 4.234 , 3.256 , 108.226 , 409.266 , 104.420 , 8511.800]\n",
    "\n",
    "cl_idx = 10\n",
    "#print(\"%.3f\" % (np.dot(net_msr[:cl_idx],support[:cl_idx])/sum(support[:cl_idx])))\n",
    "print(\"%.3f\" % (np.dot(net_margin[:cl_idx],support[:cl_idx])/sum(support[:cl_idx])))\n",
    "print(\"%.3f\" % (np.dot(df[:cl_idx],support[:cl_idx])/sum(support[:cl_idx])))\n",
    "print(\"%.3f\" % (np.dot(df_std[:cl_idx],support[:cl_idx])/sum(support[:cl_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get precision, recall curve for all pixels in prediction that are not in background\n",
    "y_scores = y_pred_acc.flatten()\n",
    "y_true = (y_test_nostride == y_pred_label).flatten()\n",
    "\n",
    "filt = y_test_nostride.flatten()!=0\n",
    "y_true = y_true[filt]\n",
    "y_scores = y_scores[filt]\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(y_true, y_scores)\n",
    "plot_precision_recall(precision, recall, s_name=\"../Figures/DF/AUC_pred_wo_cl_\" + str(class_to_remove) + \".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get precision, recall curve for all pixels in prediction that are not in background\n",
    "y_scores = probas_patches.flatten()\n",
    "y_true = (y_test_nostride == y_pred_label).flatten()\n",
    "\n",
    "filt = y_test_nostride.flatten()!=0\n",
    "y_true = y_true[filt]\n",
    "y_scores = y_scores[filt]\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(y_true, y_scores)\n",
    "plot_precision_recall(precision, recall, s_name=\"../Figures/DF/AUC_pred_wo_cl_\" + str(class_to_remove) + \".pdf\")#y_scores = probas_patches.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
