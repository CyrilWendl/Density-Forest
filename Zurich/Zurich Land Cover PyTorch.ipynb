{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QI_X4wRY3A_n"
   },
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "67pKQmauiPB0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import torch\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import functional\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from skimage import exposure\n",
    "from skimage.io import imread\n",
    "from skimage.util import *\n",
    "import matplotlib.pyplot as plt\n",
    "import natsort as ns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6y8L-6TcFKm9"
   },
   "outputs": [],
   "source": [
    "# import custom library\n",
    "\n",
    "import sys\n",
    "#base_dir = '/content'  # for Colaboratory\n",
    "#base_dir = '/Users/cyrilwendl/Documents/EPFL'  # for local machine\n",
    "base_dir = '/home/cyrilwendl'  # for GCE\n",
    "\n",
    "sys.path.append(base_dir + '/SIE-Master/Zurich/helpers') # Path to density Tree package\n",
    "sys.path.append(base_dir + '/SIE-Master/Zurich/data_augment') # Path to density Tree package\n",
    "from helpers.helpers import *\n",
    "from helpers.data_augment import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKrhqYZs2v8S"
   },
   "source": [
    "# UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YufUOHbJitCa"
   },
   "outputs": [],
   "source": [
    "# import torch libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1,padding=1, bias=True, groups=1):\n",
    "    return nn.Conv2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "        bias=bias,\n",
    "        groups=groups) \n",
    "\n",
    "def conv1x1(in_channels, out_channels, groups=1):\n",
    "    return nn.Conv2d(\n",
    "      in_channels,\n",
    "      out_channels,\n",
    "      kernel_size=1,\n",
    "      groups=groups,\n",
    "      stride=1)\n",
    "\n",
    "def upconv2x2(in_channels, out_channels, mode='transpose'):\n",
    "    \"\"\"\n",
    "    upsampling helper in mode 'transpose' or 'sequential'\n",
    "    \"\"\"\n",
    "    if mode == 'transpose':\n",
    "        return nn.ConvTranspose2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=2,\n",
    "            stride=2)\n",
    "    else:\n",
    "        # out_channels is always going to be the same\n",
    "        # as in_channels\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "            conv1x1(in_channels, out_channels))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DownConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Module that performs 2 convolutions and 1 MaxPool.\n",
    "    A ReLU activation follows each convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, pooling=True):\n",
    "        super(DownConv, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.pooling = pooling\n",
    "\n",
    "        self.conv1 = conv3x3(self.in_channels, self.out_channels)\n",
    "        self.conv2 = conv3x3(self.out_channels, self.out_channels)\n",
    "\n",
    "        if self.pooling:\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        before_pool = x\n",
    "        if self.pooling:\n",
    "            x = self.pool(x)\n",
    "        return x, before_pool\n",
    "\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Module that performs 2 convolutions and 1 UpConvolution.\n",
    "    A ReLU activation follows each convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 merge_mode='concat', up_mode='transpose'):\n",
    "        super(UpConv, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.merge_mode = merge_mode\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        self.upconv = upconv2x2(self.in_channels, self.out_channels,\n",
    "                                mode=self.up_mode)\n",
    "\n",
    "        if self.merge_mode == 'concat':\n",
    "            self.conv1 = conv3x3(\n",
    "                2 * self.out_channels, self.out_channels)\n",
    "        else:\n",
    "            # num of input channels to conv2 is same\n",
    "            self.conv1 = conv3x3(self.out_channels, self.out_channels)\n",
    "        self.conv2 = conv3x3(self.out_channels, self.out_channels)\n",
    "\n",
    "    def forward(self, from_down, from_up):\n",
    "        \"\"\" Forward pass\n",
    "        Arguments:\n",
    "            from_down: tensor from the encoder pathway\n",
    "            from_up: upconv'd tensor from the decoder pathway\n",
    "        \"\"\"\n",
    "        from_up = self.upconv(from_up)\n",
    "        if self.merge_mode == 'concat':\n",
    "            x = torch.cat((from_up, from_down), 1)\n",
    "        else:\n",
    "            x = from_up + from_down\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\" `UNet` class is based on https://arxiv.org/abs/1505.04597\n",
    "    The U-Net is a convolutional encoder-decoder neural network.\n",
    "    Contextual spatial information (from the decoding,\n",
    "    expansive pathway) about an input tensor is merged with\n",
    "    information representing the localization of details\n",
    "    (from the encoding, compressive pathway).\n",
    "    Modifications to the original paper:\n",
    "    (1) padding is used in 3x3 convolutions to prevent loss\n",
    "        of border pixels\n",
    "    (2) merging outputs does not require cropping due to (1)\n",
    "    (3) residual connections can be used by specifying\n",
    "        UNet(merge_mode='add')\n",
    "    (4) if non-parametric upsampling is used in the decoder\n",
    "        pathway (specified by upmode='upsample'), then an\n",
    "        additional 1x1 2d convolution occurs after upsampling\n",
    "        to reduce channel dimensionality by a factor of 2.\n",
    "        This channel halving happens with the convolution in\n",
    "        the tranpose convolution (specified by upmode='transpose')\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, in_channels=3, depth=5,\n",
    "                 start_filts=64, up_mode='transpose',\n",
    "                 merge_mode='concat'):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            in_channels: int, number of channels in the input tensor.\n",
    "                Default is 3 for RGB images.\n",
    "            depth: int, number of MaxPools in the U-Net.\n",
    "            start_filts: int, number of convolutional filters for the\n",
    "                first conv.\n",
    "            up_mode: string, type of upconvolution. Choices: 'transpose'\n",
    "                for transpose convolution or 'upsample' for nearest neighbour\n",
    "                tupsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        if up_mode in ('transpose', 'upsample'):\n",
    "            self.up_mode = up_mode\n",
    "        else:\n",
    "            raise ValueError(\"\\\"{}\\\" is not a valid mode for \"\n",
    "                             \"upsampling. Only \\\"transpose\\\" and \"\n",
    "                             \"\\\"upsample\\\" are allowed.\".format(up_mode))\n",
    "\n",
    "        if merge_mode in ('concat', 'add'):\n",
    "            self.merge_mode = merge_mode\n",
    "        else:\n",
    "            raise ValueError(\"\\\"{}\\\" is not a valid mode for\"\n",
    "                             \"merging up and down paths. \"\n",
    "                             \"Only \\\"concat\\\" and \"\n",
    "                             \"\\\"add\\\" are allowed.\".format(up_mode))\n",
    "\n",
    "        # NOTE: up_mode 'upsample' is incompatible with merge_mode 'add'\n",
    "        if self.up_mode == 'upsample' and self.merge_mode == 'add':\n",
    "            raise ValueError(\"up_mode \\\"upsample\\\" is incompatible \"\n",
    "                             \"with merge_mode \\\"add\\\" at the moment \"\n",
    "                             \"because it doesn't make sense to use \"\n",
    "                             \"nearest neighbour to reduce \"\n",
    "                             \"depth channels (by half).\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.start_filts = start_filts\n",
    "        self.depth = depth\n",
    "\n",
    "        self.down_convs = []\n",
    "        self.up_convs = []\n",
    "\n",
    "        # create the encoder pathway and add to a list\n",
    "        for i in range(depth):\n",
    "            ins = self.in_channels if i == 0 else outs\n",
    "            outs = self.start_filts * (2 ** i)\n",
    "            pooling = True if i < depth - 1 else False\n",
    "\n",
    "            down_conv = DownConv(ins, outs, pooling=pooling)\n",
    "            self.down_convs.append(down_conv)\n",
    "\n",
    "        # create the decoder pathway and add to a list\n",
    "        # - careful! decoding only requires depth-1 blocks\n",
    "        for i in range(depth - 1):\n",
    "            ins = outs\n",
    "            outs = ins // 2\n",
    "            up_conv = UpConv(ins, outs, up_mode=up_mode,\n",
    "                             merge_mode=merge_mode)\n",
    "            self.up_convs.append(up_conv)\n",
    "\n",
    "        self.conv_final = conv1x1(outs, self.num_classes)\n",
    "\n",
    "        # add the list of modules to current module\n",
    "        self.down_convs = nn.ModuleList(self.down_convs)\n",
    "        self.up_convs = nn.ModuleList(self.up_convs)\n",
    "\n",
    "        self.reset_params()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.xavier_normal(m.weight)\n",
    "            init.constant(m.bias, 0)\n",
    "\n",
    "    def reset_params(self):\n",
    "        for i, m in enumerate(self.modules()):\n",
    "            self.weight_init(m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_outs = []\n",
    "\n",
    "        # encoder pathway, save outputs for merging\n",
    "        for i, module in enumerate(self.down_convs):\n",
    "            x, before_pool = module(x)\n",
    "            encoder_outs.append(before_pool)\n",
    "\n",
    "        for i, module in enumerate(self.up_convs):\n",
    "            before_pool = encoder_outs[-(i + 2)]\n",
    "            x = module(before_pool, x)\n",
    "\n",
    "        # No softmax is used. This means you need to use\n",
    "        # nn.CrossEntropyLoss is your training script,\n",
    "        # as this module includes a softmax already.\n",
    "        x = self.conv_final(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    testing\n",
    "    \"\"\"\n",
    "    model = UNet(3, depth=5, merge_mode='concat')\n",
    "    x = Variable(torch.FloatTensor(np.random.random((1, 3, 320, 320))))\n",
    "    out = model(x)\n",
    "    loss = torch.sum(out)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTyqu4PE2qAb"
   },
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LIVU22oPjJhj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils import data\n",
    "import torch\n",
    "\n",
    "\n",
    "class ZurichLoader(data.Dataset):\n",
    "    \"\"\"\n",
    "    Data loader for Zurich dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, im_patches, gt_patches, split, data_augmentation = False):\n",
    "        \"\"\"\n",
    "        Load data.\n",
    "        :param split: 'train', 'val' or empty\n",
    "        :param transform_data: list of data transformations for images\n",
    "        :param transform_labels: list of data transformations for labels\n",
    "        :param im_size: size to which to crop labels and images\n",
    "        :param patch_size: size of image patches\n",
    "        \"\"\"\n",
    "        # data transformations\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.split = split\n",
    "         \n",
    "        # Load image indexes, depending on set:\n",
    "        if split == 'train':\n",
    "            self.img_idx = np.arange(0, int(len(im_patches)*.8))\n",
    "        else:  # elif split == 'val':\n",
    "            self.img_idx = np.arange(int(len(im_patches)*.8), int(len(im_patches)))\n",
    "        \n",
    "\n",
    "        self.im_patches = [im_patches[i] for i in self.img_idx]\n",
    "        self.gt_patches= [gt_patches[i] for i in self.img_idx]\n",
    "\n",
    "        # translate to data and label paths\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        function must be overridden: returns data-label pair of tensors for data point at index\n",
    "        Here we just return the entire images for demonstration reasons. In reality, you would crop\n",
    "        from each image at random here, or would have a pre-defined list of coordinates initialised\n",
    "        in the constructor and crop according to it.\n",
    "        \"\"\"\n",
    "\n",
    "        img = self.im_patches[idx]\n",
    "        gt = self.gt_patches[idx]\n",
    "\n",
    "        # convert image\n",
    "        # img = Image.fromarray((img*255).astype(np.uint8))\n",
    "        # gt = Image.fromarray(gt.astype(np.uint8)).convert('L')\n",
    "        # apply transformations\n",
    "\n",
    "        if self.data_augmentation:\n",
    "            img, gt = augment_images_and_gt(img, gt)\n",
    "        \n",
    "\n",
    "        # If you want to do special transforms like rotation, do them here.\n",
    "        # Don't forget to apply the same transforms to both the data and label tensors.\n",
    "        # You can use Torchsample, or else convert the data to numpy (e.g.: img.numpy())\n",
    "        # and then load it again into a torch tensor (img = torch.from_numpy(img)).\n",
    "\n",
    "        # TODO transformations using torchsample\n",
    "        img = np.asarray(img).transpose((2, 0, 1)).astype(np.float64)\n",
    "        img = torch.from_numpy(img.copy()).type(torch.FloatTensor)\n",
    "        gt = torch.from_numpy(gt.copy()).type(torch.LongTensor)  # .astype(np.double))\n",
    "\n",
    "        return img, gt\n",
    "\n",
    "    def __len__(self):\n",
    "        # function must be overridden: returns number of data points in data set\n",
    "        return len(self.gt_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_s1NRtBH3NMI"
   },
   "source": [
    "# Load Data and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0t4YjXQfGXoW"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=31616704512, available=26244857856, percent=17.0, used=4915703808, free=23746056192, active=5223993344, inactive=2155925504, buffers=101834752, cached=2853109760, shared=9654272)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 623,
     "output_extras": [
      {
       "item_id": 6
      },
      {
       "item_id": 7
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15614,
     "status": "error",
     "timestamp": 1522939488007,
     "user": {
      "displayName": "Cyril Wendl",
      "photoUrl": "//lh3.googleusercontent.com/-DqqZqA64TFk/AAAAAAAAAAI/AAAAAAAAdgI/1gqPBDp80k8/s50-c-k-no/photo.jpg",
      "userId": "102487229239284781410"
     },
     "user_tz": -120
    },
    "id": "_UsSCb212j2e",
    "outputId": "f21f7cca-83fe-4bae-901d-a081a14bfd6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-17c6c0d958a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mim_patches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_patches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mim_patches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-17c6c0d958a9>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(base_dir)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mimgs_eq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs_stretch_eq_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs_eq\u001b[0m    \u001b[0;31m# continue using stretched image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mpatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-17c6c0d958a9>\u001b[0m in \u001b[0;36mimgs_stretch_eq_\u001b[0;34m(imgs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mp98\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m98\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mband\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mimgs_eq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_im\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mband\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexposure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrescale_intensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mband\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp98\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# stretch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mimgs_eq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_im\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mband\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexposure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalize_hist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_eq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_im\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mband\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# equalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "# Training settings\n",
    "batch_size = 10\n",
    "test_batch_size = 20\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "no_cuda = False\n",
    "seed = 1\n",
    "log_interval = 50\n",
    "\n",
    "cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "def imgs_stretch_eq_(imgs):\n",
    "    \"\"\"\n",
    "    perform histogram stretching and equalization\n",
    "    :param imgs: images to stretch and equalize\n",
    "    :return imgs_eq: equalized images\n",
    "    \"\"\"\n",
    "\n",
    "    imgs_eq = imgs.copy()\n",
    "    for idx_im, im in enumerate(imgs):\n",
    "        # Contrast stretching\n",
    "        p2 = np.percentile(im, 2)\n",
    "        p98 = np.percentile(im, 98)\n",
    "        for band in range(im.shape[-1]):\n",
    "            imgs_eq[idx_im][:, :, band] = exposure.rescale_intensity(im[:, :, band], in_range=(p2, p98))  # stretch\n",
    "            imgs_eq[idx_im][:, :, band] = exposure.equalize_hist(imgs_eq[idx_im][:, :, band])  # equalize\n",
    "\n",
    "    # convert to np arrays\n",
    "    imgs_eq = np.asarray(imgs_eq)\n",
    "    return imgs_eq\n",
    "\n",
    "\n",
    "# load data\n",
    "def load_data(base_dir):\n",
    "    base_dir = base_dir + \"/SIE-Master/Zurich/Zurich_dataset\"\n",
    "    im_dir = base_dir + '/images_tif/'\n",
    "    gt_dir = base_dir + '/groundtruth/'\n",
    "    im_names = ns.natsorted(os.listdir(im_dir))\n",
    "    gt_names = ns.natsorted(os.listdir(gt_dir))\n",
    "    imgs = np.asarray([im_load(im_dir + im_name) for im_name in im_names])\n",
    "    gt = np.asarray([im_load(gt_dir + gt_name) for gt_name in gt_names])\n",
    "    # histogram stretching\n",
    "    \n",
    "    print(sys.getsizeof(imgs))\n",
    "    imgs_eq = imgs_stretch_eq_(imgs)\n",
    "    imgs = imgs_eq    # continue using stretched image\n",
    "    patch_size = 64\n",
    "    stride = 64\n",
    "    im_patches = get_padded_patches(imgs[:10], patch_size = patch_size, stride = stride)\n",
    "    print(sys.getsizeof(im_patches))\n",
    "    gt_patches = get_gt_patches(gt[:10], patch_size = patch_size, stride = stride)\n",
    "    return im_patches, gt_patches\n",
    "\n",
    "im_patches, gt_patches = load_data(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train_loader = data.DataLoader(\n",
    "    ZurichLoader(im_patches, gt_patches, 'train', data_augmentation=True),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "    ZurichLoader(im_patches, gt_patches, 'val'),\n",
    "    batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "n_classes = 9  # TODO parse\n",
    "\n",
    "model = UNet(n_classes, in_channels=4, depth=7)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def train(epochs=epochs):\n",
    "    \"\"\"Train model\"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (im_data, labels) in enumerate(train_loader):\n",
    "        im_data, labels = Variable(im_data), Variable(labels)\n",
    "        if cuda:\n",
    "            im_data, labels = im_data.cuda(), labels.cuda()\n",
    "        # class_weights = class_weight.compute_class_weight('balanced', np.unique(labels.data.numpy().flatten()),\n",
    "        #                                                np.arange(10))\n",
    "        # class_weights=Variable(torch.from_numpy(class_weights).type(torch.FloatTensor))\n",
    "        optimizer.zero_grad()\n",
    "        output = model(im_data)\n",
    "        loss = functional.cross_entropy(output, labels, ignore_index=0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                epochs, batch_idx * len(im_data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"Show accuracies for training and test sets\"\"\"\n",
    "    model.eval()\n",
    "    loss_test = 0\n",
    "    loss_train = 0\n",
    "    correct_test = 0\n",
    "    correct_train = 0\n",
    "    for im_data, labels in test_loader:\n",
    "        im_data, labels = Variable(im_data, volatile=True), Variable(labels)\n",
    "        if cuda:\n",
    "            im_data, labels = im_data.cuda(), labels.cuda()\n",
    "        output = model(im_data)\n",
    "        loss_test += functional.cross_entropy(output, labels, ignore_index=0).data[0]  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct_test += pred.eq(labels.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    for im_data, labels in train_loader:\n",
    "        im_data, labels = Variable(im_data, volatile=True), Variable(labels)\n",
    "        if cuda:\n",
    "            im_data, labels = im_data.cuda(), labels.cuda()\n",
    "        output = model(im_data)\n",
    "        loss_train += functional.cross_entropy(output, labels, ignore_index=0).data[0]  # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct_train += pred.eq(labels.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    loss_test /= len(test_loader.dataset)\n",
    "    print('\\nTraining set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss_train, correct_train, len(train_loader.dataset) * 64 * 64,\n",
    "        100. * correct_train / (len(train_loader.dataset) * 64 * 64)))\n",
    "      \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss_test, correct_test, len(test_loader.dataset) * 64 * 64,\n",
    "        100. * correct_test / (len(test_loader.dataset) * 64 * 64)))\n",
    "\n",
    "    \n",
    "def predict():\n",
    "    \"\"\"return predictions after training\"\"\"\n",
    "    # TODO test\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    for im_data, labels in test_loader:\n",
    "        im_data, labels = Variable(im_data, volatile=True), Variable(labels)\n",
    "        if cuda:\n",
    "            im_data, labels = im_data.cuda(), labels.cuda()\n",
    "        output = model(im_data)\n",
    "        pred.append(output.data.max(1, keepdim=True)[1])   # get the index of the max log-probability\n",
    "    return pred\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "def test_show_some_images():\n",
    "    # get data\n",
    "    test_im = train_loader.dataset[8]\n",
    "    im_test = Variable(test_im[0]).data.numpy()\n",
    "    im_test_l = Variable(test_im[1]).data.numpy()\n",
    "    im_test = np.transpose(im_test, (1, 2, 0))\n",
    "\n",
    "    # show figures\n",
    "    plt.figure()\n",
    "    plt.imshow(im_test[:, :, :3])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(im_test_l)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "y7HGx6zAkEud"
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "      train(epoch)\n",
    "      test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JVlG88xgGZTk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Keras with GPU Zurich Land Cover",
   "provenance": [
    {
     "file_id": "/v2/external/notebooks/gpu.ipynb",
     "timestamp": 1520510975028
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
