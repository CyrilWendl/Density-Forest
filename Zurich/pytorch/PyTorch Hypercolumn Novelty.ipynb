{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Hypercolumn CNN  for Zurich Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "from helpers_pytorch import *\n",
    "\n",
    "# custom libraries\n",
    "base_dir = '/raid/home/cwendl'  # for guanabana\n",
    "sys.path.append(base_dir + '/SIE-Master/Code')  # Path to density Tree package\n",
    "from helpers.data_loader import ZurichLoader\n",
    "from helpers.helpers import *\n",
    "from hypercolumn import HyperColumn\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "base_dir = '/raid/home/cwendl'  # for guanabana\n",
    "root_dir = base_dir + '/SIE-Master/Zurich'\n",
    "patch_size = 64\n",
    "class_to_remove = 1\n",
    "\n",
    "# load data\n",
    "dataset_train = ZurichLoader(root_dir, 'train', patch_size=patch_size, stride=patch_size, transform='augment',\n",
    "                             class_to_remove=class_to_remove)\n",
    "dataset_val = ZurichLoader(root_dir, 'val', patch_size=patch_size, stride=patch_size,\n",
    "                           class_to_remove=class_to_remove)\n",
    "dataset_test = ZurichLoader(root_dir, 'test', patch_size=patch_size, stride=patch_size,\n",
    "                            class_to_remove=class_to_remove)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=100, shuffle=True, num_workers=20)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=100, shuffle=False, num_workers=20)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=100, shuffle=False, num_workers=20)\n",
    "\n",
    "# load patches with overlap (for prediction)\n",
    "dataset_train_overlap = ZurichLoader(root_dir, 'train', patch_size=patch_size, stride=int(patch_size/2), \n",
    "                                     inherit_loader=dataset_train)\n",
    "dataset_val_overlap = ZurichLoader(root_dir, 'val', patch_size=patch_size, stride=int(patch_size/2), \n",
    "                                   inherit_loader=dataset_val)\n",
    "dataset_test_overlap = ZurichLoader(root_dir, 'test', patch_size=patch_size, stride=int(patch_size/2), \n",
    "                                    inherit_loader=dataset_test)\n",
    "\n",
    "dataloader_train_overlap = DataLoader(dataset_train_overlap, batch_size=100, shuffle=False, num_workers=40)\n",
    "dataloader_val_overlap = DataLoader(dataset_val_overlap, batch_size=100, shuffle=False, num_workers=40)\n",
    "dataloader_test_overlap = DataLoader(dataset_test_overlap, batch_size=100, shuffle=False, num_workers=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Train set: Average loss: 0.0160, Accuracy: 24.98%\n",
      "Val set: Average loss: 0.0175, Accuracy: 25.67%\n",
      "Epoch 1:\n"
     ]
    }
   ],
   "source": [
    "# train or load CNN model\n",
    "train_bool = True\n",
    "\n",
    "model = HyperColumn(in_dim=4, out_dim=9, n_filters=32, patch_size=patch_size).cuda()\n",
    "if train_bool:\n",
    "    train(model, dataloader_train, dataloader_val, epochs=200, verbosity=1, plot=True, class_to_remove=class_to_remove)\n",
    "    \n",
    "    # save model\n",
    "    state = {'model': model.state_dict(), \n",
    "             'n_epochs': 50,\n",
    "             'loss_tr':0.\n",
    "            }\n",
    "    torch.save(state, 'models/model_wo_cl_' + str(class_to_remove) + '.pytorch')\n",
    "     \n",
    "else:  # load saved model\n",
    "    state = torch.load('models/model_wo_cl_' + str(class_to_remove) + '.pytorch')\n",
    "    model.load_state_dict(state['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "# train\n",
    "preds_tr = predict_softmax(model, dataloader_train_overlap)\n",
    "preds_tr = remove_overlap(dataset_train.imgs, preds_tr, np.arange(10), patch_size=patch_size, stride=int(patch_size/2))\n",
    "preds_tr = np.concatenate(preds_tr)\n",
    "\n",
    "# val\n",
    "preds_val = predict_softmax(model, dataloader_val_overlap)\n",
    "preds_val = remove_overlap(dataset_val.imgs, preds_val, np.arange(5), patch_size=patch_size, stride=int(patch_size/2))\n",
    "preds_val = np.concatenate(preds_val)\n",
    "\n",
    "# test\n",
    "preds_te = predict_softmax(model, dataloader_test_overlap)\n",
    "preds_te = remove_overlap(dataset_test.imgs, preds_te, np.arange(5), patch_size=patch_size, stride=int(patch_size/2))\n",
    "preds_te = np.concatenate(preds_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free GPU memory\n",
    "while gc.collect():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels\n",
    "pred_labels_tr = get_y_pred_labels(preds_tr, class_to_remove=class_to_remove, background=False)\n",
    "pred_labels_val = get_y_pred_labels(preds_val, class_to_remove=class_to_remove, background=False)\n",
    "pred_labels_te = get_y_pred_labels(preds_te, class_to_remove=class_to_remove, background=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of correctly / incorrectly predicted pixels\n",
    "# train\n",
    "pred_t_tr = (dataset_train.gt_patches != class_to_remove) & (dataset_train.gt_patches != 0)\n",
    "pred_f_tr = dataset_train.gt_patches == class_to_remove\n",
    "\n",
    "# val\n",
    "pred_t_val = (dataset_val.gt_patches != class_to_remove) & (dataset_val.gt_patches != 0)\n",
    "pred_f_val = dataset_val.gt_patches == class_to_remove\n",
    "\n",
    "# test\n",
    "pred_t_te = (dataset_test.gt_patches != class_to_remove) & (dataset_test.gt_patches != 0)\n",
    "pred_f_te = dataset_test.gt_patches == class_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 1\n",
    "\n",
    "img = convert_patches_to_image(dataset_train.imgs, pred_labels_tr[..., np.newaxis], img_idx, patch_size, patch_size, 0)\n",
    "\n",
    "# pred\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(gt_label_to_color(img[..., 0], dataset_train.colors)*255)\n",
    "plt.show()\n",
    "\n",
    "# im\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 7))\n",
    "axes[0].imshow(dataset_train.imgs[img_idx][..., :3])\n",
    "axes[1].imshow(gt_label_to_color(dataset_train.gt[img_idx], dataset_train.colors)*255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 2\n",
    "\n",
    "img = convert_patches_to_image(dataset_val.imgs, pred_labels_val[..., np.newaxis], img_idx, patch_size, patch_size, 0)\n",
    "\n",
    "# pred\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(gt_label_to_color(img[..., 0], dataset_val.colors)*255)\n",
    "plt.show()\n",
    "\n",
    "# im\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 7))\n",
    "axes[0].imshow(dataset_val.imgs[img_idx][..., :3])\n",
    "axes[1].imshow(gt_label_to_color(dataset_val.gt[img_idx], dataset_val.colors)*255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class names and colors\n",
    "names = dataset_train.names\n",
    "colors = dataset_train.colors\n",
    "n_classes = 9\n",
    "classes_to_keep = np.asarray([x for x in range(1, n_classes) if x != class_to_remove])\n",
    "names_keep = np.asarray(names)[classes_to_keep]\n",
    "print(\"classes to keep: \" + str(names_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of predicted labels in test set\n",
    "pred_labels, pred_counts = np.unique(pred_labels_te, return_counts=True)\n",
    "pred_counts = pred_counts / sum(pred_counts) * 100\n",
    "\n",
    "# visualization\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "plt.bar(pred_labels, pred_counts)\n",
    "plt.xticks(np.arange(0, 10))\n",
    "plt.ylim([0, 100])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"Count [%]\")\n",
    "plt.grid(alpha=.3)\n",
    "fig.axes[0].spines['right'].set_visible(False)\n",
    "fig.axes[0].spines['top'].set_visible(False)\n",
    "plt.title(\"Predicted labels\")\n",
    "plt.xticks(np.arange(len(names)), names, rotation=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of predicted label that were misclassified\n",
    "pred_labels, pred_counts = np.unique(pred_labels_te[pred_f_te], return_counts=True)\n",
    "pred_counts = pred_counts / sum(pred_counts) * 100\n",
    "\n",
    "# visualization\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "plt.bar(pred_labels, pred_counts)\n",
    "plt.xticks(np.arange(0, 10))\n",
    "plt.ylim([0, 100])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"Count [%]\")\n",
    "plt.grid(alpha=.3)\n",
    "fig.axes[0].spines['right'].set_visible(False)\n",
    "fig.axes[0].spines['top'].set_visible(False)\n",
    "plt.title(\"Misclassified labels (mean MSR=%.2f)\" % np.mean(get_acc_net_msr(preds_te[pred_f_te])))\n",
    "plt.xticks(np.arange(len(names)), names, rotation=20)\n",
    "plt.savefig(\"Figures/Pred_count/ZH_pred-count_wo_cl\" + str(class_to_remove) + \".pdf\",\n",
    "            bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO get weights\n",
    "# p = [p for p in model.parameters()]\n",
    "# for p_ in p:\n",
    "    # print(p_.cpu().detach().numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones(len(np.unique(dataset_train.gt_patches)))\n",
    "ones[0] = 0\n",
    "weight = torch.from_numpy(ones).float().cuda()\n",
    "f_loss = nn.CrossEntropyLoss(weight=weight)\n",
    "p = test(model, f_loss, dataloader_train, \"Train\", verbosity=1)\n",
    "p = test(model, f_loss, dataloader_val, \"Val\", verbosity=1)\n",
    "p = test(model, f_loss, dataloader_test, \"Test\", verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy measures for each class\n",
    "y_preds = [pred_labels_tr, pred_labels_val, pred_labels_te]\n",
    "datasets = [dataset_train, dataset_val, dataset_test]\n",
    "\n",
    "aa_sets, oa_sets = [], []\n",
    "for y_pred, y_true in zip(y_preds, datasets):\n",
    "    y_pred_flattened = np.asarray(y_pred.flatten()).astype('int') \n",
    "    y_true_flattened = np.asarray(y_true.gt_patches.flatten()).astype('int') \n",
    "\n",
    "    # mask background and removed classes for evaluation metrics\n",
    "    filter_items = (y_true_flattened != 0) & (y_true_flattened != class_to_remove)\n",
    "    \n",
    "    aa_set, _ = aa(y_true_flattened[filter_items], y_pred_flattened[filter_items])\n",
    "    oa_set = oa(y_true_flattened[filter_items], y_pred_flattened[filter_items])\n",
    "    print(\"OA: %.3f, AA: %.3f\" % (oa_set, aa_set))  # slightly higher accuracy because of overlapping patches\n",
    "    oa_sets.append(oa_set)\n",
    "    aa_sets.append(aa_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO save CSV\n",
    "# write metrics to CSV files\n",
    "df_metrics = pd.read_csv('models/metrics_ND.csv', index_col=0)\n",
    "accs = np.concatenate([[oa_sets[i], aa_sets[i]] for i in range(3)])  # [oa, aa] for tr, val, te\n",
    "df2 = pd.DataFrame({str(names[class_to_remove]):accs},\n",
    "                   index = ['OA Train', 'AA Train', 'OA Val', 'AA Val', 'OA Test', 'AA Test']).T\n",
    "df_metrics = df_metrics.append(df2)\n",
    "df_metrics = df_metrics[~df_metrics.index.duplicated(keep='last')]  # avoid duplicates\n",
    "df_metrics.to_csv('models/metrics_ND.csv')\n",
    "print((df_metrics*100).round(2).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision-recall curves\n",
    "y_true = pred_f_te.flatten()\n",
    "\n",
    "# msr\n",
    "y_scores = (-get_acc_net_msr(preds_te)).flatten()\n",
    "precision_msr, recall_msr, _ = metrics.precision_recall_curve(y_true, y_scores)\n",
    "pr_auc_msr = metrics.average_precision_score(y_true, y_scores)\n",
    "auroc_msr = metrics.roc_auc_score(y_true, y_scores)\n",
    "fpr_msr, tpr_msr, _ = metrics.roc_curve(y_true, y_scores)\n",
    "\n",
    "# margin\n",
    "y_scores = (-get_acc_net_max_margin(preds_te)).flatten()\n",
    "precision_margin, recall_margin, _ = metrics.precision_recall_curve(y_true, y_scores)\n",
    "pr_auc_margin = metrics.average_precision_score(y_true, y_scores)\n",
    "auroc_margin = metrics.roc_auc_score(y_true, y_scores)\n",
    "fpr_margin, tpr_margin, _ = metrics.roc_curve(y_true, y_scores)\n",
    "\n",
    "# entropy\n",
    "y_scores = (-get_acc_net_entropy(preds_te)).flatten()\n",
    "precision_entropy, recall_entropy, _ = metrics.precision_recall_curve(y_true, y_scores)\n",
    "pr_auc_entropy = metrics.average_precision_score(y_true, y_scores)\n",
    "auroc_entropy = metrics.roc_auc_score(y_true, y_scores)\n",
    "fpr_entropy, tpr_entropy, _ = metrics.roc_curve(y_true, y_scores)\n",
    "\n",
    "print(\"AUROC: %.2f, PR AUC: %.2f\" % (auroc_msr, pr_auc_msr))\n",
    "print(\"AUROC: %.2f, PR AUC: %.2f\" % (auroc_margin, pr_auc_margin))\n",
    "print(\"AUROC: %.2f, PR AUC: %.2f\" % (auroc_entropy, pr_auc_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "# MSR\n",
    "probas_patches_msr = np.reshape((get_acc_net_msr(preds_te)).flatten(), np.shape(dataset_test.gt_patches))\n",
    "probas_patches_msr -= np.min(probas_patches_msr)\n",
    "probas_patches_msr /= np.max(probas_patches_msr)\n",
    "\n",
    "# margin\n",
    "probas_patches_margin = np.reshape((get_acc_net_max_margin(preds_te)).flatten(), np.shape(dataset_test.gt_patches))\n",
    "probas_patches_margin -= np.min(probas_patches_margin)\n",
    "probas_patches_margin /= np.max(probas_patches_margin)\n",
    "\n",
    "# entropy\n",
    "probas_patches_entropy = np.reshape((get_acc_net_entropy(preds_te)).flatten(), np.shape(dataset_test.gt_patches))\n",
    "probas_patches_entropy -= np.min(probas_patches_entropy)\n",
    "probas_patches_entropy /= np.max(probas_patches_entropy)\n",
    "\n",
    "base_folder = \"Figures/Im_cert/cl_\" + str(class_to_remove)\n",
    "\n",
    "# show images\n",
    "for img_idx in range(len(dataset_test.imgs)):\n",
    "    acc_im_msr = convert_patches_to_image(dataset_test.imgs, probas_patches_msr[..., np.newaxis], img_idx, 64, 64, 0)\n",
    "    acc_im_msr = imgs_stretch_eq([acc_im_msr])[0]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(acc_im_msr[..., 0], cmap='RdYlGn')\n",
    "    plt.axis('off')\n",
    "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.savefig(base_folder + \"/ZH_wo_cl_\" + str(class_to_remove) + \"_net_msr_im_\" + str(img_idx) + \".pdf\", \n",
    "                bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    \n",
    "    acc_im_margin = convert_patches_to_image(dataset_test.imgs, probas_patches_margin[..., np.newaxis],\n",
    "                                             img_idx, 64, 64, 0)\n",
    "    acc_im_margin = imgs_stretch_eq([acc_im_margin])[0]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.imshow(acc_im_margin[..., 0], cmap='RdYlGn')\n",
    "    plt.savefig(base_folder + \"/ZH_wo_cl_\" + str(class_to_remove) + \"_net_margin_im_\" + str(img_idx) + \".pdf\", \n",
    "                bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    \n",
    "    acc_im_entropy = convert_patches_to_image(dataset_test.imgs, probas_patches_entropy[..., np.newaxis],\n",
    "                                              img_idx, 64, 64, 0)\n",
    "    acc_im_entropy = imgs_stretch_eq([acc_im_entropy])[0]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(acc_im_entropy[..., 0], cmap='RdYlGn')\n",
    "    plt.axis('off')\n",
    "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.savefig(base_folder + \"/ZH_wo_cl_\" + str(class_to_remove) + \"_net_entropy_im_\" + str(img_idx) + \".pdf\", \n",
    "                bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get softmax scores with dropout\n",
    "n_iters = 10\n",
    "probas_dropout = predict_softmax_w_dropout(model, dataloader_test, n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "y_scores = -get_acc_net_entropy(np.mean(probas_dropout, 0)).flatten()\n",
    "\n",
    "# PR\n",
    "precision_dropout, recall_dropout, _ = metrics.precision_recall_curve(y_true, y_scores)\n",
    "pr_auc_dropout = metrics.auc(recall_dropout, precision_dropout)\n",
    "\n",
    "# ROC\n",
    "fpr_dropout, tpr_dropout, _ = metrics.roc_curve(y_true, y_scores)\n",
    "auroc_dropout = metrics.roc_auc_score(y_true, y_scores)\n",
    "plt.step(recall_dropout, precision_dropout)\n",
    "print(\"AUROC: %.2f, PR AUC: %.2f\" % (auroc_dropout, pr_auc_dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear CUDA storage\n",
    "while gc.collect():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export images\n",
    "probas_patches_dropout = np.reshape(y_scores, np.shape(dataset_test.gt_patches))\n",
    "probas_patches_dropout -= np.min(probas_patches_dropout)\n",
    "probas_patches_dropout /= np.max(probas_patches_dropout)\n",
    "\n",
    "# show image of DF uncertainty vs. max margin uncertainty\n",
    "for img_idx in range(len(dataset_test.imgs)):\n",
    "    acc_im_dropout = convert_patches_to_image(dataset_test.imgs, probas_patches_dropout[..., np.newaxis], img_idx, 64, 64, 0)\n",
    "    acc_im_dropout = imgs_stretch_eq([acc_im_dropout])[0]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.imshow(acc_im_dropout[..., 0], cmap='RdYlGn')\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    plt.savefig(base_folder + \"/ZH_wo_cl_\" + str(class_to_remove) + \"_dropout_im_\" + str(img_idx) + \".pdf\", \n",
    "                bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_val = get_activations(model, dataloader_val_overlap, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_val_overlap, dataloader_val_overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import decomposition, svm, preprocessing\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn import metrics\n",
    "\n",
    "# custom libraries\n",
    "from helpers.parameter_search import *\n",
    "from density_forest.density_forest import *\n",
    "from density_forest.plots import *\n",
    "from density_forest.helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(draw_subsamples(act_val, .1)))\n",
    "print(np.shape(np.concatenate(np.concatenate(draw_subsamples(act_val, .1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_bal = get_balanced_subset_indices(dataset_test.gt_patches.flatten(), np.arange(1, 9)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create density tree for activation weights of training data\n",
    "# PCA\n",
    "pca = decomposition.PCA(n_components=.95)\n",
    "\n",
    "# TODO fit on training set without background pixels\n",
    "pca.fit(draw_subsamples(np.concatenate(np.concatenate(draw_subsamples(act_val, .1)))))  \n",
    "n_components = np.alen(pca.explained_variance_ratio_)\n",
    "print(\"Variance explained by first %i components: %.2f\" % (\n",
    "    n_components, sum(pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative explained variance\n",
    "fig = plt.figure()\n",
    "plt.scatter(np.arange(n_components), np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cumulative sum of explained variance\")\n",
    "plt.grid(alpha=.3)\n",
    "fig.axes[0].spines['right'].set_visible(False)\n",
    "fig.axes[0].spines['top'].set_visible(False)\n",
    "plt.savefig(\"Figures/PCA/pca_components_wo_cl_\" + str(class_to_remove) + \".pdf\",\n",
    "            bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_val = pca.transform(np.concatenate(np.concatenate(act_val)))[..., :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get other activations\n",
    "\n",
    "# test\n",
    "act_test = get_activations(model, dataloader_test_overlap, 64)\n",
    "print(np.shape(act_test))\n",
    "\n",
    "# PCA \n",
    "act_test = pca.transform(np.concatenate(np.concatenate(act_test)))[..., :10]\n",
    "print(np.shape(act_test))\n",
    "del dataset_test_overlap, dataloader_test_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=500)\n",
    "tsne_all = tsne.fit_transform(act_test[ind_bal])\n",
    "tsne_y = dataset_test.gt_patches.flatten()[ind_bal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "_, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "ax.set_axis_off()\n",
    "plot_pts_2d(tsne_all, tsne_y, ax, classes_to_keep, colors, class_to_remove=class_to_remove)\n",
    "plt.savefig(\"Figures/tSNE/t-SNE_\" + str(names[class_to_remove]).lower().replace(\" \", \"\") + \"_after_PCA.pdf\",\n",
    "            bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear CUDA storage\n",
    "while gc.collect():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get other activations\n",
    "\n",
    "# train\n",
    "act_train = get_activations(model, dataloader_train_overlap, 64, pca)\n",
    "print(act_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_train_overlap, dataloader_train_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear CUDA storage\n",
    "while gc.collect():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Estimation using Network Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GMM\n",
    "gmm = GaussianMixture(n_components=6, max_iter=10000)\n",
    "gmm.fit(draw_subsamples(act_train[pred_t_tr.flatten()], .01))\n",
    "\n",
    "# Predict\n",
    "probas_gmm = gmm.predict_proba(act_test)\n",
    "probas_gmm = get_acc_net_entropy(probas_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "y_true = pred_f_te.flatten()\n",
    "y_scores = -probas_gmm\n",
    "\n",
    "# PR\n",
    "precision_gmm, recall_gmm, _ = metrics.precision_recall_curve(y_true, y_scores)\n",
    "pr_auc_gmm = metrics.auc(recall_gmm, precision_gmm)\n",
    "\n",
    "# ROC\n",
    "fpr_gmm, tpr_gmm, _ = metrics.roc_curve(y_true, y_scores)\n",
    "auroc_gmm = metrics.roc_auc_score(y_true, y_scores)\n",
    "plt.step(recall_gmm, precision_gmm)\n",
    "print(\"AUROC: %.2f, PR AUC: %.2f\" % (auroc_gmm, pr_auc_gmm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "probas_patches_gmm = np.reshape(y_scores, np.shape(dataset_test.gt_patches))\n",
    "probas_patches_gmm -= np.min(probas_patches_gmm)\n",
    "probas_patches_gmm /= np.max(probas_patches_gmm)\n",
    "\n",
    "# show image of DF uncertainty vs. max margin uncertainty\n",
    "for img_idx in range(len(dataset_test.imgs)):\n",
    "    acc_im_gmm = convert_patches_to_image(dataset_test.imgs, probas_patches_gmm[..., np.newaxis], img_idx, 64, 64, 0)\n",
    "    acc_im_gmm = imgs_stretch_eq([acc_im_gmm])[0]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.imshow(acc_im_gmm[..., 0], cmap='RdYlGn')\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    plt.savefig(base_folder + \"/ZH_wo_cl_\" + str(class_to_remove) + \"_gmm_im_\" + str(img_idx) + \".pdf\", \n",
    "                bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_svm = preprocessing.scale(act_train)\n",
    "# act_val_svm = preprocessing.scale(act_val)\n",
    "act_test_svm = preprocessing.scale(act_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SVM\n",
    "# TODO test limitation to 5 components\n",
    "max_dim = 5\n",
    "clf_svm = svm.OneClassSVM(kernel='rbf', max_iter=10000, verbose=True)\n",
    "clf_svm.fit(draw_subsamples(act_train_svm[pred_t_tr.flatten(), :max_dim], .001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "probas_svm = clf_svm.decision_function(act_test_svm[..., :max_dim])\n",
    "probas_svm -= np.min(probas_svm)\n",
    "probas_svm /= np.max(probas_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "\n",
    "y_scores = -probas_svm[:]\n",
    "# PR\n",
    "precision_svm, recall_svm, _ = metrics.precision_recall_curve(y_true, y_scores)\n",
    "pr_auc_svm = metrics.auc(recall_svm, precision_svm)\n",
    "\n",
    "# ROC\n",
    "fpr_svm, tpr_svm, _ = metrics.roc_curve(y_true, y_scores)\n",
    "auroc_svm = metrics.roc_auc_score(y_true, y_scores)\n",
    "\n",
    "print(\"AUROC: %.2f, PR AUC: %.2f\" % (auroc_svm, pr_auc_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DensityForest instance\n",
    "clf_df = DensityForest(max_depth=2, min_subset=0, n_trees=20,\n",
    "                       subsample_pct=.0001, n_jobs=-1, verbose=10, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_df.fit(act_train[pred_t_tr.flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_df = clf_df.predict(act_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "y_scores = -probas_df\n",
    "\n",
    "# PR\n",
    "precision_df, recall_df, _ = metrics.precision_recall_curve(y_true, y_scores)\n",
    "pr_auc_df = metrics.auc(recall_df, precision_df)\n",
    "\n",
    "# ROC\n",
    "fpr_df, tpr_df, _ = metrics.roc_curve(y_true, y_scores)\n",
    "auroc_df = metrics.roc_auc_score(y_true, y_scores)\n",
    "\n",
    "print(\"AUROC: %.2f, PR AUC: %.2f\" % (auroc_df, pr_auc_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "probas_patches_df = np.reshape(probas_df, np.shape(dataset_test.gt_patches))\n",
    "probas_patches_df -= np.min(probas_patches_df)\n",
    "probas_patches_df /= np.max(probas_patches_df)\n",
    "\n",
    "# show image of DF uncertainty vs. max margin uncertainty\n",
    "for img_idx in range(len(dataset_test.imgs)):\n",
    "    acc_im_df = convert_patches_to_image(dataset_test.imgs, probas_patches_df[..., np.newaxis], img_idx, 64, 64, 0)\n",
    "    acc_im_df = imgs_stretch_eq([acc_im_df])[0]\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.imshow(acc_im_df[..., 0], cmap='RdYlGn')\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    plt.savefig(base_folder + \"/ZH_wo_cl_\" + str(class_to_remove) + \"_df_im_\" + str(img_idx) + \".pdf\", \n",
    "                bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "# order according to increasing score\n",
    "scores_pr = [pr_auc_msr, pr_auc_margin, pr_auc_entropy, pr_auc_dropout, pr_auc_gmm, pr_auc_svm, pr_auc_df]\n",
    "\n",
    "recalls = [recall_msr, recall_margin, recall_entropy, recall_dropout, recall_gmm, recall_svm, recall_df]\n",
    "precisions = [precision_msr, precision_margin, precision_entropy, precision_dropout, \n",
    "              precision_gmm, precision_svm, precision_df]\n",
    "\n",
    "names_methods = np.array(['MSR', 'Margin', 'Entropy', 'Dropout', 'GMM', 'OC SVM', 'DF'])\n",
    "scores_order = np.argsort(scores_pr)\n",
    "colors_lines = plt.cm.rainbow(np.linspace(0, 1, len(scores_pr)))[:, :3]\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "for i in scores_order:\n",
    "    plt.step(recalls[i], precisions[i], where='post', c=colors_lines[i])\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.grid(alpha=.3)\n",
    "fig.axes[0].spines['right'].set_visible(False)\n",
    "fig.axes[0].spines['top'].set_visible(False)\n",
    "plt.legend([str.format('%s: %.2f') % (names_methods[i], scores_pr[i]) for i in scores_order], title=\"PR AUC\")\n",
    "plt.savefig(\"Figures/Metrics/PR_pred_wo_cl_\" + str(class_to_remove) + \".pdf\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "# order according to increasing score\n",
    "scores_auc = [auroc_msr, auroc_margin, auroc_entropy, auroc_dropout, auroc_gmm, auroc_svm, auroc_df]\n",
    "fprs = [fpr_msr, fpr_margin, fpr_entropy, fpr_dropout, fpr_gmm, fpr_svm, fpr_df]\n",
    "tprs = [tpr_msr, tpr_margin, tpr_entropy, tpr_dropout, tpr_gmm, tpr_svm, tpr_df]\n",
    "scores_order = np.argsort(scores_auc)\n",
    "colors_lines = plt.cm.rainbow(np.linspace(0, 1, len(scores_auc)))[:, :3]\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "for i in scores_order:\n",
    "    plt.step(fprs[i], tprs[i], where='post', c=colors_lines[i])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', c='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.grid(alpha=.3)\n",
    "fig.axes[0].spines['right'].set_visible(False)\n",
    "fig.axes[0].spines['top'].set_visible(False)\n",
    "plt.legend([str.format('%s: %.2f') % (names_methods[i], scores_auc[i]) for i in scores_order], title=\"AUROC\")\n",
    "plt.savefig(\"Figures/Metrics/ROC_pred_wo_cl_\" + str(class_to_remove) + \".pdf\", bbox_inches='tight', pad_inches=0)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
